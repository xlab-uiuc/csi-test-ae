{
    "5":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws5; line 1 pos 14;",
            "write_value":"No output, find exception: java.lang.NumberFormatException: For input string: \"0/0\"",
            "log_location":"(read) log_w_df_r_df_orc (line 100)",
            "pass":true
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.NumberFormatException: For input string: \"0/0\"",
            "log_location":"(write) log_w_df_avro (line 2325)",
            "pass":true
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.NumberFormatException: For input string: \"0/0\"",
            "log_location":"(write) log_w_df_parquet (line 9874)",
            "pass":true
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws5; line 1 pos 14;",
            "write_value":"No output, find exception: java.lang.NumberFormatException: For input string: \"0/0\"",
            "log_location":"(read) log_w_df_r_df_parquet (line 102)",
            "pass":true
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws5; line 1 pos 14;",
            "write_value":"No output, find exception: java.lang.NumberFormatException: For input string: \"0/0\"",
            "log_location":"(read) log_w_df_r_df_avro (line 943)",
            "pass":true
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.NumberFormatException: For input string: \"0/0\"",
            "log_location":"(write) log_w_df_orc (line 9872)",
            "pass":true
        }
    },
    "6":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"2147483647",
            "write_value":"2147483647",
            "pass":false
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws6; line 1 pos 14;",
            "write_value":"No output, find exception: java.lang.RuntimeException: Error while encoding: java.lang.RuntimeException: java.lang.Integer is not a valid external type for schema of tinyint",
            "log_location":"(read) log_w_df_r_df_orc (line 140)",
            "pass":true
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"2147483647, find exception: java.lang.ArithmeticException: Casting 2147483647 to tinyint causes overflow",
            "log_location":"(write) log_w_sql_orc (line 938)",
            "pass":true
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.RuntimeException: Error while encoding: java.lang.RuntimeException: java.lang.Integer is not a valid external type for schema of tinyint",
            "log_location":"(write) log_w_df_avro (line 2364)",
            "pass":true
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"2147483647",
            "write_value":"2147483647",
            "pass":false
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.RuntimeException: Error while encoding: java.lang.RuntimeException: java.lang.Integer is not a valid external type for schema of tinyint",
            "log_location":"(write) log_w_df_parquet (line 9913)",
            "pass":true
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"2147483647, find exception: java.lang.ArithmeticException: Casting 2147483647 to tinyint causes overflow",
            "log_location":"(write) log_w_sql_parquet (line 939)",
            "pass":true
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws6; line 1 pos 14;",
            "write_value":"No output, find exception: java.lang.RuntimeException: Error while encoding: java.lang.RuntimeException: java.lang.Integer is not a valid external type for schema of tinyint",
            "log_location":"(read) log_w_df_r_df_parquet (line 142)",
            "pass":true
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"2147483647, find exception: java.lang.ArithmeticException: Casting 2147483647 to tinyint causes overflow",
            "log_location":"(write) log_w_sql_orc (line 938)",
            "pass":true
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"2147483647, find exception: java.lang.ArithmeticException: Casting 2147483647 to tinyint causes overflow",
            "log_location":"(write) log_w_sql_parquet (line 939)",
            "pass":true
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws6; line 1 pos 14;",
            "write_value":"No output, find exception: java.lang.RuntimeException: Error while encoding: java.lang.RuntimeException: java.lang.Integer is not a valid external type for schema of tinyint",
            "log_location":"(read) log_w_df_r_df_avro (line 983)",
            "pass":true
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.RuntimeException: Error while encoding: java.lang.RuntimeException: java.lang.Integer is not a valid external type for schema of tinyint",
            "log_location":"(write) log_w_df_orc (line 9911)",
            "pass":true
        }
    },
    "7":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"-2147483647",
            "write_value":"-2147483647",
            "pass":false
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws7; line 1 pos 14;",
            "write_value":"No output, find exception: java.lang.RuntimeException: Error while encoding: java.lang.RuntimeException: java.lang.Integer is not a valid external type for schema of tinyint",
            "log_location":"(read) log_w_df_r_df_orc (line 180)",
            "pass":true
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"-2147483647, find exception: java.lang.ArithmeticException: Casting -2147483647 to tinyint causes overflow",
            "log_location":"(write) log_w_sql_orc (line 1211)",
            "pass":true
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.RuntimeException: Error while encoding: java.lang.RuntimeException: java.lang.Integer is not a valid external type for schema of tinyint",
            "log_location":"(write) log_w_df_avro (line 2849)",
            "pass":true
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"-2147483647",
            "write_value":"-2147483647",
            "pass":false
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.RuntimeException: Error while encoding: java.lang.RuntimeException: java.lang.Integer is not a valid external type for schema of tinyint",
            "log_location":"(write) log_w_df_parquet (line 10421)",
            "pass":true
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"-2147483647, find exception: java.lang.ArithmeticException: Casting -2147483647 to tinyint causes overflow",
            "log_location":"(write) log_w_sql_parquet (line 1212)",
            "pass":true
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws7; line 1 pos 14;",
            "write_value":"No output, find exception: java.lang.RuntimeException: Error while encoding: java.lang.RuntimeException: java.lang.Integer is not a valid external type for schema of tinyint",
            "log_location":"(read) log_w_df_r_df_parquet (line 182)",
            "pass":true
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"-2147483647, find exception: java.lang.ArithmeticException: Casting -2147483647 to tinyint causes overflow",
            "log_location":"(write) log_w_sql_orc (line 1211)",
            "pass":true
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"-2147483647, find exception: java.lang.ArithmeticException: Casting -2147483647 to tinyint causes overflow",
            "log_location":"(write) log_w_sql_parquet (line 1212)",
            "pass":true
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws7; line 1 pos 14;",
            "write_value":"No output, find exception: java.lang.RuntimeException: Error while encoding: java.lang.RuntimeException: java.lang.Integer is not a valid external type for schema of tinyint",
            "log_location":"(read) log_w_df_r_df_avro (line 1023)",
            "pass":true
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.RuntimeException: Error while encoding: java.lang.RuntimeException: java.lang.Integer is not a valid external type for schema of tinyint",
            "log_location":"(write) log_w_df_orc (line 10419)",
            "pass":true
        }
    },
    "8":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"1",
            "write_value":"1",
            "pass":false
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws8; line 1 pos 14;",
            "write_value":"No output, find exception: java.lang.NumberFormatException: For input string: \"1.1\"",
            "log_location":"(read) log_w_df_r_df_orc (line 220)",
            "pass":true
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"1",
            "write_value":"1",
            "pass":false
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.NumberFormatException: For input string: \"1.1\"",
            "log_location":"(write) log_w_df_avro (line 3324)",
            "pass":true
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"1",
            "write_value":"1",
            "pass":false
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.NumberFormatException: For input string: \"1.1\"",
            "log_location":"(write) log_w_df_parquet (line 10919)",
            "pass":true
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"1",
            "write_value":"1",
            "pass":false
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws8; line 1 pos 14;",
            "write_value":"No output, find exception: java.lang.NumberFormatException: For input string: \"1.1\"",
            "log_location":"(read) log_w_df_r_df_parquet (line 222)",
            "pass":true
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"1",
            "write_value":"1",
            "pass":false
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"1",
            "write_value":"1",
            "pass":false
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws8; line 1 pos 14;",
            "write_value":"No output, find exception: java.lang.NumberFormatException: For input string: \"1.1\"",
            "log_location":"(read) log_w_df_r_df_avro (line 1063)",
            "pass":true
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.NumberFormatException: For input string: \"1.1\"",
            "log_location":"(write) log_w_df_orc (line 10894)",
            "pass":true
        }
    },
    "14":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws14; line 1 pos 14;",
            "write_value":"No output, find exception: java.lang.NumberFormatException: For input string: \"0/0\"",
            "log_location":"(read) log_w_df_r_df_orc (line 300)",
            "pass":true
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.NumberFormatException: For input string: \"0/0\"",
            "log_location":"(write) log_w_df_avro (line 3448)",
            "pass":true
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.NumberFormatException: For input string: \"0/0\"",
            "log_location":"(write) log_w_df_parquet (line 11043)",
            "pass":true
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws14; line 1 pos 14;",
            "write_value":"No output, find exception: java.lang.NumberFormatException: For input string: \"0/0\"",
            "log_location":"(read) log_w_df_r_df_parquet (line 302)",
            "pass":true
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws14; line 1 pos 14;",
            "write_value":"No output, find exception: java.lang.NumberFormatException: For input string: \"0/0\"",
            "log_location":"(read) log_w_df_r_df_avro (line 1988)",
            "pass":true
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.NumberFormatException: For input string: \"0/0\"",
            "log_location":"(write) log_w_df_orc (line 11018)",
            "pass":true
        }
    },
    "15":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"2147483647",
            "write_value":"2147483647",
            "pass":false
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws15; line 1 pos 14;",
            "write_value":"No output, find exception: java.lang.RuntimeException: Error while encoding: java.lang.RuntimeException: java.lang.Integer is not a valid external type for schema of smallint",
            "log_location":"(read) log_w_df_r_df_orc (line 340)",
            "pass":true
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"2147483647, find exception: java.lang.ArithmeticException: Casting 2147483647 to smallint causes overflow",
            "log_location":"(write) log_w_sql_orc (line 1540)",
            "pass":true
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.RuntimeException: Error while encoding: java.lang.RuntimeException: java.lang.Integer is not a valid external type for schema of smallint",
            "log_location":"(write) log_w_df_avro (line 3487)",
            "pass":true
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"2147483647",
            "write_value":"2147483647",
            "pass":false
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.RuntimeException: Error while encoding: java.lang.RuntimeException: java.lang.Integer is not a valid external type for schema of smallint",
            "log_location":"(write) log_w_df_parquet (line 11082)",
            "pass":true
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"2147483647, find exception: java.lang.ArithmeticException: Casting 2147483647 to smallint causes overflow",
            "log_location":"(write) log_w_sql_parquet (line 1541)",
            "pass":true
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws15; line 1 pos 14;",
            "write_value":"No output, find exception: java.lang.RuntimeException: Error while encoding: java.lang.RuntimeException: java.lang.Integer is not a valid external type for schema of smallint",
            "log_location":"(read) log_w_df_r_df_parquet (line 342)",
            "pass":true
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"2147483647, find exception: java.lang.ArithmeticException: Casting 2147483647 to smallint causes overflow",
            "log_location":"(write) log_w_sql_orc (line 1540)",
            "pass":true
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"2147483647, find exception: java.lang.ArithmeticException: Casting 2147483647 to smallint causes overflow",
            "log_location":"(write) log_w_sql_parquet (line 1541)",
            "pass":true
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws15; line 1 pos 14;",
            "write_value":"No output, find exception: java.lang.RuntimeException: Error while encoding: java.lang.RuntimeException: java.lang.Integer is not a valid external type for schema of smallint",
            "log_location":"(read) log_w_df_r_df_avro (line 2028)",
            "pass":true
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.RuntimeException: Error while encoding: java.lang.RuntimeException: java.lang.Integer is not a valid external type for schema of smallint",
            "log_location":"(write) log_w_df_orc (line 11057)",
            "pass":true
        }
    },
    "16":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"-2147483647, find exception: Error in query: Table not found: ws16; line 1 pos 12;",
            "log_location":"(write) log_w_sql_avro (line 1067)",
            "pass":true
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws16; line 1 pos 14;",
            "write_value":"No output, find exception: java.lang.RuntimeException: Error while encoding: java.lang.RuntimeException: java.lang.Integer is not a valid external type for schema of smallint",
            "log_location":"(read) log_w_df_r_df_orc (line 380)",
            "pass":true
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"-2147483647, find exception: java.lang.ArithmeticException: Casting -2147483647 to smallint causes overflow",
            "log_location":"(write) log_w_sql_orc (line 1813)",
            "pass":true
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.RuntimeException: Error while encoding: java.lang.RuntimeException: java.lang.Integer is not a valid external type for schema of smallint",
            "log_location":"(write) log_w_df_avro (line 3972)",
            "pass":true
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws16; line 1 pos 14;",
            "write_value":"-2147483647, find exception: Error in query: Table not found: ws16; line 1 pos 12;",
            "log_location":"(read) log_w_sql_r_df_avro (line 204)",
            "pass":true
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.RuntimeException: Error while encoding: java.lang.RuntimeException: java.lang.Integer is not a valid external type for schema of smallint",
            "log_location":"(write) log_w_df_parquet (line 11567)",
            "pass":true
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"-2147483647, find exception: java.lang.ArithmeticException: Casting -2147483647 to smallint causes overflow",
            "log_location":"(write) log_w_sql_parquet (line 1814)",
            "pass":true
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws16; line 1 pos 14;",
            "write_value":"No output, find exception: java.lang.RuntimeException: Error while encoding: java.lang.RuntimeException: java.lang.Integer is not a valid external type for schema of smallint",
            "log_location":"(read) log_w_df_r_df_parquet (line 382)",
            "pass":true
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"-2147483647, find exception: java.lang.ArithmeticException: Casting -2147483647 to smallint causes overflow",
            "log_location":"(write) log_w_sql_orc (line 1813)",
            "pass":true
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"-2147483647, find exception: java.lang.ArithmeticException: Casting -2147483647 to smallint causes overflow",
            "log_location":"(write) log_w_sql_parquet (line 1814)",
            "pass":true
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws16; line 1 pos 14;",
            "write_value":"No output, find exception: java.lang.RuntimeException: Error while encoding: java.lang.RuntimeException: java.lang.Integer is not a valid external type for schema of smallint",
            "log_location":"(read) log_w_df_r_df_avro (line 2068)",
            "pass":true
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.RuntimeException: Error while encoding: java.lang.RuntimeException: java.lang.Integer is not a valid external type for schema of smallint",
            "log_location":"(write) log_w_df_orc (line 11542)",
            "pass":true
        }
    },
    "17":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"1",
            "write_value":"1",
            "pass":false
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws17; line 1 pos 14;",
            "write_value":"No output, find exception: java.lang.NumberFormatException: For input string: \"1.1\"",
            "log_location":"(read) log_w_df_r_df_orc (line 420)",
            "pass":true
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"1",
            "write_value":"1",
            "pass":false
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.NumberFormatException: For input string: \"1.1\"",
            "log_location":"(write) log_w_df_avro (line 4447)",
            "pass":true
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"1",
            "write_value":"1",
            "pass":false
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.NumberFormatException: For input string: \"1.1\"",
            "log_location":"(write) log_w_df_parquet (line 12042)",
            "pass":true
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"1",
            "write_value":"1",
            "pass":false
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws17; line 1 pos 14;",
            "write_value":"No output, find exception: java.lang.NumberFormatException: For input string: \"1.1\"",
            "log_location":"(read) log_w_df_r_df_parquet (line 422)",
            "pass":true
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"1",
            "write_value":"1",
            "pass":false
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"1",
            "write_value":"1",
            "pass":false
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws17; line 1 pos 14;",
            "write_value":"No output, find exception: java.lang.NumberFormatException: For input string: \"1.1\"",
            "log_location":"(read) log_w_df_r_df_avro (line 2108)",
            "pass":true
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.NumberFormatException: For input string: \"1.1\"",
            "log_location":"(write) log_w_df_orc (line 12039)",
            "pass":true
        }
    },
    "23":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws23; line 1 pos 14;",
            "write_value":"No output, find exception: java.lang.ArithmeticException: / by zero",
            "log_location":"(read) log_w_df_r_df_orc (line 500)",
            "pass":true
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: / by zero",
            "log_location":"(write) log_w_df_avro (line 4571)",
            "pass":true
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: / by zero",
            "log_location":"(write) log_w_df_parquet (line 12166)",
            "pass":true
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws23; line 1 pos 14;",
            "write_value":"No output, find exception: java.lang.ArithmeticException: / by zero",
            "log_location":"(read) log_w_df_r_df_parquet (line 502)",
            "pass":true
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws23; line 1 pos 14;",
            "write_value":"No output, find exception: java.lang.ArithmeticException: / by zero",
            "log_location":"(read) log_w_df_r_df_avro (line 2190)",
            "pass":true
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: / by zero",
            "log_location":"(write) log_w_df_orc (line 12163)",
            "pass":true
        }
    },
    "24":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"1",
            "write_value":"1.1",
            "pass":false
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws24; line 1 pos 14;",
            "write_value":"No output, find exception: java.lang.RuntimeException: Error while encoding: java.lang.RuntimeException: java.lang.Double is not a valid external type for schema of int",
            "log_location":"(read) log_w_df_r_df_orc (line 540)",
            "pass":true
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"1",
            "write_value":"1.1",
            "pass":false
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.RuntimeException: Error while encoding: java.lang.RuntimeException: java.lang.Double is not a valid external type for schema of int",
            "log_location":"(write) log_w_df_avro (line 4603)",
            "pass":true
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"1",
            "write_value":"1.1",
            "pass":false
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.RuntimeException: Error while encoding: java.lang.RuntimeException: java.lang.Double is not a valid external type for schema of int",
            "log_location":"(write) log_w_df_parquet (line 12198)",
            "pass":true
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"1",
            "write_value":"1.1",
            "pass":false
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws24; line 1 pos 14;",
            "write_value":"No output, find exception: java.lang.RuntimeException: Error while encoding: java.lang.RuntimeException: java.lang.Double is not a valid external type for schema of int",
            "log_location":"(read) log_w_df_r_df_parquet (line 542)",
            "pass":true
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"1",
            "write_value":"1.1",
            "pass":false
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"1",
            "write_value":"1.1",
            "pass":false
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws24; line 1 pos 14;",
            "write_value":"No output, find exception: java.lang.RuntimeException: Error while encoding: java.lang.RuntimeException: java.lang.Double is not a valid external type for schema of int",
            "log_location":"(read) log_w_df_r_df_avro (line 2230)",
            "pass":true
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.RuntimeException: Error while encoding: java.lang.RuntimeException: java.lang.Double is not a valid external type for schema of int",
            "log_location":"(write) log_w_df_orc (line 12195)",
            "pass":true
        }
    },
    "25":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: mismatched input '<EOF>' expecting {'(', 'ADD', 'AFTER', 'ALL', 'ALTER', 'ANALYZE', 'AND', 'ANTI', 'ANY', 'ARCHIVE', 'ARRAY', 'AS', 'ASC', 'AT', 'AUTHORIZATION', 'BETWEEN', 'BOTH', 'BUCKET', 'BUCKETS', 'BY', 'CACHE', 'CASCADE', 'CASE', 'CAST', 'CHANGE', 'CHECK', 'CLEAR', 'CLUSTER', 'CLUSTERED', 'CODEGEN', 'COLLATE', 'COLLECTION', 'COLUMN', 'COLUMNS', 'COMMENT', 'COMMIT', 'COMPACT', 'COMPACTIONS', 'COMPUTE', 'CONCATENATE', 'CONSTRAINT', 'COST', 'CREATE', 'CROSS', 'CUBE', 'CURRENT', 'CURRENT_DATE', 'CURRENT_TIME', 'CURRENT_TIMESTAMP', 'CURRENT_USER', 'DAY', 'DATA', 'DATABASE', DATABASES, 'DBPROPERTIES', 'DEFINED', 'DELETE', 'DELIMITED', 'DESC', 'DESCRIBE', 'DFS', 'DIRECTORIES', 'DIRECTORY', 'DISTINCT', 'DISTRIBUTE', 'DIV', 'DROP', 'ELSE', 'END', 'ESCAPE', 'ESCAPED', 'EXCEPT', 'EXCHANGE', 'EXISTS', 'EXPLAIN', 'EXPORT', 'EXTENDED', 'EXTERNAL', 'EXTRACT', 'FALSE', 'FETCH', 'FIELDS', 'FILTER', 'FILEFORMAT', 'FIRST', 'FOLLOWING', 'FOR', 'FOREIGN', 'FORMAT', 'FORMATTED', 'FROM', 'FULL', 'FUNCTION', 'FUNCTIONS', 'GLOBAL', 'GRANT', 'GROUP', 'GROUPING', 'HAVING', 'HOUR', 'IF', 'IGNORE', 'IMPORT', 'IN', 'INDEX', 'INDEXES', 'INNER', 'INPATH', 'INPUTFORMAT', 'INSERT', 'INTERSECT', 'INTERVAL', 'INTO', 'IS', 'ITEMS', 'JOIN', 'KEYS', 'LAST', 'LATERAL', 'LAZY', 'LEADING', 'LEFT', 'LIKE', 'LIMIT', 'LINES', 'LIST', 'LOAD', 'LOCAL', 'LOCATION', 'LOCK', 'LOCKS', 'LOGICAL', 'MACRO', 'MAP', 'MATCHED', 'MERGE', 'MINUTE', 'MONTH', 'MSCK', 'NAMESPACE', 'NAMESPACES', 'NATURAL', 'NO', NOT, 'NULL', 'NULLS', 'OF', 'ON', 'ONLY', 'OPTION', 'OPTIONS', 'OR', 'ORDER', 'OUT', 'OUTER', 'OUTPUTFORMAT', 'OVER', 'OVERLAPS', 'OVERLAY', 'OVERWRITE', 'PARTITION', 'PARTITIONED', 'PARTITIONS', 'PERCENT', 'PIVOT', 'PLACING', 'POSITION', 'PRECEDING', 'PRIMARY', 'PRINCIPALS', 'PROPERTIES', 'PURGE', 'QUERY', 'RANGE', 'RECORDREADER', 'RECORDWRITER', 'RECOVER', 'REDUCE', 'REFERENCES', 'REFRESH', 'RENAME', 'REPAIR', 'REPLACE', 'RESET', 'RESPECT', 'RESTRICT', 'REVOKE', 'RIGHT', RLIKE, 'ROLE', 'ROLES', 'ROLLBACK', 'ROLLUP', 'ROW', 'ROWS', 'SECOND', 'SCHEMA', 'SELECT', 'SEMI', 'SEPARATED', 'SERDE', 'SERDEPROPERTIES', 'SESSION_USER', 'SET', 'MINUS', 'SETS', 'SHOW', 'SKEWED', 'SOME', 'SORT', 'SORTED', 'START', 'STATISTICS', 'STORED', 'STRATIFY', 'STRUCT', 'SUBSTR', 'SUBSTRING', 'SYNC', 'TABLE', 'TABLES', 'TABLESAMPLE', 'TBLPROPERTIES', TEMPORARY, 'TERMINATED', 'THEN', 'TIME', 'TO', 'TOUCH', 'TRAILING', 'TRANSACTION', 'TRANSACTIONS', 'TRANSFORM', 'TRIM', 'TRUE', 'TRUNCATE', 'TRY_CAST', 'TYPE', 'UNARCHIVE', 'UNBOUNDED', 'UNCACHE', 'UNION', 'UNIQUE', 'UNKNOWN', 'UNLOCK', 'UNSET', 'UPDATE', 'USE', 'USER', 'USING', 'VALUES', 'VIEW', 'VIEWS', 'WHEN', 'WHERE', 'WINDOW', 'WITH', 'YEAR', 'ZONE', '+', '-', '*', '~', STRING, BIGINT_LITERAL, SMALLINT_LITERAL, TINYINT_LITERAL, INTEGER_VALUE, EXPONENT_VALUE, DECIMAL_VALUE, FLOAT_LITERAL, DOUBLE_LITERAL, BIGDECIMAL_LITERAL, IDENTIFIER, BACKQUOTED_IDENTIFIER}(line 1, pos 28)",
            "log_location":"(write) log_w_sql_avro (line 1160)",
            "pass":true
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws25; line 1 pos 14;",
            "write_value":"No output, find exception: <console>:1: error: illegal start of simple expression",
            "log_location":"(read) log_w_df_r_df_orc (line 580)",
            "pass":true
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: mismatched input '<EOF>' expecting {'(', 'ADD', 'AFTER', 'ALL', 'ALTER', 'ANALYZE', 'AND', 'ANTI', 'ANY', 'ARCHIVE', 'ARRAY', 'AS', 'ASC', 'AT', 'AUTHORIZATION', 'BETWEEN', 'BOTH', 'BUCKET', 'BUCKETS', 'BY', 'CACHE', 'CASCADE', 'CASE', 'CAST', 'CHANGE', 'CHECK', 'CLEAR', 'CLUSTER', 'CLUSTERED', 'CODEGEN', 'COLLATE', 'COLLECTION', 'COLUMN', 'COLUMNS', 'COMMENT', 'COMMIT', 'COMPACT', 'COMPACTIONS', 'COMPUTE', 'CONCATENATE', 'CONSTRAINT', 'COST', 'CREATE', 'CROSS', 'CUBE', 'CURRENT', 'CURRENT_DATE', 'CURRENT_TIME', 'CURRENT_TIMESTAMP', 'CURRENT_USER', 'DAY', 'DATA', 'DATABASE', DATABASES, 'DBPROPERTIES', 'DEFINED', 'DELETE', 'DELIMITED', 'DESC', 'DESCRIBE', 'DFS', 'DIRECTORIES', 'DIRECTORY', 'DISTINCT', 'DISTRIBUTE', 'DIV', 'DROP', 'ELSE', 'END', 'ESCAPE', 'ESCAPED', 'EXCEPT', 'EXCHANGE', 'EXISTS', 'EXPLAIN', 'EXPORT', 'EXTENDED', 'EXTERNAL', 'EXTRACT', 'FALSE', 'FETCH', 'FIELDS', 'FILTER', 'FILEFORMAT', 'FIRST', 'FOLLOWING', 'FOR', 'FOREIGN', 'FORMAT', 'FORMATTED', 'FROM', 'FULL', 'FUNCTION', 'FUNCTIONS', 'GLOBAL', 'GRANT', 'GROUP', 'GROUPING', 'HAVING', 'HOUR', 'IF', 'IGNORE', 'IMPORT', 'IN', 'INDEX', 'INDEXES', 'INNER', 'INPATH', 'INPUTFORMAT', 'INSERT', 'INTERSECT', 'INTERVAL', 'INTO', 'IS', 'ITEMS', 'JOIN', 'KEYS', 'LAST', 'LATERAL', 'LAZY', 'LEADING', 'LEFT', 'LIKE', 'LIMIT', 'LINES', 'LIST', 'LOAD', 'LOCAL', 'LOCATION', 'LOCK', 'LOCKS', 'LOGICAL', 'MACRO', 'MAP', 'MATCHED', 'MERGE', 'MINUTE', 'MONTH', 'MSCK', 'NAMESPACE', 'NAMESPACES', 'NATURAL', 'NO', NOT, 'NULL', 'NULLS', 'OF', 'ON', 'ONLY', 'OPTION', 'OPTIONS', 'OR', 'ORDER', 'OUT', 'OUTER', 'OUTPUTFORMAT', 'OVER', 'OVERLAPS', 'OVERLAY', 'OVERWRITE', 'PARTITION', 'PARTITIONED', 'PARTITIONS', 'PERCENT', 'PIVOT', 'PLACING', 'POSITION', 'PRECEDING', 'PRIMARY', 'PRINCIPALS', 'PROPERTIES', 'PURGE', 'QUERY', 'RANGE', 'RECORDREADER', 'RECORDWRITER', 'RECOVER', 'REDUCE', 'REFERENCES', 'REFRESH', 'RENAME', 'REPAIR', 'REPLACE', 'RESET', 'RESPECT', 'RESTRICT', 'REVOKE', 'RIGHT', RLIKE, 'ROLE', 'ROLES', 'ROLLBACK', 'ROLLUP', 'ROW', 'ROWS', 'SECOND', 'SCHEMA', 'SELECT', 'SEMI', 'SEPARATED', 'SERDE', 'SERDEPROPERTIES', 'SESSION_USER', 'SET', 'MINUS', 'SETS', 'SHOW', 'SKEWED', 'SOME', 'SORT', 'SORTED', 'START', 'STATISTICS', 'STORED', 'STRATIFY', 'STRUCT', 'SUBSTR', 'SUBSTRING', 'SYNC', 'TABLE', 'TABLES', 'TABLESAMPLE', 'TBLPROPERTIES', TEMPORARY, 'TERMINATED', 'THEN', 'TIME', 'TO', 'TOUCH', 'TRAILING', 'TRANSACTION', 'TRANSACTIONS', 'TRANSFORM', 'TRIM', 'TRUE', 'TRUNCATE', 'TRY_CAST', 'TYPE', 'UNARCHIVE', 'UNBOUNDED', 'UNCACHE', 'UNION', 'UNIQUE', 'UNKNOWN', 'UNLOCK', 'UNSET', 'UPDATE', 'USE', 'USER', 'USING', 'VALUES', 'VIEW', 'VIEWS', 'WHEN', 'WHERE', 'WINDOW', 'WITH', 'YEAR', 'ZONE', '+', '-', '*', '~', STRING, BIGINT_LITERAL, SMALLINT_LITERAL, TINYINT_LITERAL, INTEGER_VALUE, EXPONENT_VALUE, DECIMAL_VALUE, FLOAT_LITERAL, DOUBLE_LITERAL, BIGDECIMAL_LITERAL, IDENTIFIER, BACKQUOTED_IDENTIFIER}(line 1, pos 28)",
            "log_location":"(write) log_w_sql_orc (line 2155)",
            "pass":true
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: <console>:1: error: illegal start of simple expression",
            "log_location":"(write) log_w_df_avro (line 5078)",
            "pass":true
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: mismatched input '<EOF>' expecting {'(', 'ADD', 'AFTER', 'ALL', 'ALTER', 'ANALYZE', 'AND', 'ANTI', 'ANY', 'ARCHIVE', 'ARRAY', 'AS', 'ASC', 'AT', 'AUTHORIZATION', 'BETWEEN', 'BOTH', 'BUCKET', 'BUCKETS', 'BY', 'CACHE', 'CASCADE', 'CASE', 'CAST', 'CHANGE', 'CHECK', 'CLEAR', 'CLUSTER', 'CLUSTERED', 'CODEGEN', 'COLLATE', 'COLLECTION', 'COLUMN', 'COLUMNS', 'COMMENT', 'COMMIT', 'COMPACT', 'COMPACTIONS', 'COMPUTE', 'CONCATENATE', 'CONSTRAINT', 'COST', 'CREATE', 'CROSS', 'CUBE', 'CURRENT', 'CURRENT_DATE', 'CURRENT_TIME', 'CURRENT_TIMESTAMP', 'CURRENT_USER', 'DAY', 'DATA', 'DATABASE', DATABASES, 'DBPROPERTIES', 'DEFINED', 'DELETE', 'DELIMITED', 'DESC', 'DESCRIBE', 'DFS', 'DIRECTORIES', 'DIRECTORY', 'DISTINCT', 'DISTRIBUTE', 'DIV', 'DROP', 'ELSE', 'END', 'ESCAPE', 'ESCAPED', 'EXCEPT', 'EXCHANGE', 'EXISTS', 'EXPLAIN', 'EXPORT', 'EXTENDED', 'EXTERNAL', 'EXTRACT', 'FALSE', 'FETCH', 'FIELDS', 'FILTER', 'FILEFORMAT', 'FIRST', 'FOLLOWING', 'FOR', 'FOREIGN', 'FORMAT', 'FORMATTED', 'FROM', 'FULL', 'FUNCTION', 'FUNCTIONS', 'GLOBAL', 'GRANT', 'GROUP', 'GROUPING', 'HAVING', 'HOUR', 'IF', 'IGNORE', 'IMPORT', 'IN', 'INDEX', 'INDEXES', 'INNER', 'INPATH', 'INPUTFORMAT', 'INSERT', 'INTERSECT', 'INTERVAL', 'INTO', 'IS', 'ITEMS', 'JOIN', 'KEYS', 'LAST', 'LATERAL', 'LAZY', 'LEADING', 'LEFT', 'LIKE', 'LIMIT', 'LINES', 'LIST', 'LOAD', 'LOCAL', 'LOCATION', 'LOCK', 'LOCKS', 'LOGICAL', 'MACRO', 'MAP', 'MATCHED', 'MERGE', 'MINUTE', 'MONTH', 'MSCK', 'NAMESPACE', 'NAMESPACES', 'NATURAL', 'NO', NOT, 'NULL', 'NULLS', 'OF', 'ON', 'ONLY', 'OPTION', 'OPTIONS', 'OR', 'ORDER', 'OUT', 'OUTER', 'OUTPUTFORMAT', 'OVER', 'OVERLAPS', 'OVERLAY', 'OVERWRITE', 'PARTITION', 'PARTITIONED', 'PARTITIONS', 'PERCENT', 'PIVOT', 'PLACING', 'POSITION', 'PRECEDING', 'PRIMARY', 'PRINCIPALS', 'PROPERTIES', 'PURGE', 'QUERY', 'RANGE', 'RECORDREADER', 'RECORDWRITER', 'RECOVER', 'REDUCE', 'REFERENCES', 'REFRESH', 'RENAME', 'REPAIR', 'REPLACE', 'RESET', 'RESPECT', 'RESTRICT', 'REVOKE', 'RIGHT', RLIKE, 'ROLE', 'ROLES', 'ROLLBACK', 'ROLLUP', 'ROW', 'ROWS', 'SECOND', 'SCHEMA', 'SELECT', 'SEMI', 'SEPARATED', 'SERDE', 'SERDEPROPERTIES', 'SESSION_USER', 'SET', 'MINUS', 'SETS', 'SHOW', 'SKEWED', 'SOME', 'SORT', 'SORTED', 'START', 'STATISTICS', 'STORED', 'STRATIFY', 'STRUCT', 'SUBSTR', 'SUBSTRING', 'SYNC', 'TABLE', 'TABLES', 'TABLESAMPLE', 'TBLPROPERTIES', TEMPORARY, 'TERMINATED', 'THEN', 'TIME', 'TO', 'TOUCH', 'TRAILING', 'TRANSACTION', 'TRANSACTIONS', 'TRANSFORM', 'TRIM', 'TRUE', 'TRUNCATE', 'TRY_CAST', 'TYPE', 'UNARCHIVE', 'UNBOUNDED', 'UNCACHE', 'UNION', 'UNIQUE', 'UNKNOWN', 'UNLOCK', 'UNSET', 'UPDATE', 'USE', 'USER', 'USING', 'VALUES', 'VIEW', 'VIEWS', 'WHEN', 'WHERE', 'WINDOW', 'WITH', 'YEAR', 'ZONE', '+', '-', '*', '~', STRING, BIGINT_LITERAL, SMALLINT_LITERAL, TINYINT_LITERAL, INTEGER_VALUE, EXPONENT_VALUE, DECIMAL_VALUE, FLOAT_LITERAL, DOUBLE_LITERAL, BIGDECIMAL_LITERAL, IDENTIFIER, BACKQUOTED_IDENTIFIER}(line 1, pos 28)",
            "log_location":"(write) log_w_sql_avro (line 1160)",
            "pass":true
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: <console>:1: error: illegal start of simple expression",
            "log_location":"(write) log_w_df_parquet (line 12673)",
            "pass":true
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: mismatched input '<EOF>' expecting {'(', 'ADD', 'AFTER', 'ALL', 'ALTER', 'ANALYZE', 'AND', 'ANTI', 'ANY', 'ARCHIVE', 'ARRAY', 'AS', 'ASC', 'AT', 'AUTHORIZATION', 'BETWEEN', 'BOTH', 'BUCKET', 'BUCKETS', 'BY', 'CACHE', 'CASCADE', 'CASE', 'CAST', 'CHANGE', 'CHECK', 'CLEAR', 'CLUSTER', 'CLUSTERED', 'CODEGEN', 'COLLATE', 'COLLECTION', 'COLUMN', 'COLUMNS', 'COMMENT', 'COMMIT', 'COMPACT', 'COMPACTIONS', 'COMPUTE', 'CONCATENATE', 'CONSTRAINT', 'COST', 'CREATE', 'CROSS', 'CUBE', 'CURRENT', 'CURRENT_DATE', 'CURRENT_TIME', 'CURRENT_TIMESTAMP', 'CURRENT_USER', 'DAY', 'DATA', 'DATABASE', DATABASES, 'DBPROPERTIES', 'DEFINED', 'DELETE', 'DELIMITED', 'DESC', 'DESCRIBE', 'DFS', 'DIRECTORIES', 'DIRECTORY', 'DISTINCT', 'DISTRIBUTE', 'DIV', 'DROP', 'ELSE', 'END', 'ESCAPE', 'ESCAPED', 'EXCEPT', 'EXCHANGE', 'EXISTS', 'EXPLAIN', 'EXPORT', 'EXTENDED', 'EXTERNAL', 'EXTRACT', 'FALSE', 'FETCH', 'FIELDS', 'FILTER', 'FILEFORMAT', 'FIRST', 'FOLLOWING', 'FOR', 'FOREIGN', 'FORMAT', 'FORMATTED', 'FROM', 'FULL', 'FUNCTION', 'FUNCTIONS', 'GLOBAL', 'GRANT', 'GROUP', 'GROUPING', 'HAVING', 'HOUR', 'IF', 'IGNORE', 'IMPORT', 'IN', 'INDEX', 'INDEXES', 'INNER', 'INPATH', 'INPUTFORMAT', 'INSERT', 'INTERSECT', 'INTERVAL', 'INTO', 'IS', 'ITEMS', 'JOIN', 'KEYS', 'LAST', 'LATERAL', 'LAZY', 'LEADING', 'LEFT', 'LIKE', 'LIMIT', 'LINES', 'LIST', 'LOAD', 'LOCAL', 'LOCATION', 'LOCK', 'LOCKS', 'LOGICAL', 'MACRO', 'MAP', 'MATCHED', 'MERGE', 'MINUTE', 'MONTH', 'MSCK', 'NAMESPACE', 'NAMESPACES', 'NATURAL', 'NO', NOT, 'NULL', 'NULLS', 'OF', 'ON', 'ONLY', 'OPTION', 'OPTIONS', 'OR', 'ORDER', 'OUT', 'OUTER', 'OUTPUTFORMAT', 'OVER', 'OVERLAPS', 'OVERLAY', 'OVERWRITE', 'PARTITION', 'PARTITIONED', 'PARTITIONS', 'PERCENT', 'PIVOT', 'PLACING', 'POSITION', 'PRECEDING', 'PRIMARY', 'PRINCIPALS', 'PROPERTIES', 'PURGE', 'QUERY', 'RANGE', 'RECORDREADER', 'RECORDWRITER', 'RECOVER', 'REDUCE', 'REFERENCES', 'REFRESH', 'RENAME', 'REPAIR', 'REPLACE', 'RESET', 'RESPECT', 'RESTRICT', 'REVOKE', 'RIGHT', RLIKE, 'ROLE', 'ROLES', 'ROLLBACK', 'ROLLUP', 'ROW', 'ROWS', 'SECOND', 'SCHEMA', 'SELECT', 'SEMI', 'SEPARATED', 'SERDE', 'SERDEPROPERTIES', 'SESSION_USER', 'SET', 'MINUS', 'SETS', 'SHOW', 'SKEWED', 'SOME', 'SORT', 'SORTED', 'START', 'STATISTICS', 'STORED', 'STRATIFY', 'STRUCT', 'SUBSTR', 'SUBSTRING', 'SYNC', 'TABLE', 'TABLES', 'TABLESAMPLE', 'TBLPROPERTIES', TEMPORARY, 'TERMINATED', 'THEN', 'TIME', 'TO', 'TOUCH', 'TRAILING', 'TRANSACTION', 'TRANSACTIONS', 'TRANSFORM', 'TRIM', 'TRUE', 'TRUNCATE', 'TRY_CAST', 'TYPE', 'UNARCHIVE', 'UNBOUNDED', 'UNCACHE', 'UNION', 'UNIQUE', 'UNKNOWN', 'UNLOCK', 'UNSET', 'UPDATE', 'USE', 'USER', 'USING', 'VALUES', 'VIEW', 'VIEWS', 'WHEN', 'WHERE', 'WINDOW', 'WITH', 'YEAR', 'ZONE', '+', '-', '*', '~', STRING, BIGINT_LITERAL, SMALLINT_LITERAL, TINYINT_LITERAL, INTEGER_VALUE, EXPONENT_VALUE, DECIMAL_VALUE, FLOAT_LITERAL, DOUBLE_LITERAL, BIGDECIMAL_LITERAL, IDENTIFIER, BACKQUOTED_IDENTIFIER}(line 1, pos 28)",
            "log_location":"(write) log_w_sql_parquet (line 2156)",
            "pass":true
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws25; line 1 pos 14;",
            "write_value":"No output, find exception: <console>:1: error: illegal start of simple expression",
            "log_location":"(read) log_w_df_r_df_parquet (line 582)",
            "pass":true
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: mismatched input '<EOF>' expecting {'(', 'ADD', 'AFTER', 'ALL', 'ALTER', 'ANALYZE', 'AND', 'ANTI', 'ANY', 'ARCHIVE', 'ARRAY', 'AS', 'ASC', 'AT', 'AUTHORIZATION', 'BETWEEN', 'BOTH', 'BUCKET', 'BUCKETS', 'BY', 'CACHE', 'CASCADE', 'CASE', 'CAST', 'CHANGE', 'CHECK', 'CLEAR', 'CLUSTER', 'CLUSTERED', 'CODEGEN', 'COLLATE', 'COLLECTION', 'COLUMN', 'COLUMNS', 'COMMENT', 'COMMIT', 'COMPACT', 'COMPACTIONS', 'COMPUTE', 'CONCATENATE', 'CONSTRAINT', 'COST', 'CREATE', 'CROSS', 'CUBE', 'CURRENT', 'CURRENT_DATE', 'CURRENT_TIME', 'CURRENT_TIMESTAMP', 'CURRENT_USER', 'DAY', 'DATA', 'DATABASE', DATABASES, 'DBPROPERTIES', 'DEFINED', 'DELETE', 'DELIMITED', 'DESC', 'DESCRIBE', 'DFS', 'DIRECTORIES', 'DIRECTORY', 'DISTINCT', 'DISTRIBUTE', 'DIV', 'DROP', 'ELSE', 'END', 'ESCAPE', 'ESCAPED', 'EXCEPT', 'EXCHANGE', 'EXISTS', 'EXPLAIN', 'EXPORT', 'EXTENDED', 'EXTERNAL', 'EXTRACT', 'FALSE', 'FETCH', 'FIELDS', 'FILTER', 'FILEFORMAT', 'FIRST', 'FOLLOWING', 'FOR', 'FOREIGN', 'FORMAT', 'FORMATTED', 'FROM', 'FULL', 'FUNCTION', 'FUNCTIONS', 'GLOBAL', 'GRANT', 'GROUP', 'GROUPING', 'HAVING', 'HOUR', 'IF', 'IGNORE', 'IMPORT', 'IN', 'INDEX', 'INDEXES', 'INNER', 'INPATH', 'INPUTFORMAT', 'INSERT', 'INTERSECT', 'INTERVAL', 'INTO', 'IS', 'ITEMS', 'JOIN', 'KEYS', 'LAST', 'LATERAL', 'LAZY', 'LEADING', 'LEFT', 'LIKE', 'LIMIT', 'LINES', 'LIST', 'LOAD', 'LOCAL', 'LOCATION', 'LOCK', 'LOCKS', 'LOGICAL', 'MACRO', 'MAP', 'MATCHED', 'MERGE', 'MINUTE', 'MONTH', 'MSCK', 'NAMESPACE', 'NAMESPACES', 'NATURAL', 'NO', NOT, 'NULL', 'NULLS', 'OF', 'ON', 'ONLY', 'OPTION', 'OPTIONS', 'OR', 'ORDER', 'OUT', 'OUTER', 'OUTPUTFORMAT', 'OVER', 'OVERLAPS', 'OVERLAY', 'OVERWRITE', 'PARTITION', 'PARTITIONED', 'PARTITIONS', 'PERCENT', 'PIVOT', 'PLACING', 'POSITION', 'PRECEDING', 'PRIMARY', 'PRINCIPALS', 'PROPERTIES', 'PURGE', 'QUERY', 'RANGE', 'RECORDREADER', 'RECORDWRITER', 'RECOVER', 'REDUCE', 'REFERENCES', 'REFRESH', 'RENAME', 'REPAIR', 'REPLACE', 'RESET', 'RESPECT', 'RESTRICT', 'REVOKE', 'RIGHT', RLIKE, 'ROLE', 'ROLES', 'ROLLBACK', 'ROLLUP', 'ROW', 'ROWS', 'SECOND', 'SCHEMA', 'SELECT', 'SEMI', 'SEPARATED', 'SERDE', 'SERDEPROPERTIES', 'SESSION_USER', 'SET', 'MINUS', 'SETS', 'SHOW', 'SKEWED', 'SOME', 'SORT', 'SORTED', 'START', 'STATISTICS', 'STORED', 'STRATIFY', 'STRUCT', 'SUBSTR', 'SUBSTRING', 'SYNC', 'TABLE', 'TABLES', 'TABLESAMPLE', 'TBLPROPERTIES', TEMPORARY, 'TERMINATED', 'THEN', 'TIME', 'TO', 'TOUCH', 'TRAILING', 'TRANSACTION', 'TRANSACTIONS', 'TRANSFORM', 'TRIM', 'TRUE', 'TRUNCATE', 'TRY_CAST', 'TYPE', 'UNARCHIVE', 'UNBOUNDED', 'UNCACHE', 'UNION', 'UNIQUE', 'UNKNOWN', 'UNLOCK', 'UNSET', 'UPDATE', 'USE', 'USER', 'USING', 'VALUES', 'VIEW', 'VIEWS', 'WHEN', 'WHERE', 'WINDOW', 'WITH', 'YEAR', 'ZONE', '+', '-', '*', '~', STRING, BIGINT_LITERAL, SMALLINT_LITERAL, TINYINT_LITERAL, INTEGER_VALUE, EXPONENT_VALUE, DECIMAL_VALUE, FLOAT_LITERAL, DOUBLE_LITERAL, BIGDECIMAL_LITERAL, IDENTIFIER, BACKQUOTED_IDENTIFIER}(line 1, pos 28)",
            "log_location":"(write) log_w_sql_orc (line 2155)",
            "pass":true
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: mismatched input '<EOF>' expecting {'(', 'ADD', 'AFTER', 'ALL', 'ALTER', 'ANALYZE', 'AND', 'ANTI', 'ANY', 'ARCHIVE', 'ARRAY', 'AS', 'ASC', 'AT', 'AUTHORIZATION', 'BETWEEN', 'BOTH', 'BUCKET', 'BUCKETS', 'BY', 'CACHE', 'CASCADE', 'CASE', 'CAST', 'CHANGE', 'CHECK', 'CLEAR', 'CLUSTER', 'CLUSTERED', 'CODEGEN', 'COLLATE', 'COLLECTION', 'COLUMN', 'COLUMNS', 'COMMENT', 'COMMIT', 'COMPACT', 'COMPACTIONS', 'COMPUTE', 'CONCATENATE', 'CONSTRAINT', 'COST', 'CREATE', 'CROSS', 'CUBE', 'CURRENT', 'CURRENT_DATE', 'CURRENT_TIME', 'CURRENT_TIMESTAMP', 'CURRENT_USER', 'DAY', 'DATA', 'DATABASE', DATABASES, 'DBPROPERTIES', 'DEFINED', 'DELETE', 'DELIMITED', 'DESC', 'DESCRIBE', 'DFS', 'DIRECTORIES', 'DIRECTORY', 'DISTINCT', 'DISTRIBUTE', 'DIV', 'DROP', 'ELSE', 'END', 'ESCAPE', 'ESCAPED', 'EXCEPT', 'EXCHANGE', 'EXISTS', 'EXPLAIN', 'EXPORT', 'EXTENDED', 'EXTERNAL', 'EXTRACT', 'FALSE', 'FETCH', 'FIELDS', 'FILTER', 'FILEFORMAT', 'FIRST', 'FOLLOWING', 'FOR', 'FOREIGN', 'FORMAT', 'FORMATTED', 'FROM', 'FULL', 'FUNCTION', 'FUNCTIONS', 'GLOBAL', 'GRANT', 'GROUP', 'GROUPING', 'HAVING', 'HOUR', 'IF', 'IGNORE', 'IMPORT', 'IN', 'INDEX', 'INDEXES', 'INNER', 'INPATH', 'INPUTFORMAT', 'INSERT', 'INTERSECT', 'INTERVAL', 'INTO', 'IS', 'ITEMS', 'JOIN', 'KEYS', 'LAST', 'LATERAL', 'LAZY', 'LEADING', 'LEFT', 'LIKE', 'LIMIT', 'LINES', 'LIST', 'LOAD', 'LOCAL', 'LOCATION', 'LOCK', 'LOCKS', 'LOGICAL', 'MACRO', 'MAP', 'MATCHED', 'MERGE', 'MINUTE', 'MONTH', 'MSCK', 'NAMESPACE', 'NAMESPACES', 'NATURAL', 'NO', NOT, 'NULL', 'NULLS', 'OF', 'ON', 'ONLY', 'OPTION', 'OPTIONS', 'OR', 'ORDER', 'OUT', 'OUTER', 'OUTPUTFORMAT', 'OVER', 'OVERLAPS', 'OVERLAY', 'OVERWRITE', 'PARTITION', 'PARTITIONED', 'PARTITIONS', 'PERCENT', 'PIVOT', 'PLACING', 'POSITION', 'PRECEDING', 'PRIMARY', 'PRINCIPALS', 'PROPERTIES', 'PURGE', 'QUERY', 'RANGE', 'RECORDREADER', 'RECORDWRITER', 'RECOVER', 'REDUCE', 'REFERENCES', 'REFRESH', 'RENAME', 'REPAIR', 'REPLACE', 'RESET', 'RESPECT', 'RESTRICT', 'REVOKE', 'RIGHT', RLIKE, 'ROLE', 'ROLES', 'ROLLBACK', 'ROLLUP', 'ROW', 'ROWS', 'SECOND', 'SCHEMA', 'SELECT', 'SEMI', 'SEPARATED', 'SERDE', 'SERDEPROPERTIES', 'SESSION_USER', 'SET', 'MINUS', 'SETS', 'SHOW', 'SKEWED', 'SOME', 'SORT', 'SORTED', 'START', 'STATISTICS', 'STORED', 'STRATIFY', 'STRUCT', 'SUBSTR', 'SUBSTRING', 'SYNC', 'TABLE', 'TABLES', 'TABLESAMPLE', 'TBLPROPERTIES', TEMPORARY, 'TERMINATED', 'THEN', 'TIME', 'TO', 'TOUCH', 'TRAILING', 'TRANSACTION', 'TRANSACTIONS', 'TRANSFORM', 'TRIM', 'TRUE', 'TRUNCATE', 'TRY_CAST', 'TYPE', 'UNARCHIVE', 'UNBOUNDED', 'UNCACHE', 'UNION', 'UNIQUE', 'UNKNOWN', 'UNLOCK', 'UNSET', 'UPDATE', 'USE', 'USER', 'USING', 'VALUES', 'VIEW', 'VIEWS', 'WHEN', 'WHERE', 'WINDOW', 'WITH', 'YEAR', 'ZONE', '+', '-', '*', '~', STRING, BIGINT_LITERAL, SMALLINT_LITERAL, TINYINT_LITERAL, INTEGER_VALUE, EXPONENT_VALUE, DECIMAL_VALUE, FLOAT_LITERAL, DOUBLE_LITERAL, BIGDECIMAL_LITERAL, IDENTIFIER, BACKQUOTED_IDENTIFIER}(line 1, pos 28)",
            "log_location":"(write) log_w_sql_parquet (line 2156)",
            "pass":true
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws25; line 1 pos 14;",
            "write_value":"No output, find exception: <console>:1: error: illegal start of simple expression",
            "log_location":"(read) log_w_df_r_df_avro (line 2270)",
            "pass":true
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: <console>:1: error: illegal start of simple expression",
            "log_location":"(write) log_w_df_orc (line 12670)",
            "pass":true
        }
    },
    "26":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"9223372036854775808, find exception: java.lang.ArithmeticException: Casting 9223372036854775808 to int causes overflow",
            "log_location":"(write) log_w_sql_avro (line 1175)",
            "pass":true
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws26; line 1 pos 14;",
            "write_value":"No output, find exception: <console>:1: error: integer number too large",
            "log_location":"(read) log_w_df_r_df_orc (line 620)",
            "pass":true
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"9223372036854775808, find exception: java.lang.ArithmeticException: Casting 9223372036854775808 to int causes overflow",
            "log_location":"(write) log_w_sql_orc (line 2169)",
            "pass":true
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: <console>:1: error: integer number too large",
            "log_location":"(write) log_w_df_avro (line 5101)",
            "pass":true
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"9223372036854775808, find exception: java.lang.ArithmeticException: Casting 9223372036854775808 to int causes overflow",
            "log_location":"(write) log_w_sql_avro (line 1175)",
            "pass":true
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: <console>:1: error: integer number too large",
            "log_location":"(write) log_w_df_parquet (line 12696)",
            "pass":true
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"9223372036854775808, find exception: java.lang.ArithmeticException: Casting 9223372036854775808 to int causes overflow",
            "log_location":"(write) log_w_sql_parquet (line 2170)",
            "pass":true
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws26; line 1 pos 14;",
            "write_value":"No output, find exception: <console>:1: error: integer number too large",
            "log_location":"(read) log_w_df_r_df_parquet (line 622)",
            "pass":true
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"9223372036854775808, find exception: java.lang.ArithmeticException: Casting 9223372036854775808 to int causes overflow",
            "log_location":"(write) log_w_sql_orc (line 2169)",
            "pass":true
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"9223372036854775808, find exception: java.lang.ArithmeticException: Casting 9223372036854775808 to int causes overflow",
            "log_location":"(write) log_w_sql_parquet (line 2170)",
            "pass":true
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws26; line 1 pos 14;",
            "write_value":"No output, find exception: <console>:1: error: integer number too large",
            "log_location":"(read) log_w_df_r_df_avro (line 2310)",
            "pass":true
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: <console>:1: error: integer number too large",
            "log_location":"(write) log_w_df_orc (line 12693)",
            "pass":true
        }
    },
    "27":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"-9223372036854775809, find exception: java.lang.ArithmeticException: Casting -9223372036854775809 to int causes overflow",
            "log_location":"(write) log_w_sql_avro (line 1457)",
            "pass":true
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws27; line 1 pos 14;",
            "write_value":"No output, find exception: <console>:1: error: integer number too large",
            "log_location":"(read) log_w_df_r_df_orc (line 660)",
            "pass":true
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"-9223372036854775809, find exception: java.lang.ArithmeticException: Casting -9223372036854775809 to int causes overflow",
            "log_location":"(write) log_w_sql_orc (line 2450)",
            "pass":true
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: <console>:1: error: integer number too large",
            "log_location":"(write) log_w_df_avro (line 5124)",
            "pass":true
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"-9223372036854775809, find exception: java.lang.ArithmeticException: Casting -9223372036854775809 to int causes overflow",
            "log_location":"(write) log_w_sql_avro (line 1457)",
            "pass":true
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: <console>:1: error: integer number too large",
            "log_location":"(write) log_w_df_parquet (line 12719)",
            "pass":true
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"-9223372036854775809, find exception: java.lang.ArithmeticException: Casting -9223372036854775809 to int causes overflow",
            "log_location":"(write) log_w_sql_parquet (line 2451)",
            "pass":true
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws27; line 1 pos 14;",
            "write_value":"No output, find exception: <console>:1: error: integer number too large",
            "log_location":"(read) log_w_df_r_df_parquet (line 662)",
            "pass":true
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"-9223372036854775809, find exception: java.lang.ArithmeticException: Casting -9223372036854775809 to int causes overflow",
            "log_location":"(write) log_w_sql_orc (line 2450)",
            "pass":true
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"-9223372036854775809, find exception: java.lang.ArithmeticException: Casting -9223372036854775809 to int causes overflow",
            "log_location":"(write) log_w_sql_parquet (line 2451)",
            "pass":true
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws27; line 1 pos 14;",
            "write_value":"No output, find exception: <console>:1: error: integer number too large",
            "log_location":"(read) log_w_df_r_df_avro (line 2350)",
            "pass":true
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: <console>:1: error: integer number too large",
            "log_location":"(write) log_w_df_orc (line 12716)",
            "pass":true
        }
    },
    "28":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: 'InsertIntoStatement 'UnresolvedRelation [ws28], [], false, false, false",
            "log_location":"(write) log_w_sql_avro (line 1741)",
            "pass":true
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws28; line 1 pos 14;",
            "write_value":"No output, find exception: <console>:28: error: not found: value foo",
            "log_location":"(read) log_w_df_r_df_orc (line 700)",
            "pass":true
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: 'InsertIntoStatement 'UnresolvedRelation [ws28], [], false, false, false",
            "log_location":"(write) log_w_sql_orc (line 2733)",
            "pass":true
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: <console>:28: error: not found: value foo",
            "log_location":"(write) log_w_df_avro (line 5147)",
            "pass":true
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: 'InsertIntoStatement 'UnresolvedRelation [ws28], [], false, false, false",
            "log_location":"(write) log_w_sql_avro (line 1741)",
            "pass":true
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: <console>:28: error: not found: value foo",
            "log_location":"(write) log_w_df_parquet (line 12742)",
            "pass":true
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: 'InsertIntoStatement 'UnresolvedRelation [ws28], [], false, false, false",
            "log_location":"(write) log_w_sql_parquet (line 2734)",
            "pass":true
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws28; line 1 pos 14;",
            "write_value":"No output, find exception: <console>:28: error: not found: value foo",
            "log_location":"(read) log_w_df_r_df_parquet (line 702)",
            "pass":true
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: 'InsertIntoStatement 'UnresolvedRelation [ws28], [], false, false, false",
            "log_location":"(write) log_w_sql_orc (line 2733)",
            "pass":true
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: 'InsertIntoStatement 'UnresolvedRelation [ws28], [], false, false, false",
            "log_location":"(write) log_w_sql_parquet (line 2734)",
            "pass":true
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws28; line 1 pos 14;",
            "write_value":"No output, find exception: <console>:28: error: not found: value foo",
            "log_location":"(read) log_w_df_r_df_avro (line 2390)",
            "pass":true
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: <console>:28: error: not found: value foo",
            "log_location":"(write) log_w_df_orc (line 12739)",
            "pass":true
        }
    },
    "34":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws34; line 1 pos 14;",
            "write_value":"No output, find exception: java.lang.NumberFormatException: For input string: \"/0\"",
            "log_location":"(read) log_w_df_r_df_orc (line 780)",
            "pass":true
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.NumberFormatException: For input string: \"/0\"",
            "log_location":"(write) log_w_df_avro (line 5266)",
            "pass":true
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.NumberFormatException: For input string: \"/0\"",
            "log_location":"(write) log_w_df_parquet (line 12861)",
            "pass":true
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws34; line 1 pos 14;",
            "write_value":"No output, find exception: java.lang.NumberFormatException: For input string: \"/0\"",
            "log_location":"(read) log_w_df_r_df_parquet (line 782)",
            "pass":true
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws34; line 1 pos 14;",
            "write_value":"No output, find exception: java.lang.NumberFormatException: For input string: \"/0\"",
            "log_location":"(read) log_w_df_r_df_avro (line 2470)",
            "pass":true
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.NumberFormatException: For input string: \"/0\"",
            "log_location":"(write) log_w_df_orc (line 12858)",
            "pass":true
        }
    },
    "35":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"1",
            "write_value":"1",
            "pass":false
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws35; line 1 pos 14;",
            "write_value":"No output, find exception: java.lang.NumberFormatException: For input string: \"1.1\"",
            "log_location":"(read) log_w_df_r_df_orc (line 820)",
            "pass":true
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"1",
            "write_value":"1",
            "pass":false
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.NumberFormatException: For input string: \"1.1\"",
            "log_location":"(write) log_w_df_avro (line 5293)",
            "pass":true
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"1",
            "write_value":"1",
            "pass":false
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.NumberFormatException: For input string: \"1.1\"",
            "log_location":"(write) log_w_df_parquet (line 12888)",
            "pass":true
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"1",
            "write_value":"1",
            "pass":false
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws35; line 1 pos 14;",
            "write_value":"No output, find exception: java.lang.NumberFormatException: For input string: \"1.1\"",
            "log_location":"(read) log_w_df_r_df_parquet (line 822)",
            "pass":true
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"1",
            "write_value":"1",
            "pass":false
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"1",
            "write_value":"1",
            "pass":false
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws35; line 1 pos 14;",
            "write_value":"No output, find exception: java.lang.NumberFormatException: For input string: \"1.1\"",
            "log_location":"(read) log_w_df_r_df_avro (line 2510)",
            "pass":true
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.NumberFormatException: For input string: \"1.1\"",
            "log_location":"(write) log_w_df_orc (line 12885)",
            "pass":true
        }
    },
    "36":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"9223372036854775808, find exception: java.lang.ArithmeticException: Casting 9223372036854775808 to long causes overflow",
            "log_location":"(write) log_w_sql_avro (line 1817)",
            "pass":true
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws36; line 1 pos 14;",
            "write_value":"No output, find exception: <console>:1: error: integer number too large",
            "log_location":"(read) log_w_df_r_df_orc (line 860)",
            "pass":true
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"9223372036854775808, find exception: java.lang.ArithmeticException: Casting 9223372036854775808 to long causes overflow",
            "log_location":"(write) log_w_sql_orc (line 2801)",
            "pass":true
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: <console>:1: error: integer number too large",
            "log_location":"(write) log_w_df_avro (line 5320)",
            "pass":true
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"9223372036854775808, find exception: java.lang.ArithmeticException: Casting 9223372036854775808 to long causes overflow",
            "log_location":"(write) log_w_sql_avro (line 1817)",
            "pass":true
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: <console>:1: error: integer number too large",
            "log_location":"(write) log_w_df_parquet (line 12915)",
            "pass":true
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"9223372036854775808, find exception: java.lang.ArithmeticException: Casting 9223372036854775808 to long causes overflow",
            "log_location":"(write) log_w_sql_parquet (line 2802)",
            "pass":true
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws36; line 1 pos 14;",
            "write_value":"No output, find exception: <console>:1: error: integer number too large",
            "log_location":"(read) log_w_df_r_df_parquet (line 862)",
            "pass":true
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"9223372036854775808, find exception: java.lang.ArithmeticException: Casting 9223372036854775808 to long causes overflow",
            "log_location":"(write) log_w_sql_orc (line 2801)",
            "pass":true
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"9223372036854775808, find exception: java.lang.ArithmeticException: Casting 9223372036854775808 to long causes overflow",
            "log_location":"(write) log_w_sql_parquet (line 2802)",
            "pass":true
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws36; line 1 pos 14;",
            "write_value":"No output, find exception: <console>:1: error: integer number too large",
            "log_location":"(read) log_w_df_r_df_avro (line 2550)",
            "pass":true
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: <console>:1: error: integer number too large",
            "log_location":"(write) log_w_df_orc (line 12912)",
            "pass":true
        }
    },
    "37":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"-9223372036854775809, find exception: java.lang.ArithmeticException: Casting -9223372036854775809 to long causes overflow",
            "log_location":"(write) log_w_sql_avro (line 2099)",
            "pass":true
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws37; line 1 pos 14;",
            "write_value":"No output, find exception: <console>:1: error: integer number too large",
            "log_location":"(read) log_w_df_r_df_orc (line 900)",
            "pass":true
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"-9223372036854775809, find exception: java.lang.ArithmeticException: Casting -9223372036854775809 to long causes overflow",
            "log_location":"(write) log_w_sql_orc (line 3082)",
            "pass":true
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: <console>:1: error: integer number too large",
            "log_location":"(write) log_w_df_avro (line 5343)",
            "pass":true
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"-9223372036854775809, find exception: java.lang.ArithmeticException: Casting -9223372036854775809 to long causes overflow",
            "log_location":"(write) log_w_sql_avro (line 2099)",
            "pass":true
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: <console>:1: error: integer number too large",
            "log_location":"(write) log_w_df_parquet (line 12938)",
            "pass":true
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"-9223372036854775809, find exception: java.lang.ArithmeticException: Casting -9223372036854775809 to long causes overflow",
            "log_location":"(write) log_w_sql_parquet (line 3083)",
            "pass":true
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws37; line 1 pos 14;",
            "write_value":"No output, find exception: <console>:1: error: integer number too large",
            "log_location":"(read) log_w_df_r_df_parquet (line 902)",
            "pass":true
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"-9223372036854775809, find exception: java.lang.ArithmeticException: Casting -9223372036854775809 to long causes overflow",
            "log_location":"(write) log_w_sql_orc (line 3082)",
            "pass":true
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"-9223372036854775809, find exception: java.lang.ArithmeticException: Casting -9223372036854775809 to long causes overflow",
            "log_location":"(write) log_w_sql_parquet (line 3083)",
            "pass":true
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws37; line 1 pos 14;",
            "write_value":"No output, find exception: <console>:1: error: integer number too large",
            "log_location":"(read) log_w_df_r_df_avro (line 2590)",
            "pass":true
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: <console>:1: error: integer number too large",
            "log_location":"(write) log_w_df_orc (line 12935)",
            "pass":true
        }
    },
    "51":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: mismatched input '<EOF>' expecting {'(', 'ADD', 'AFTER', 'ALL', 'ALTER', 'ANALYZE', 'AND', 'ANTI', 'ANY', 'ARCHIVE', 'ARRAY', 'AS', 'ASC', 'AT', 'AUTHORIZATION', 'BETWEEN', 'BOTH', 'BUCKET', 'BUCKETS', 'BY', 'CACHE', 'CASCADE', 'CASE', 'CAST', 'CHANGE', 'CHECK', 'CLEAR', 'CLUSTER', 'CLUSTERED', 'CODEGEN', 'COLLATE', 'COLLECTION', 'COLUMN', 'COLUMNS', 'COMMENT', 'COMMIT', 'COMPACT', 'COMPACTIONS', 'COMPUTE', 'CONCATENATE', 'CONSTRAINT', 'COST', 'CREATE', 'CROSS', 'CUBE', 'CURRENT', 'CURRENT_DATE', 'CURRENT_TIME', 'CURRENT_TIMESTAMP', 'CURRENT_USER', 'DAY', 'DATA', 'DATABASE', DATABASES, 'DBPROPERTIES', 'DEFINED', 'DELETE', 'DELIMITED', 'DESC', 'DESCRIBE', 'DFS', 'DIRECTORIES', 'DIRECTORY', 'DISTINCT', 'DISTRIBUTE', 'DIV', 'DROP', 'ELSE', 'END', 'ESCAPE', 'ESCAPED', 'EXCEPT', 'EXCHANGE', 'EXISTS', 'EXPLAIN', 'EXPORT', 'EXTENDED', 'EXTERNAL', 'EXTRACT', 'FALSE', 'FETCH', 'FIELDS', 'FILTER', 'FILEFORMAT', 'FIRST', 'FOLLOWING', 'FOR', 'FOREIGN', 'FORMAT', 'FORMATTED', 'FROM', 'FULL', 'FUNCTION', 'FUNCTIONS', 'GLOBAL', 'GRANT', 'GROUP', 'GROUPING', 'HAVING', 'HOUR', 'IF', 'IGNORE', 'IMPORT', 'IN', 'INDEX', 'INDEXES', 'INNER', 'INPATH', 'INPUTFORMAT', 'INSERT', 'INTERSECT', 'INTERVAL', 'INTO', 'IS', 'ITEMS', 'JOIN', 'KEYS', 'LAST', 'LATERAL', 'LAZY', 'LEADING', 'LEFT', 'LIKE', 'LIMIT', 'LINES', 'LIST', 'LOAD', 'LOCAL', 'LOCATION', 'LOCK', 'LOCKS', 'LOGICAL', 'MACRO', 'MAP', 'MATCHED', 'MERGE', 'MINUTE', 'MONTH', 'MSCK', 'NAMESPACE', 'NAMESPACES', 'NATURAL', 'NO', NOT, 'NULL', 'NULLS', 'OF', 'ON', 'ONLY', 'OPTION', 'OPTIONS', 'OR', 'ORDER', 'OUT', 'OUTER', 'OUTPUTFORMAT', 'OVER', 'OVERLAPS', 'OVERLAY', 'OVERWRITE', 'PARTITION', 'PARTITIONED', 'PARTITIONS', 'PERCENT', 'PIVOT', 'PLACING', 'POSITION', 'PRECEDING', 'PRIMARY', 'PRINCIPALS', 'PROPERTIES', 'PURGE', 'QUERY', 'RANGE', 'RECORDREADER', 'RECORDWRITER', 'RECOVER', 'REDUCE', 'REFERENCES', 'REFRESH', 'RENAME', 'REPAIR', 'REPLACE', 'RESET', 'RESPECT', 'RESTRICT', 'REVOKE', 'RIGHT', RLIKE, 'ROLE', 'ROLES', 'ROLLBACK', 'ROLLUP', 'ROW', 'ROWS', 'SECOND', 'SCHEMA', 'SELECT', 'SEMI', 'SEPARATED', 'SERDE', 'SERDEPROPERTIES', 'SESSION_USER', 'SET', 'MINUS', 'SETS', 'SHOW', 'SKEWED', 'SOME', 'SORT', 'SORTED', 'START', 'STATISTICS', 'STORED', 'STRATIFY', 'STRUCT', 'SUBSTR', 'SUBSTRING', 'SYNC', 'TABLE', 'TABLES', 'TABLESAMPLE', 'TBLPROPERTIES', TEMPORARY, 'TERMINATED', 'THEN', 'TIME', 'TO', 'TOUCH', 'TRAILING', 'TRANSACTION', 'TRANSACTIONS', 'TRANSFORM', 'TRIM', 'TRUE', 'TRUNCATE', 'TRY_CAST', 'TYPE', 'UNARCHIVE', 'UNBOUNDED', 'UNCACHE', 'UNION', 'UNIQUE', 'UNKNOWN', 'UNLOCK', 'UNSET', 'UPDATE', 'USE', 'USER', 'USING', 'VALUES', 'VIEW', 'VIEWS', 'WHEN', 'WHERE', 'WINDOW', 'WITH', 'YEAR', 'ZONE', '+', '-', '*', '~', STRING, BIGINT_LITERAL, SMALLINT_LITERAL, TINYINT_LITERAL, INTEGER_VALUE, EXPONENT_VALUE, DECIMAL_VALUE, FLOAT_LITERAL, DOUBLE_LITERAL, BIGDECIMAL_LITERAL, IDENTIFIER, BACKQUOTED_IDENTIFIER}(line 1, pos 28)",
            "log_location":"(write) log_w_sql_avro (line 2503)",
            "pass":true
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws51; line 1 pos 14;",
            "write_value":"No output, find exception: <console>:1: error: illegal start of simple expression",
            "log_location":"(read) log_w_df_r_df_orc (line 1044)",
            "pass":true
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: mismatched input '<EOF>' expecting {'(', 'ADD', 'AFTER', 'ALL', 'ALTER', 'ANALYZE', 'AND', 'ANTI', 'ANY', 'ARCHIVE', 'ARRAY', 'AS', 'ASC', 'AT', 'AUTHORIZATION', 'BETWEEN', 'BOTH', 'BUCKET', 'BUCKETS', 'BY', 'CACHE', 'CASCADE', 'CASE', 'CAST', 'CHANGE', 'CHECK', 'CLEAR', 'CLUSTER', 'CLUSTERED', 'CODEGEN', 'COLLATE', 'COLLECTION', 'COLUMN', 'COLUMNS', 'COMMENT', 'COMMIT', 'COMPACT', 'COMPACTIONS', 'COMPUTE', 'CONCATENATE', 'CONSTRAINT', 'COST', 'CREATE', 'CROSS', 'CUBE', 'CURRENT', 'CURRENT_DATE', 'CURRENT_TIME', 'CURRENT_TIMESTAMP', 'CURRENT_USER', 'DAY', 'DATA', 'DATABASE', DATABASES, 'DBPROPERTIES', 'DEFINED', 'DELETE', 'DELIMITED', 'DESC', 'DESCRIBE', 'DFS', 'DIRECTORIES', 'DIRECTORY', 'DISTINCT', 'DISTRIBUTE', 'DIV', 'DROP', 'ELSE', 'END', 'ESCAPE', 'ESCAPED', 'EXCEPT', 'EXCHANGE', 'EXISTS', 'EXPLAIN', 'EXPORT', 'EXTENDED', 'EXTERNAL', 'EXTRACT', 'FALSE', 'FETCH', 'FIELDS', 'FILTER', 'FILEFORMAT', 'FIRST', 'FOLLOWING', 'FOR', 'FOREIGN', 'FORMAT', 'FORMATTED', 'FROM', 'FULL', 'FUNCTION', 'FUNCTIONS', 'GLOBAL', 'GRANT', 'GROUP', 'GROUPING', 'HAVING', 'HOUR', 'IF', 'IGNORE', 'IMPORT', 'IN', 'INDEX', 'INDEXES', 'INNER', 'INPATH', 'INPUTFORMAT', 'INSERT', 'INTERSECT', 'INTERVAL', 'INTO', 'IS', 'ITEMS', 'JOIN', 'KEYS', 'LAST', 'LATERAL', 'LAZY', 'LEADING', 'LEFT', 'LIKE', 'LIMIT', 'LINES', 'LIST', 'LOAD', 'LOCAL', 'LOCATION', 'LOCK', 'LOCKS', 'LOGICAL', 'MACRO', 'MAP', 'MATCHED', 'MERGE', 'MINUTE', 'MONTH', 'MSCK', 'NAMESPACE', 'NAMESPACES', 'NATURAL', 'NO', NOT, 'NULL', 'NULLS', 'OF', 'ON', 'ONLY', 'OPTION', 'OPTIONS', 'OR', 'ORDER', 'OUT', 'OUTER', 'OUTPUTFORMAT', 'OVER', 'OVERLAPS', 'OVERLAY', 'OVERWRITE', 'PARTITION', 'PARTITIONED', 'PARTITIONS', 'PERCENT', 'PIVOT', 'PLACING', 'POSITION', 'PRECEDING', 'PRIMARY', 'PRINCIPALS', 'PROPERTIES', 'PURGE', 'QUERY', 'RANGE', 'RECORDREADER', 'RECORDWRITER', 'RECOVER', 'REDUCE', 'REFERENCES', 'REFRESH', 'RENAME', 'REPAIR', 'REPLACE', 'RESET', 'RESPECT', 'RESTRICT', 'REVOKE', 'RIGHT', RLIKE, 'ROLE', 'ROLES', 'ROLLBACK', 'ROLLUP', 'ROW', 'ROWS', 'SECOND', 'SCHEMA', 'SELECT', 'SEMI', 'SEPARATED', 'SERDE', 'SERDEPROPERTIES', 'SESSION_USER', 'SET', 'MINUS', 'SETS', 'SHOW', 'SKEWED', 'SOME', 'SORT', 'SORTED', 'START', 'STATISTICS', 'STORED', 'STRATIFY', 'STRUCT', 'SUBSTR', 'SUBSTRING', 'SYNC', 'TABLE', 'TABLES', 'TABLESAMPLE', 'TBLPROPERTIES', TEMPORARY, 'TERMINATED', 'THEN', 'TIME', 'TO', 'TOUCH', 'TRAILING', 'TRANSACTION', 'TRANSACTIONS', 'TRANSFORM', 'TRIM', 'TRUE', 'TRUNCATE', 'TRY_CAST', 'TYPE', 'UNARCHIVE', 'UNBOUNDED', 'UNCACHE', 'UNION', 'UNIQUE', 'UNKNOWN', 'UNLOCK', 'UNSET', 'UPDATE', 'USE', 'USER', 'USING', 'VALUES', 'VIEW', 'VIEWS', 'WHEN', 'WHERE', 'WINDOW', 'WITH', 'YEAR', 'ZONE', '+', '-', '*', '~', STRING, BIGINT_LITERAL, SMALLINT_LITERAL, TINYINT_LITERAL, INTEGER_VALUE, EXPONENT_VALUE, DECIMAL_VALUE, FLOAT_LITERAL, DOUBLE_LITERAL, BIGDECIMAL_LITERAL, IDENTIFIER, BACKQUOTED_IDENTIFIER}(line 1, pos 28)",
            "log_location":"(write) log_w_sql_orc (line 3472)",
            "pass":true
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: <console>:1: error: illegal start of simple expression",
            "log_location":"(write) log_w_df_avro (line 5613)",
            "pass":true
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: mismatched input '<EOF>' expecting {'(', 'ADD', 'AFTER', 'ALL', 'ALTER', 'ANALYZE', 'AND', 'ANTI', 'ANY', 'ARCHIVE', 'ARRAY', 'AS', 'ASC', 'AT', 'AUTHORIZATION', 'BETWEEN', 'BOTH', 'BUCKET', 'BUCKETS', 'BY', 'CACHE', 'CASCADE', 'CASE', 'CAST', 'CHANGE', 'CHECK', 'CLEAR', 'CLUSTER', 'CLUSTERED', 'CODEGEN', 'COLLATE', 'COLLECTION', 'COLUMN', 'COLUMNS', 'COMMENT', 'COMMIT', 'COMPACT', 'COMPACTIONS', 'COMPUTE', 'CONCATENATE', 'CONSTRAINT', 'COST', 'CREATE', 'CROSS', 'CUBE', 'CURRENT', 'CURRENT_DATE', 'CURRENT_TIME', 'CURRENT_TIMESTAMP', 'CURRENT_USER', 'DAY', 'DATA', 'DATABASE', DATABASES, 'DBPROPERTIES', 'DEFINED', 'DELETE', 'DELIMITED', 'DESC', 'DESCRIBE', 'DFS', 'DIRECTORIES', 'DIRECTORY', 'DISTINCT', 'DISTRIBUTE', 'DIV', 'DROP', 'ELSE', 'END', 'ESCAPE', 'ESCAPED', 'EXCEPT', 'EXCHANGE', 'EXISTS', 'EXPLAIN', 'EXPORT', 'EXTENDED', 'EXTERNAL', 'EXTRACT', 'FALSE', 'FETCH', 'FIELDS', 'FILTER', 'FILEFORMAT', 'FIRST', 'FOLLOWING', 'FOR', 'FOREIGN', 'FORMAT', 'FORMATTED', 'FROM', 'FULL', 'FUNCTION', 'FUNCTIONS', 'GLOBAL', 'GRANT', 'GROUP', 'GROUPING', 'HAVING', 'HOUR', 'IF', 'IGNORE', 'IMPORT', 'IN', 'INDEX', 'INDEXES', 'INNER', 'INPATH', 'INPUTFORMAT', 'INSERT', 'INTERSECT', 'INTERVAL', 'INTO', 'IS', 'ITEMS', 'JOIN', 'KEYS', 'LAST', 'LATERAL', 'LAZY', 'LEADING', 'LEFT', 'LIKE', 'LIMIT', 'LINES', 'LIST', 'LOAD', 'LOCAL', 'LOCATION', 'LOCK', 'LOCKS', 'LOGICAL', 'MACRO', 'MAP', 'MATCHED', 'MERGE', 'MINUTE', 'MONTH', 'MSCK', 'NAMESPACE', 'NAMESPACES', 'NATURAL', 'NO', NOT, 'NULL', 'NULLS', 'OF', 'ON', 'ONLY', 'OPTION', 'OPTIONS', 'OR', 'ORDER', 'OUT', 'OUTER', 'OUTPUTFORMAT', 'OVER', 'OVERLAPS', 'OVERLAY', 'OVERWRITE', 'PARTITION', 'PARTITIONED', 'PARTITIONS', 'PERCENT', 'PIVOT', 'PLACING', 'POSITION', 'PRECEDING', 'PRIMARY', 'PRINCIPALS', 'PROPERTIES', 'PURGE', 'QUERY', 'RANGE', 'RECORDREADER', 'RECORDWRITER', 'RECOVER', 'REDUCE', 'REFERENCES', 'REFRESH', 'RENAME', 'REPAIR', 'REPLACE', 'RESET', 'RESPECT', 'RESTRICT', 'REVOKE', 'RIGHT', RLIKE, 'ROLE', 'ROLES', 'ROLLBACK', 'ROLLUP', 'ROW', 'ROWS', 'SECOND', 'SCHEMA', 'SELECT', 'SEMI', 'SEPARATED', 'SERDE', 'SERDEPROPERTIES', 'SESSION_USER', 'SET', 'MINUS', 'SETS', 'SHOW', 'SKEWED', 'SOME', 'SORT', 'SORTED', 'START', 'STATISTICS', 'STORED', 'STRATIFY', 'STRUCT', 'SUBSTR', 'SUBSTRING', 'SYNC', 'TABLE', 'TABLES', 'TABLESAMPLE', 'TBLPROPERTIES', TEMPORARY, 'TERMINATED', 'THEN', 'TIME', 'TO', 'TOUCH', 'TRAILING', 'TRANSACTION', 'TRANSACTIONS', 'TRANSFORM', 'TRIM', 'TRUE', 'TRUNCATE', 'TRY_CAST', 'TYPE', 'UNARCHIVE', 'UNBOUNDED', 'UNCACHE', 'UNION', 'UNIQUE', 'UNKNOWN', 'UNLOCK', 'UNSET', 'UPDATE', 'USE', 'USER', 'USING', 'VALUES', 'VIEW', 'VIEWS', 'WHEN', 'WHERE', 'WINDOW', 'WITH', 'YEAR', 'ZONE', '+', '-', '*', '~', STRING, BIGINT_LITERAL, SMALLINT_LITERAL, TINYINT_LITERAL, INTEGER_VALUE, EXPONENT_VALUE, DECIMAL_VALUE, FLOAT_LITERAL, DOUBLE_LITERAL, BIGDECIMAL_LITERAL, IDENTIFIER, BACKQUOTED_IDENTIFIER}(line 1, pos 28)",
            "log_location":"(write) log_w_sql_avro (line 2503)",
            "pass":true
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: <console>:1: error: illegal start of simple expression",
            "log_location":"(write) log_w_df_parquet (line 13208)",
            "pass":true
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: mismatched input '<EOF>' expecting {'(', 'ADD', 'AFTER', 'ALL', 'ALTER', 'ANALYZE', 'AND', 'ANTI', 'ANY', 'ARCHIVE', 'ARRAY', 'AS', 'ASC', 'AT', 'AUTHORIZATION', 'BETWEEN', 'BOTH', 'BUCKET', 'BUCKETS', 'BY', 'CACHE', 'CASCADE', 'CASE', 'CAST', 'CHANGE', 'CHECK', 'CLEAR', 'CLUSTER', 'CLUSTERED', 'CODEGEN', 'COLLATE', 'COLLECTION', 'COLUMN', 'COLUMNS', 'COMMENT', 'COMMIT', 'COMPACT', 'COMPACTIONS', 'COMPUTE', 'CONCATENATE', 'CONSTRAINT', 'COST', 'CREATE', 'CROSS', 'CUBE', 'CURRENT', 'CURRENT_DATE', 'CURRENT_TIME', 'CURRENT_TIMESTAMP', 'CURRENT_USER', 'DAY', 'DATA', 'DATABASE', DATABASES, 'DBPROPERTIES', 'DEFINED', 'DELETE', 'DELIMITED', 'DESC', 'DESCRIBE', 'DFS', 'DIRECTORIES', 'DIRECTORY', 'DISTINCT', 'DISTRIBUTE', 'DIV', 'DROP', 'ELSE', 'END', 'ESCAPE', 'ESCAPED', 'EXCEPT', 'EXCHANGE', 'EXISTS', 'EXPLAIN', 'EXPORT', 'EXTENDED', 'EXTERNAL', 'EXTRACT', 'FALSE', 'FETCH', 'FIELDS', 'FILTER', 'FILEFORMAT', 'FIRST', 'FOLLOWING', 'FOR', 'FOREIGN', 'FORMAT', 'FORMATTED', 'FROM', 'FULL', 'FUNCTION', 'FUNCTIONS', 'GLOBAL', 'GRANT', 'GROUP', 'GROUPING', 'HAVING', 'HOUR', 'IF', 'IGNORE', 'IMPORT', 'IN', 'INDEX', 'INDEXES', 'INNER', 'INPATH', 'INPUTFORMAT', 'INSERT', 'INTERSECT', 'INTERVAL', 'INTO', 'IS', 'ITEMS', 'JOIN', 'KEYS', 'LAST', 'LATERAL', 'LAZY', 'LEADING', 'LEFT', 'LIKE', 'LIMIT', 'LINES', 'LIST', 'LOAD', 'LOCAL', 'LOCATION', 'LOCK', 'LOCKS', 'LOGICAL', 'MACRO', 'MAP', 'MATCHED', 'MERGE', 'MINUTE', 'MONTH', 'MSCK', 'NAMESPACE', 'NAMESPACES', 'NATURAL', 'NO', NOT, 'NULL', 'NULLS', 'OF', 'ON', 'ONLY', 'OPTION', 'OPTIONS', 'OR', 'ORDER', 'OUT', 'OUTER', 'OUTPUTFORMAT', 'OVER', 'OVERLAPS', 'OVERLAY', 'OVERWRITE', 'PARTITION', 'PARTITIONED', 'PARTITIONS', 'PERCENT', 'PIVOT', 'PLACING', 'POSITION', 'PRECEDING', 'PRIMARY', 'PRINCIPALS', 'PROPERTIES', 'PURGE', 'QUERY', 'RANGE', 'RECORDREADER', 'RECORDWRITER', 'RECOVER', 'REDUCE', 'REFERENCES', 'REFRESH', 'RENAME', 'REPAIR', 'REPLACE', 'RESET', 'RESPECT', 'RESTRICT', 'REVOKE', 'RIGHT', RLIKE, 'ROLE', 'ROLES', 'ROLLBACK', 'ROLLUP', 'ROW', 'ROWS', 'SECOND', 'SCHEMA', 'SELECT', 'SEMI', 'SEPARATED', 'SERDE', 'SERDEPROPERTIES', 'SESSION_USER', 'SET', 'MINUS', 'SETS', 'SHOW', 'SKEWED', 'SOME', 'SORT', 'SORTED', 'START', 'STATISTICS', 'STORED', 'STRATIFY', 'STRUCT', 'SUBSTR', 'SUBSTRING', 'SYNC', 'TABLE', 'TABLES', 'TABLESAMPLE', 'TBLPROPERTIES', TEMPORARY, 'TERMINATED', 'THEN', 'TIME', 'TO', 'TOUCH', 'TRAILING', 'TRANSACTION', 'TRANSACTIONS', 'TRANSFORM', 'TRIM', 'TRUE', 'TRUNCATE', 'TRY_CAST', 'TYPE', 'UNARCHIVE', 'UNBOUNDED', 'UNCACHE', 'UNION', 'UNIQUE', 'UNKNOWN', 'UNLOCK', 'UNSET', 'UPDATE', 'USE', 'USER', 'USING', 'VALUES', 'VIEW', 'VIEWS', 'WHEN', 'WHERE', 'WINDOW', 'WITH', 'YEAR', 'ZONE', '+', '-', '*', '~', STRING, BIGINT_LITERAL, SMALLINT_LITERAL, TINYINT_LITERAL, INTEGER_VALUE, EXPONENT_VALUE, DECIMAL_VALUE, FLOAT_LITERAL, DOUBLE_LITERAL, BIGDECIMAL_LITERAL, IDENTIFIER, BACKQUOTED_IDENTIFIER}(line 1, pos 28)",
            "log_location":"(write) log_w_sql_parquet (line 3473)",
            "pass":true
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws51; line 1 pos 14;",
            "write_value":"No output, find exception: <console>:1: error: illegal start of simple expression",
            "log_location":"(read) log_w_df_r_df_parquet (line 1046)",
            "pass":true
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: mismatched input '<EOF>' expecting {'(', 'ADD', 'AFTER', 'ALL', 'ALTER', 'ANALYZE', 'AND', 'ANTI', 'ANY', 'ARCHIVE', 'ARRAY', 'AS', 'ASC', 'AT', 'AUTHORIZATION', 'BETWEEN', 'BOTH', 'BUCKET', 'BUCKETS', 'BY', 'CACHE', 'CASCADE', 'CASE', 'CAST', 'CHANGE', 'CHECK', 'CLEAR', 'CLUSTER', 'CLUSTERED', 'CODEGEN', 'COLLATE', 'COLLECTION', 'COLUMN', 'COLUMNS', 'COMMENT', 'COMMIT', 'COMPACT', 'COMPACTIONS', 'COMPUTE', 'CONCATENATE', 'CONSTRAINT', 'COST', 'CREATE', 'CROSS', 'CUBE', 'CURRENT', 'CURRENT_DATE', 'CURRENT_TIME', 'CURRENT_TIMESTAMP', 'CURRENT_USER', 'DAY', 'DATA', 'DATABASE', DATABASES, 'DBPROPERTIES', 'DEFINED', 'DELETE', 'DELIMITED', 'DESC', 'DESCRIBE', 'DFS', 'DIRECTORIES', 'DIRECTORY', 'DISTINCT', 'DISTRIBUTE', 'DIV', 'DROP', 'ELSE', 'END', 'ESCAPE', 'ESCAPED', 'EXCEPT', 'EXCHANGE', 'EXISTS', 'EXPLAIN', 'EXPORT', 'EXTENDED', 'EXTERNAL', 'EXTRACT', 'FALSE', 'FETCH', 'FIELDS', 'FILTER', 'FILEFORMAT', 'FIRST', 'FOLLOWING', 'FOR', 'FOREIGN', 'FORMAT', 'FORMATTED', 'FROM', 'FULL', 'FUNCTION', 'FUNCTIONS', 'GLOBAL', 'GRANT', 'GROUP', 'GROUPING', 'HAVING', 'HOUR', 'IF', 'IGNORE', 'IMPORT', 'IN', 'INDEX', 'INDEXES', 'INNER', 'INPATH', 'INPUTFORMAT', 'INSERT', 'INTERSECT', 'INTERVAL', 'INTO', 'IS', 'ITEMS', 'JOIN', 'KEYS', 'LAST', 'LATERAL', 'LAZY', 'LEADING', 'LEFT', 'LIKE', 'LIMIT', 'LINES', 'LIST', 'LOAD', 'LOCAL', 'LOCATION', 'LOCK', 'LOCKS', 'LOGICAL', 'MACRO', 'MAP', 'MATCHED', 'MERGE', 'MINUTE', 'MONTH', 'MSCK', 'NAMESPACE', 'NAMESPACES', 'NATURAL', 'NO', NOT, 'NULL', 'NULLS', 'OF', 'ON', 'ONLY', 'OPTION', 'OPTIONS', 'OR', 'ORDER', 'OUT', 'OUTER', 'OUTPUTFORMAT', 'OVER', 'OVERLAPS', 'OVERLAY', 'OVERWRITE', 'PARTITION', 'PARTITIONED', 'PARTITIONS', 'PERCENT', 'PIVOT', 'PLACING', 'POSITION', 'PRECEDING', 'PRIMARY', 'PRINCIPALS', 'PROPERTIES', 'PURGE', 'QUERY', 'RANGE', 'RECORDREADER', 'RECORDWRITER', 'RECOVER', 'REDUCE', 'REFERENCES', 'REFRESH', 'RENAME', 'REPAIR', 'REPLACE', 'RESET', 'RESPECT', 'RESTRICT', 'REVOKE', 'RIGHT', RLIKE, 'ROLE', 'ROLES', 'ROLLBACK', 'ROLLUP', 'ROW', 'ROWS', 'SECOND', 'SCHEMA', 'SELECT', 'SEMI', 'SEPARATED', 'SERDE', 'SERDEPROPERTIES', 'SESSION_USER', 'SET', 'MINUS', 'SETS', 'SHOW', 'SKEWED', 'SOME', 'SORT', 'SORTED', 'START', 'STATISTICS', 'STORED', 'STRATIFY', 'STRUCT', 'SUBSTR', 'SUBSTRING', 'SYNC', 'TABLE', 'TABLES', 'TABLESAMPLE', 'TBLPROPERTIES', TEMPORARY, 'TERMINATED', 'THEN', 'TIME', 'TO', 'TOUCH', 'TRAILING', 'TRANSACTION', 'TRANSACTIONS', 'TRANSFORM', 'TRIM', 'TRUE', 'TRUNCATE', 'TRY_CAST', 'TYPE', 'UNARCHIVE', 'UNBOUNDED', 'UNCACHE', 'UNION', 'UNIQUE', 'UNKNOWN', 'UNLOCK', 'UNSET', 'UPDATE', 'USE', 'USER', 'USING', 'VALUES', 'VIEW', 'VIEWS', 'WHEN', 'WHERE', 'WINDOW', 'WITH', 'YEAR', 'ZONE', '+', '-', '*', '~', STRING, BIGINT_LITERAL, SMALLINT_LITERAL, TINYINT_LITERAL, INTEGER_VALUE, EXPONENT_VALUE, DECIMAL_VALUE, FLOAT_LITERAL, DOUBLE_LITERAL, BIGDECIMAL_LITERAL, IDENTIFIER, BACKQUOTED_IDENTIFIER}(line 1, pos 28)",
            "log_location":"(write) log_w_sql_orc (line 3472)",
            "pass":true
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: mismatched input '<EOF>' expecting {'(', 'ADD', 'AFTER', 'ALL', 'ALTER', 'ANALYZE', 'AND', 'ANTI', 'ANY', 'ARCHIVE', 'ARRAY', 'AS', 'ASC', 'AT', 'AUTHORIZATION', 'BETWEEN', 'BOTH', 'BUCKET', 'BUCKETS', 'BY', 'CACHE', 'CASCADE', 'CASE', 'CAST', 'CHANGE', 'CHECK', 'CLEAR', 'CLUSTER', 'CLUSTERED', 'CODEGEN', 'COLLATE', 'COLLECTION', 'COLUMN', 'COLUMNS', 'COMMENT', 'COMMIT', 'COMPACT', 'COMPACTIONS', 'COMPUTE', 'CONCATENATE', 'CONSTRAINT', 'COST', 'CREATE', 'CROSS', 'CUBE', 'CURRENT', 'CURRENT_DATE', 'CURRENT_TIME', 'CURRENT_TIMESTAMP', 'CURRENT_USER', 'DAY', 'DATA', 'DATABASE', DATABASES, 'DBPROPERTIES', 'DEFINED', 'DELETE', 'DELIMITED', 'DESC', 'DESCRIBE', 'DFS', 'DIRECTORIES', 'DIRECTORY', 'DISTINCT', 'DISTRIBUTE', 'DIV', 'DROP', 'ELSE', 'END', 'ESCAPE', 'ESCAPED', 'EXCEPT', 'EXCHANGE', 'EXISTS', 'EXPLAIN', 'EXPORT', 'EXTENDED', 'EXTERNAL', 'EXTRACT', 'FALSE', 'FETCH', 'FIELDS', 'FILTER', 'FILEFORMAT', 'FIRST', 'FOLLOWING', 'FOR', 'FOREIGN', 'FORMAT', 'FORMATTED', 'FROM', 'FULL', 'FUNCTION', 'FUNCTIONS', 'GLOBAL', 'GRANT', 'GROUP', 'GROUPING', 'HAVING', 'HOUR', 'IF', 'IGNORE', 'IMPORT', 'IN', 'INDEX', 'INDEXES', 'INNER', 'INPATH', 'INPUTFORMAT', 'INSERT', 'INTERSECT', 'INTERVAL', 'INTO', 'IS', 'ITEMS', 'JOIN', 'KEYS', 'LAST', 'LATERAL', 'LAZY', 'LEADING', 'LEFT', 'LIKE', 'LIMIT', 'LINES', 'LIST', 'LOAD', 'LOCAL', 'LOCATION', 'LOCK', 'LOCKS', 'LOGICAL', 'MACRO', 'MAP', 'MATCHED', 'MERGE', 'MINUTE', 'MONTH', 'MSCK', 'NAMESPACE', 'NAMESPACES', 'NATURAL', 'NO', NOT, 'NULL', 'NULLS', 'OF', 'ON', 'ONLY', 'OPTION', 'OPTIONS', 'OR', 'ORDER', 'OUT', 'OUTER', 'OUTPUTFORMAT', 'OVER', 'OVERLAPS', 'OVERLAY', 'OVERWRITE', 'PARTITION', 'PARTITIONED', 'PARTITIONS', 'PERCENT', 'PIVOT', 'PLACING', 'POSITION', 'PRECEDING', 'PRIMARY', 'PRINCIPALS', 'PROPERTIES', 'PURGE', 'QUERY', 'RANGE', 'RECORDREADER', 'RECORDWRITER', 'RECOVER', 'REDUCE', 'REFERENCES', 'REFRESH', 'RENAME', 'REPAIR', 'REPLACE', 'RESET', 'RESPECT', 'RESTRICT', 'REVOKE', 'RIGHT', RLIKE, 'ROLE', 'ROLES', 'ROLLBACK', 'ROLLUP', 'ROW', 'ROWS', 'SECOND', 'SCHEMA', 'SELECT', 'SEMI', 'SEPARATED', 'SERDE', 'SERDEPROPERTIES', 'SESSION_USER', 'SET', 'MINUS', 'SETS', 'SHOW', 'SKEWED', 'SOME', 'SORT', 'SORTED', 'START', 'STATISTICS', 'STORED', 'STRATIFY', 'STRUCT', 'SUBSTR', 'SUBSTRING', 'SYNC', 'TABLE', 'TABLES', 'TABLESAMPLE', 'TBLPROPERTIES', TEMPORARY, 'TERMINATED', 'THEN', 'TIME', 'TO', 'TOUCH', 'TRAILING', 'TRANSACTION', 'TRANSACTIONS', 'TRANSFORM', 'TRIM', 'TRUE', 'TRUNCATE', 'TRY_CAST', 'TYPE', 'UNARCHIVE', 'UNBOUNDED', 'UNCACHE', 'UNION', 'UNIQUE', 'UNKNOWN', 'UNLOCK', 'UNSET', 'UPDATE', 'USE', 'USER', 'USING', 'VALUES', 'VIEW', 'VIEWS', 'WHEN', 'WHERE', 'WINDOW', 'WITH', 'YEAR', 'ZONE', '+', '-', '*', '~', STRING, BIGINT_LITERAL, SMALLINT_LITERAL, TINYINT_LITERAL, INTEGER_VALUE, EXPONENT_VALUE, DECIMAL_VALUE, FLOAT_LITERAL, DOUBLE_LITERAL, BIGDECIMAL_LITERAL, IDENTIFIER, BACKQUOTED_IDENTIFIER}(line 1, pos 28)",
            "log_location":"(write) log_w_sql_parquet (line 3473)",
            "pass":true
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws51; line 1 pos 14;",
            "write_value":"No output, find exception: <console>:1: error: illegal start of simple expression",
            "log_location":"(read) log_w_df_r_df_avro (line 2734)",
            "pass":true
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: <console>:1: error: illegal start of simple expression",
            "log_location":"(write) log_w_df_orc (line 13205)",
            "pass":true
        }
    },
    "52":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: 'InsertIntoStatement 'UnresolvedRelation [ws52], [], false, false, false",
            "log_location":"(write) log_w_sql_avro (line 2520)",
            "pass":true
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws52; line 1 pos 14;",
            "write_value":"No output, find exception: <console>:28: error: not found: value foo",
            "log_location":"(read) log_w_df_r_df_orc (line 1084)",
            "pass":true
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: 'InsertIntoStatement 'UnresolvedRelation [ws52], [], false, false, false",
            "log_location":"(write) log_w_sql_orc (line 3488)",
            "pass":true
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: <console>:28: error: not found: value foo",
            "log_location":"(write) log_w_df_avro (line 5636)",
            "pass":true
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: 'InsertIntoStatement 'UnresolvedRelation [ws52], [], false, false, false",
            "log_location":"(write) log_w_sql_avro (line 2520)",
            "pass":true
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: <console>:28: error: not found: value foo",
            "log_location":"(write) log_w_df_parquet (line 13231)",
            "pass":true
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: 'InsertIntoStatement 'UnresolvedRelation [ws52], [], false, false, false",
            "log_location":"(write) log_w_sql_parquet (line 3489)",
            "pass":true
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws52; line 1 pos 14;",
            "write_value":"No output, find exception: <console>:28: error: not found: value foo",
            "log_location":"(read) log_w_df_r_df_parquet (line 1086)",
            "pass":true
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: 'InsertIntoStatement 'UnresolvedRelation [ws52], [], false, false, false",
            "log_location":"(write) log_w_sql_orc (line 3488)",
            "pass":true
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: 'InsertIntoStatement 'UnresolvedRelation [ws52], [], false, false, false",
            "log_location":"(write) log_w_sql_parquet (line 3489)",
            "pass":true
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws52; line 1 pos 14;",
            "write_value":"No output, find exception: <console>:28: error: not found: value foo",
            "log_location":"(read) log_w_df_r_df_avro (line 2774)",
            "pass":true
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: <console>:28: error: not found: value foo",
            "log_location":"(write) log_w_df_orc (line 13228)",
            "pass":true
        }
    },
    "66":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: mismatched input '<EOF>' expecting {'(', 'ADD', 'AFTER', 'ALL', 'ALTER', 'ANALYZE', 'AND', 'ANTI', 'ANY', 'ARCHIVE', 'ARRAY', 'AS', 'ASC', 'AT', 'AUTHORIZATION', 'BETWEEN', 'BOTH', 'BUCKET', 'BUCKETS', 'BY', 'CACHE', 'CASCADE', 'CASE', 'CAST', 'CHANGE', 'CHECK', 'CLEAR', 'CLUSTER', 'CLUSTERED', 'CODEGEN', 'COLLATE', 'COLLECTION', 'COLUMN', 'COLUMNS', 'COMMENT', 'COMMIT', 'COMPACT', 'COMPACTIONS', 'COMPUTE', 'CONCATENATE', 'CONSTRAINT', 'COST', 'CREATE', 'CROSS', 'CUBE', 'CURRENT', 'CURRENT_DATE', 'CURRENT_TIME', 'CURRENT_TIMESTAMP', 'CURRENT_USER', 'DAY', 'DATA', 'DATABASE', DATABASES, 'DBPROPERTIES', 'DEFINED', 'DELETE', 'DELIMITED', 'DESC', 'DESCRIBE', 'DFS', 'DIRECTORIES', 'DIRECTORY', 'DISTINCT', 'DISTRIBUTE', 'DIV', 'DROP', 'ELSE', 'END', 'ESCAPE', 'ESCAPED', 'EXCEPT', 'EXCHANGE', 'EXISTS', 'EXPLAIN', 'EXPORT', 'EXTENDED', 'EXTERNAL', 'EXTRACT', 'FALSE', 'FETCH', 'FIELDS', 'FILTER', 'FILEFORMAT', 'FIRST', 'FOLLOWING', 'FOR', 'FOREIGN', 'FORMAT', 'FORMATTED', 'FROM', 'FULL', 'FUNCTION', 'FUNCTIONS', 'GLOBAL', 'GRANT', 'GROUP', 'GROUPING', 'HAVING', 'HOUR', 'IF', 'IGNORE', 'IMPORT', 'IN', 'INDEX', 'INDEXES', 'INNER', 'INPATH', 'INPUTFORMAT', 'INSERT', 'INTERSECT', 'INTERVAL', 'INTO', 'IS', 'ITEMS', 'JOIN', 'KEYS', 'LAST', 'LATERAL', 'LAZY', 'LEADING', 'LEFT', 'LIKE', 'LIMIT', 'LINES', 'LIST', 'LOAD', 'LOCAL', 'LOCATION', 'LOCK', 'LOCKS', 'LOGICAL', 'MACRO', 'MAP', 'MATCHED', 'MERGE', 'MINUTE', 'MONTH', 'MSCK', 'NAMESPACE', 'NAMESPACES', 'NATURAL', 'NO', NOT, 'NULL', 'NULLS', 'OF', 'ON', 'ONLY', 'OPTION', 'OPTIONS', 'OR', 'ORDER', 'OUT', 'OUTER', 'OUTPUTFORMAT', 'OVER', 'OVERLAPS', 'OVERLAY', 'OVERWRITE', 'PARTITION', 'PARTITIONED', 'PARTITIONS', 'PERCENT', 'PIVOT', 'PLACING', 'POSITION', 'PRECEDING', 'PRIMARY', 'PRINCIPALS', 'PROPERTIES', 'PURGE', 'QUERY', 'RANGE', 'RECORDREADER', 'RECORDWRITER', 'RECOVER', 'REDUCE', 'REFERENCES', 'REFRESH', 'RENAME', 'REPAIR', 'REPLACE', 'RESET', 'RESPECT', 'RESTRICT', 'REVOKE', 'RIGHT', RLIKE, 'ROLE', 'ROLES', 'ROLLBACK', 'ROLLUP', 'ROW', 'ROWS', 'SECOND', 'SCHEMA', 'SELECT', 'SEMI', 'SEPARATED', 'SERDE', 'SERDEPROPERTIES', 'SESSION_USER', 'SET', 'MINUS', 'SETS', 'SHOW', 'SKEWED', 'SOME', 'SORT', 'SORTED', 'START', 'STATISTICS', 'STORED', 'STRATIFY', 'STRUCT', 'SUBSTR', 'SUBSTRING', 'SYNC', 'TABLE', 'TABLES', 'TABLESAMPLE', 'TBLPROPERTIES', TEMPORARY, 'TERMINATED', 'THEN', 'TIME', 'TO', 'TOUCH', 'TRAILING', 'TRANSACTION', 'TRANSACTIONS', 'TRANSFORM', 'TRIM', 'TRUE', 'TRUNCATE', 'TRY_CAST', 'TYPE', 'UNARCHIVE', 'UNBOUNDED', 'UNCACHE', 'UNION', 'UNIQUE', 'UNKNOWN', 'UNLOCK', 'UNSET', 'UPDATE', 'USE', 'USER', 'USING', 'VALUES', 'VIEW', 'VIEWS', 'WHEN', 'WHERE', 'WINDOW', 'WITH', 'YEAR', 'ZONE', '+', '-', '*', '~', STRING, BIGINT_LITERAL, SMALLINT_LITERAL, TINYINT_LITERAL, INTEGER_VALUE, EXPONENT_VALUE, DECIMAL_VALUE, FLOAT_LITERAL, DOUBLE_LITERAL, BIGDECIMAL_LITERAL, IDENTIFIER, BACKQUOTED_IDENTIFIER}(line 1, pos 28)",
            "log_location":"(write) log_w_sql_avro (line 2655)",
            "pass":true
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws66; line 1 pos 14;",
            "write_value":"No output, find exception: <console>:1: error: illegal start of simple expression",
            "log_location":"(read) log_w_df_r_df_orc (line 1228)",
            "pass":true
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: mismatched input '<EOF>' expecting {'(', 'ADD', 'AFTER', 'ALL', 'ALTER', 'ANALYZE', 'AND', 'ANTI', 'ANY', 'ARCHIVE', 'ARRAY', 'AS', 'ASC', 'AT', 'AUTHORIZATION', 'BETWEEN', 'BOTH', 'BUCKET', 'BUCKETS', 'BY', 'CACHE', 'CASCADE', 'CASE', 'CAST', 'CHANGE', 'CHECK', 'CLEAR', 'CLUSTER', 'CLUSTERED', 'CODEGEN', 'COLLATE', 'COLLECTION', 'COLUMN', 'COLUMNS', 'COMMENT', 'COMMIT', 'COMPACT', 'COMPACTIONS', 'COMPUTE', 'CONCATENATE', 'CONSTRAINT', 'COST', 'CREATE', 'CROSS', 'CUBE', 'CURRENT', 'CURRENT_DATE', 'CURRENT_TIME', 'CURRENT_TIMESTAMP', 'CURRENT_USER', 'DAY', 'DATA', 'DATABASE', DATABASES, 'DBPROPERTIES', 'DEFINED', 'DELETE', 'DELIMITED', 'DESC', 'DESCRIBE', 'DFS', 'DIRECTORIES', 'DIRECTORY', 'DISTINCT', 'DISTRIBUTE', 'DIV', 'DROP', 'ELSE', 'END', 'ESCAPE', 'ESCAPED', 'EXCEPT', 'EXCHANGE', 'EXISTS', 'EXPLAIN', 'EXPORT', 'EXTENDED', 'EXTERNAL', 'EXTRACT', 'FALSE', 'FETCH', 'FIELDS', 'FILTER', 'FILEFORMAT', 'FIRST', 'FOLLOWING', 'FOR', 'FOREIGN', 'FORMAT', 'FORMATTED', 'FROM', 'FULL', 'FUNCTION', 'FUNCTIONS', 'GLOBAL', 'GRANT', 'GROUP', 'GROUPING', 'HAVING', 'HOUR', 'IF', 'IGNORE', 'IMPORT', 'IN', 'INDEX', 'INDEXES', 'INNER', 'INPATH', 'INPUTFORMAT', 'INSERT', 'INTERSECT', 'INTERVAL', 'INTO', 'IS', 'ITEMS', 'JOIN', 'KEYS', 'LAST', 'LATERAL', 'LAZY', 'LEADING', 'LEFT', 'LIKE', 'LIMIT', 'LINES', 'LIST', 'LOAD', 'LOCAL', 'LOCATION', 'LOCK', 'LOCKS', 'LOGICAL', 'MACRO', 'MAP', 'MATCHED', 'MERGE', 'MINUTE', 'MONTH', 'MSCK', 'NAMESPACE', 'NAMESPACES', 'NATURAL', 'NO', NOT, 'NULL', 'NULLS', 'OF', 'ON', 'ONLY', 'OPTION', 'OPTIONS', 'OR', 'ORDER', 'OUT', 'OUTER', 'OUTPUTFORMAT', 'OVER', 'OVERLAPS', 'OVERLAY', 'OVERWRITE', 'PARTITION', 'PARTITIONED', 'PARTITIONS', 'PERCENT', 'PIVOT', 'PLACING', 'POSITION', 'PRECEDING', 'PRIMARY', 'PRINCIPALS', 'PROPERTIES', 'PURGE', 'QUERY', 'RANGE', 'RECORDREADER', 'RECORDWRITER', 'RECOVER', 'REDUCE', 'REFERENCES', 'REFRESH', 'RENAME', 'REPAIR', 'REPLACE', 'RESET', 'RESPECT', 'RESTRICT', 'REVOKE', 'RIGHT', RLIKE, 'ROLE', 'ROLES', 'ROLLBACK', 'ROLLUP', 'ROW', 'ROWS', 'SECOND', 'SCHEMA', 'SELECT', 'SEMI', 'SEPARATED', 'SERDE', 'SERDEPROPERTIES', 'SESSION_USER', 'SET', 'MINUS', 'SETS', 'SHOW', 'SKEWED', 'SOME', 'SORT', 'SORTED', 'START', 'STATISTICS', 'STORED', 'STRATIFY', 'STRUCT', 'SUBSTR', 'SUBSTRING', 'SYNC', 'TABLE', 'TABLES', 'TABLESAMPLE', 'TBLPROPERTIES', TEMPORARY, 'TERMINATED', 'THEN', 'TIME', 'TO', 'TOUCH', 'TRAILING', 'TRANSACTION', 'TRANSACTIONS', 'TRANSFORM', 'TRIM', 'TRUE', 'TRUNCATE', 'TRY_CAST', 'TYPE', 'UNARCHIVE', 'UNBOUNDED', 'UNCACHE', 'UNION', 'UNIQUE', 'UNKNOWN', 'UNLOCK', 'UNSET', 'UPDATE', 'USE', 'USER', 'USING', 'VALUES', 'VIEW', 'VIEWS', 'WHEN', 'WHERE', 'WINDOW', 'WITH', 'YEAR', 'ZONE', '+', '-', '*', '~', STRING, BIGINT_LITERAL, SMALLINT_LITERAL, TINYINT_LITERAL, INTEGER_VALUE, EXPONENT_VALUE, DECIMAL_VALUE, FLOAT_LITERAL, DOUBLE_LITERAL, BIGDECIMAL_LITERAL, IDENTIFIER, BACKQUOTED_IDENTIFIER}(line 1, pos 28)",
            "log_location":"(write) log_w_sql_orc (line 3609)",
            "pass":true
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: <console>:1: error: illegal start of simple expression",
            "log_location":"(write) log_w_df_avro (line 5907)",
            "pass":true
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: mismatched input '<EOF>' expecting {'(', 'ADD', 'AFTER', 'ALL', 'ALTER', 'ANALYZE', 'AND', 'ANTI', 'ANY', 'ARCHIVE', 'ARRAY', 'AS', 'ASC', 'AT', 'AUTHORIZATION', 'BETWEEN', 'BOTH', 'BUCKET', 'BUCKETS', 'BY', 'CACHE', 'CASCADE', 'CASE', 'CAST', 'CHANGE', 'CHECK', 'CLEAR', 'CLUSTER', 'CLUSTERED', 'CODEGEN', 'COLLATE', 'COLLECTION', 'COLUMN', 'COLUMNS', 'COMMENT', 'COMMIT', 'COMPACT', 'COMPACTIONS', 'COMPUTE', 'CONCATENATE', 'CONSTRAINT', 'COST', 'CREATE', 'CROSS', 'CUBE', 'CURRENT', 'CURRENT_DATE', 'CURRENT_TIME', 'CURRENT_TIMESTAMP', 'CURRENT_USER', 'DAY', 'DATA', 'DATABASE', DATABASES, 'DBPROPERTIES', 'DEFINED', 'DELETE', 'DELIMITED', 'DESC', 'DESCRIBE', 'DFS', 'DIRECTORIES', 'DIRECTORY', 'DISTINCT', 'DISTRIBUTE', 'DIV', 'DROP', 'ELSE', 'END', 'ESCAPE', 'ESCAPED', 'EXCEPT', 'EXCHANGE', 'EXISTS', 'EXPLAIN', 'EXPORT', 'EXTENDED', 'EXTERNAL', 'EXTRACT', 'FALSE', 'FETCH', 'FIELDS', 'FILTER', 'FILEFORMAT', 'FIRST', 'FOLLOWING', 'FOR', 'FOREIGN', 'FORMAT', 'FORMATTED', 'FROM', 'FULL', 'FUNCTION', 'FUNCTIONS', 'GLOBAL', 'GRANT', 'GROUP', 'GROUPING', 'HAVING', 'HOUR', 'IF', 'IGNORE', 'IMPORT', 'IN', 'INDEX', 'INDEXES', 'INNER', 'INPATH', 'INPUTFORMAT', 'INSERT', 'INTERSECT', 'INTERVAL', 'INTO', 'IS', 'ITEMS', 'JOIN', 'KEYS', 'LAST', 'LATERAL', 'LAZY', 'LEADING', 'LEFT', 'LIKE', 'LIMIT', 'LINES', 'LIST', 'LOAD', 'LOCAL', 'LOCATION', 'LOCK', 'LOCKS', 'LOGICAL', 'MACRO', 'MAP', 'MATCHED', 'MERGE', 'MINUTE', 'MONTH', 'MSCK', 'NAMESPACE', 'NAMESPACES', 'NATURAL', 'NO', NOT, 'NULL', 'NULLS', 'OF', 'ON', 'ONLY', 'OPTION', 'OPTIONS', 'OR', 'ORDER', 'OUT', 'OUTER', 'OUTPUTFORMAT', 'OVER', 'OVERLAPS', 'OVERLAY', 'OVERWRITE', 'PARTITION', 'PARTITIONED', 'PARTITIONS', 'PERCENT', 'PIVOT', 'PLACING', 'POSITION', 'PRECEDING', 'PRIMARY', 'PRINCIPALS', 'PROPERTIES', 'PURGE', 'QUERY', 'RANGE', 'RECORDREADER', 'RECORDWRITER', 'RECOVER', 'REDUCE', 'REFERENCES', 'REFRESH', 'RENAME', 'REPAIR', 'REPLACE', 'RESET', 'RESPECT', 'RESTRICT', 'REVOKE', 'RIGHT', RLIKE, 'ROLE', 'ROLES', 'ROLLBACK', 'ROLLUP', 'ROW', 'ROWS', 'SECOND', 'SCHEMA', 'SELECT', 'SEMI', 'SEPARATED', 'SERDE', 'SERDEPROPERTIES', 'SESSION_USER', 'SET', 'MINUS', 'SETS', 'SHOW', 'SKEWED', 'SOME', 'SORT', 'SORTED', 'START', 'STATISTICS', 'STORED', 'STRATIFY', 'STRUCT', 'SUBSTR', 'SUBSTRING', 'SYNC', 'TABLE', 'TABLES', 'TABLESAMPLE', 'TBLPROPERTIES', TEMPORARY, 'TERMINATED', 'THEN', 'TIME', 'TO', 'TOUCH', 'TRAILING', 'TRANSACTION', 'TRANSACTIONS', 'TRANSFORM', 'TRIM', 'TRUE', 'TRUNCATE', 'TRY_CAST', 'TYPE', 'UNARCHIVE', 'UNBOUNDED', 'UNCACHE', 'UNION', 'UNIQUE', 'UNKNOWN', 'UNLOCK', 'UNSET', 'UPDATE', 'USE', 'USER', 'USING', 'VALUES', 'VIEW', 'VIEWS', 'WHEN', 'WHERE', 'WINDOW', 'WITH', 'YEAR', 'ZONE', '+', '-', '*', '~', STRING, BIGINT_LITERAL, SMALLINT_LITERAL, TINYINT_LITERAL, INTEGER_VALUE, EXPONENT_VALUE, DECIMAL_VALUE, FLOAT_LITERAL, DOUBLE_LITERAL, BIGDECIMAL_LITERAL, IDENTIFIER, BACKQUOTED_IDENTIFIER}(line 1, pos 28)",
            "log_location":"(write) log_w_sql_avro (line 2655)",
            "pass":true
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: <console>:1: error: illegal start of simple expression",
            "log_location":"(write) log_w_df_parquet (line 13502)",
            "pass":true
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: mismatched input '<EOF>' expecting {'(', 'ADD', 'AFTER', 'ALL', 'ALTER', 'ANALYZE', 'AND', 'ANTI', 'ANY', 'ARCHIVE', 'ARRAY', 'AS', 'ASC', 'AT', 'AUTHORIZATION', 'BETWEEN', 'BOTH', 'BUCKET', 'BUCKETS', 'BY', 'CACHE', 'CASCADE', 'CASE', 'CAST', 'CHANGE', 'CHECK', 'CLEAR', 'CLUSTER', 'CLUSTERED', 'CODEGEN', 'COLLATE', 'COLLECTION', 'COLUMN', 'COLUMNS', 'COMMENT', 'COMMIT', 'COMPACT', 'COMPACTIONS', 'COMPUTE', 'CONCATENATE', 'CONSTRAINT', 'COST', 'CREATE', 'CROSS', 'CUBE', 'CURRENT', 'CURRENT_DATE', 'CURRENT_TIME', 'CURRENT_TIMESTAMP', 'CURRENT_USER', 'DAY', 'DATA', 'DATABASE', DATABASES, 'DBPROPERTIES', 'DEFINED', 'DELETE', 'DELIMITED', 'DESC', 'DESCRIBE', 'DFS', 'DIRECTORIES', 'DIRECTORY', 'DISTINCT', 'DISTRIBUTE', 'DIV', 'DROP', 'ELSE', 'END', 'ESCAPE', 'ESCAPED', 'EXCEPT', 'EXCHANGE', 'EXISTS', 'EXPLAIN', 'EXPORT', 'EXTENDED', 'EXTERNAL', 'EXTRACT', 'FALSE', 'FETCH', 'FIELDS', 'FILTER', 'FILEFORMAT', 'FIRST', 'FOLLOWING', 'FOR', 'FOREIGN', 'FORMAT', 'FORMATTED', 'FROM', 'FULL', 'FUNCTION', 'FUNCTIONS', 'GLOBAL', 'GRANT', 'GROUP', 'GROUPING', 'HAVING', 'HOUR', 'IF', 'IGNORE', 'IMPORT', 'IN', 'INDEX', 'INDEXES', 'INNER', 'INPATH', 'INPUTFORMAT', 'INSERT', 'INTERSECT', 'INTERVAL', 'INTO', 'IS', 'ITEMS', 'JOIN', 'KEYS', 'LAST', 'LATERAL', 'LAZY', 'LEADING', 'LEFT', 'LIKE', 'LIMIT', 'LINES', 'LIST', 'LOAD', 'LOCAL', 'LOCATION', 'LOCK', 'LOCKS', 'LOGICAL', 'MACRO', 'MAP', 'MATCHED', 'MERGE', 'MINUTE', 'MONTH', 'MSCK', 'NAMESPACE', 'NAMESPACES', 'NATURAL', 'NO', NOT, 'NULL', 'NULLS', 'OF', 'ON', 'ONLY', 'OPTION', 'OPTIONS', 'OR', 'ORDER', 'OUT', 'OUTER', 'OUTPUTFORMAT', 'OVER', 'OVERLAPS', 'OVERLAY', 'OVERWRITE', 'PARTITION', 'PARTITIONED', 'PARTITIONS', 'PERCENT', 'PIVOT', 'PLACING', 'POSITION', 'PRECEDING', 'PRIMARY', 'PRINCIPALS', 'PROPERTIES', 'PURGE', 'QUERY', 'RANGE', 'RECORDREADER', 'RECORDWRITER', 'RECOVER', 'REDUCE', 'REFERENCES', 'REFRESH', 'RENAME', 'REPAIR', 'REPLACE', 'RESET', 'RESPECT', 'RESTRICT', 'REVOKE', 'RIGHT', RLIKE, 'ROLE', 'ROLES', 'ROLLBACK', 'ROLLUP', 'ROW', 'ROWS', 'SECOND', 'SCHEMA', 'SELECT', 'SEMI', 'SEPARATED', 'SERDE', 'SERDEPROPERTIES', 'SESSION_USER', 'SET', 'MINUS', 'SETS', 'SHOW', 'SKEWED', 'SOME', 'SORT', 'SORTED', 'START', 'STATISTICS', 'STORED', 'STRATIFY', 'STRUCT', 'SUBSTR', 'SUBSTRING', 'SYNC', 'TABLE', 'TABLES', 'TABLESAMPLE', 'TBLPROPERTIES', TEMPORARY, 'TERMINATED', 'THEN', 'TIME', 'TO', 'TOUCH', 'TRAILING', 'TRANSACTION', 'TRANSACTIONS', 'TRANSFORM', 'TRIM', 'TRUE', 'TRUNCATE', 'TRY_CAST', 'TYPE', 'UNARCHIVE', 'UNBOUNDED', 'UNCACHE', 'UNION', 'UNIQUE', 'UNKNOWN', 'UNLOCK', 'UNSET', 'UPDATE', 'USE', 'USER', 'USING', 'VALUES', 'VIEW', 'VIEWS', 'WHEN', 'WHERE', 'WINDOW', 'WITH', 'YEAR', 'ZONE', '+', '-', '*', '~', STRING, BIGINT_LITERAL, SMALLINT_LITERAL, TINYINT_LITERAL, INTEGER_VALUE, EXPONENT_VALUE, DECIMAL_VALUE, FLOAT_LITERAL, DOUBLE_LITERAL, BIGDECIMAL_LITERAL, IDENTIFIER, BACKQUOTED_IDENTIFIER}(line 1, pos 28)",
            "log_location":"(write) log_w_sql_parquet (line 3610)",
            "pass":true
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws66; line 1 pos 14;",
            "write_value":"No output, find exception: <console>:1: error: illegal start of simple expression",
            "log_location":"(read) log_w_df_r_df_parquet (line 1230)",
            "pass":true
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: mismatched input '<EOF>' expecting {'(', 'ADD', 'AFTER', 'ALL', 'ALTER', 'ANALYZE', 'AND', 'ANTI', 'ANY', 'ARCHIVE', 'ARRAY', 'AS', 'ASC', 'AT', 'AUTHORIZATION', 'BETWEEN', 'BOTH', 'BUCKET', 'BUCKETS', 'BY', 'CACHE', 'CASCADE', 'CASE', 'CAST', 'CHANGE', 'CHECK', 'CLEAR', 'CLUSTER', 'CLUSTERED', 'CODEGEN', 'COLLATE', 'COLLECTION', 'COLUMN', 'COLUMNS', 'COMMENT', 'COMMIT', 'COMPACT', 'COMPACTIONS', 'COMPUTE', 'CONCATENATE', 'CONSTRAINT', 'COST', 'CREATE', 'CROSS', 'CUBE', 'CURRENT', 'CURRENT_DATE', 'CURRENT_TIME', 'CURRENT_TIMESTAMP', 'CURRENT_USER', 'DAY', 'DATA', 'DATABASE', DATABASES, 'DBPROPERTIES', 'DEFINED', 'DELETE', 'DELIMITED', 'DESC', 'DESCRIBE', 'DFS', 'DIRECTORIES', 'DIRECTORY', 'DISTINCT', 'DISTRIBUTE', 'DIV', 'DROP', 'ELSE', 'END', 'ESCAPE', 'ESCAPED', 'EXCEPT', 'EXCHANGE', 'EXISTS', 'EXPLAIN', 'EXPORT', 'EXTENDED', 'EXTERNAL', 'EXTRACT', 'FALSE', 'FETCH', 'FIELDS', 'FILTER', 'FILEFORMAT', 'FIRST', 'FOLLOWING', 'FOR', 'FOREIGN', 'FORMAT', 'FORMATTED', 'FROM', 'FULL', 'FUNCTION', 'FUNCTIONS', 'GLOBAL', 'GRANT', 'GROUP', 'GROUPING', 'HAVING', 'HOUR', 'IF', 'IGNORE', 'IMPORT', 'IN', 'INDEX', 'INDEXES', 'INNER', 'INPATH', 'INPUTFORMAT', 'INSERT', 'INTERSECT', 'INTERVAL', 'INTO', 'IS', 'ITEMS', 'JOIN', 'KEYS', 'LAST', 'LATERAL', 'LAZY', 'LEADING', 'LEFT', 'LIKE', 'LIMIT', 'LINES', 'LIST', 'LOAD', 'LOCAL', 'LOCATION', 'LOCK', 'LOCKS', 'LOGICAL', 'MACRO', 'MAP', 'MATCHED', 'MERGE', 'MINUTE', 'MONTH', 'MSCK', 'NAMESPACE', 'NAMESPACES', 'NATURAL', 'NO', NOT, 'NULL', 'NULLS', 'OF', 'ON', 'ONLY', 'OPTION', 'OPTIONS', 'OR', 'ORDER', 'OUT', 'OUTER', 'OUTPUTFORMAT', 'OVER', 'OVERLAPS', 'OVERLAY', 'OVERWRITE', 'PARTITION', 'PARTITIONED', 'PARTITIONS', 'PERCENT', 'PIVOT', 'PLACING', 'POSITION', 'PRECEDING', 'PRIMARY', 'PRINCIPALS', 'PROPERTIES', 'PURGE', 'QUERY', 'RANGE', 'RECORDREADER', 'RECORDWRITER', 'RECOVER', 'REDUCE', 'REFERENCES', 'REFRESH', 'RENAME', 'REPAIR', 'REPLACE', 'RESET', 'RESPECT', 'RESTRICT', 'REVOKE', 'RIGHT', RLIKE, 'ROLE', 'ROLES', 'ROLLBACK', 'ROLLUP', 'ROW', 'ROWS', 'SECOND', 'SCHEMA', 'SELECT', 'SEMI', 'SEPARATED', 'SERDE', 'SERDEPROPERTIES', 'SESSION_USER', 'SET', 'MINUS', 'SETS', 'SHOW', 'SKEWED', 'SOME', 'SORT', 'SORTED', 'START', 'STATISTICS', 'STORED', 'STRATIFY', 'STRUCT', 'SUBSTR', 'SUBSTRING', 'SYNC', 'TABLE', 'TABLES', 'TABLESAMPLE', 'TBLPROPERTIES', TEMPORARY, 'TERMINATED', 'THEN', 'TIME', 'TO', 'TOUCH', 'TRAILING', 'TRANSACTION', 'TRANSACTIONS', 'TRANSFORM', 'TRIM', 'TRUE', 'TRUNCATE', 'TRY_CAST', 'TYPE', 'UNARCHIVE', 'UNBOUNDED', 'UNCACHE', 'UNION', 'UNIQUE', 'UNKNOWN', 'UNLOCK', 'UNSET', 'UPDATE', 'USE', 'USER', 'USING', 'VALUES', 'VIEW', 'VIEWS', 'WHEN', 'WHERE', 'WINDOW', 'WITH', 'YEAR', 'ZONE', '+', '-', '*', '~', STRING, BIGINT_LITERAL, SMALLINT_LITERAL, TINYINT_LITERAL, INTEGER_VALUE, EXPONENT_VALUE, DECIMAL_VALUE, FLOAT_LITERAL, DOUBLE_LITERAL, BIGDECIMAL_LITERAL, IDENTIFIER, BACKQUOTED_IDENTIFIER}(line 1, pos 28)",
            "log_location":"(write) log_w_sql_orc (line 3609)",
            "pass":true
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: mismatched input '<EOF>' expecting {'(', 'ADD', 'AFTER', 'ALL', 'ALTER', 'ANALYZE', 'AND', 'ANTI', 'ANY', 'ARCHIVE', 'ARRAY', 'AS', 'ASC', 'AT', 'AUTHORIZATION', 'BETWEEN', 'BOTH', 'BUCKET', 'BUCKETS', 'BY', 'CACHE', 'CASCADE', 'CASE', 'CAST', 'CHANGE', 'CHECK', 'CLEAR', 'CLUSTER', 'CLUSTERED', 'CODEGEN', 'COLLATE', 'COLLECTION', 'COLUMN', 'COLUMNS', 'COMMENT', 'COMMIT', 'COMPACT', 'COMPACTIONS', 'COMPUTE', 'CONCATENATE', 'CONSTRAINT', 'COST', 'CREATE', 'CROSS', 'CUBE', 'CURRENT', 'CURRENT_DATE', 'CURRENT_TIME', 'CURRENT_TIMESTAMP', 'CURRENT_USER', 'DAY', 'DATA', 'DATABASE', DATABASES, 'DBPROPERTIES', 'DEFINED', 'DELETE', 'DELIMITED', 'DESC', 'DESCRIBE', 'DFS', 'DIRECTORIES', 'DIRECTORY', 'DISTINCT', 'DISTRIBUTE', 'DIV', 'DROP', 'ELSE', 'END', 'ESCAPE', 'ESCAPED', 'EXCEPT', 'EXCHANGE', 'EXISTS', 'EXPLAIN', 'EXPORT', 'EXTENDED', 'EXTERNAL', 'EXTRACT', 'FALSE', 'FETCH', 'FIELDS', 'FILTER', 'FILEFORMAT', 'FIRST', 'FOLLOWING', 'FOR', 'FOREIGN', 'FORMAT', 'FORMATTED', 'FROM', 'FULL', 'FUNCTION', 'FUNCTIONS', 'GLOBAL', 'GRANT', 'GROUP', 'GROUPING', 'HAVING', 'HOUR', 'IF', 'IGNORE', 'IMPORT', 'IN', 'INDEX', 'INDEXES', 'INNER', 'INPATH', 'INPUTFORMAT', 'INSERT', 'INTERSECT', 'INTERVAL', 'INTO', 'IS', 'ITEMS', 'JOIN', 'KEYS', 'LAST', 'LATERAL', 'LAZY', 'LEADING', 'LEFT', 'LIKE', 'LIMIT', 'LINES', 'LIST', 'LOAD', 'LOCAL', 'LOCATION', 'LOCK', 'LOCKS', 'LOGICAL', 'MACRO', 'MAP', 'MATCHED', 'MERGE', 'MINUTE', 'MONTH', 'MSCK', 'NAMESPACE', 'NAMESPACES', 'NATURAL', 'NO', NOT, 'NULL', 'NULLS', 'OF', 'ON', 'ONLY', 'OPTION', 'OPTIONS', 'OR', 'ORDER', 'OUT', 'OUTER', 'OUTPUTFORMAT', 'OVER', 'OVERLAPS', 'OVERLAY', 'OVERWRITE', 'PARTITION', 'PARTITIONED', 'PARTITIONS', 'PERCENT', 'PIVOT', 'PLACING', 'POSITION', 'PRECEDING', 'PRIMARY', 'PRINCIPALS', 'PROPERTIES', 'PURGE', 'QUERY', 'RANGE', 'RECORDREADER', 'RECORDWRITER', 'RECOVER', 'REDUCE', 'REFERENCES', 'REFRESH', 'RENAME', 'REPAIR', 'REPLACE', 'RESET', 'RESPECT', 'RESTRICT', 'REVOKE', 'RIGHT', RLIKE, 'ROLE', 'ROLES', 'ROLLBACK', 'ROLLUP', 'ROW', 'ROWS', 'SECOND', 'SCHEMA', 'SELECT', 'SEMI', 'SEPARATED', 'SERDE', 'SERDEPROPERTIES', 'SESSION_USER', 'SET', 'MINUS', 'SETS', 'SHOW', 'SKEWED', 'SOME', 'SORT', 'SORTED', 'START', 'STATISTICS', 'STORED', 'STRATIFY', 'STRUCT', 'SUBSTR', 'SUBSTRING', 'SYNC', 'TABLE', 'TABLES', 'TABLESAMPLE', 'TBLPROPERTIES', TEMPORARY, 'TERMINATED', 'THEN', 'TIME', 'TO', 'TOUCH', 'TRAILING', 'TRANSACTION', 'TRANSACTIONS', 'TRANSFORM', 'TRIM', 'TRUE', 'TRUNCATE', 'TRY_CAST', 'TYPE', 'UNARCHIVE', 'UNBOUNDED', 'UNCACHE', 'UNION', 'UNIQUE', 'UNKNOWN', 'UNLOCK', 'UNSET', 'UPDATE', 'USE', 'USER', 'USING', 'VALUES', 'VIEW', 'VIEWS', 'WHEN', 'WHERE', 'WINDOW', 'WITH', 'YEAR', 'ZONE', '+', '-', '*', '~', STRING, BIGINT_LITERAL, SMALLINT_LITERAL, TINYINT_LITERAL, INTEGER_VALUE, EXPONENT_VALUE, DECIMAL_VALUE, FLOAT_LITERAL, DOUBLE_LITERAL, BIGDECIMAL_LITERAL, IDENTIFIER, BACKQUOTED_IDENTIFIER}(line 1, pos 28)",
            "log_location":"(write) log_w_sql_parquet (line 3610)",
            "pass":true
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws66; line 1 pos 14;",
            "write_value":"No output, find exception: <console>:1: error: illegal start of simple expression",
            "log_location":"(read) log_w_df_r_df_avro (line 2918)",
            "pass":true
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: <console>:1: error: illegal start of simple expression",
            "log_location":"(write) log_w_df_orc (line 13499)",
            "pass":true
        }
    },
    "67":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: 'InsertIntoStatement 'UnresolvedRelation [ws67], [], false, false, false",
            "log_location":"(write) log_w_sql_avro (line 2672)",
            "pass":true
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws67; line 1 pos 14;",
            "write_value":"No output, find exception: <console>:28: error: not found: value foo",
            "log_location":"(read) log_w_df_r_df_orc (line 1268)",
            "pass":true
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: 'InsertIntoStatement 'UnresolvedRelation [ws67], [], false, false, false",
            "log_location":"(write) log_w_sql_orc (line 3625)",
            "pass":true
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: <console>:28: error: not found: value foo",
            "log_location":"(write) log_w_df_avro (line 5930)",
            "pass":true
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: 'InsertIntoStatement 'UnresolvedRelation [ws67], [], false, false, false",
            "log_location":"(write) log_w_sql_avro (line 2672)",
            "pass":true
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: <console>:28: error: not found: value foo",
            "log_location":"(write) log_w_df_parquet (line 13525)",
            "pass":true
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: 'InsertIntoStatement 'UnresolvedRelation [ws67], [], false, false, false",
            "log_location":"(write) log_w_sql_parquet (line 3626)",
            "pass":true
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws67; line 1 pos 14;",
            "write_value":"No output, find exception: <console>:28: error: not found: value foo",
            "log_location":"(read) log_w_df_r_df_parquet (line 1270)",
            "pass":true
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: 'InsertIntoStatement 'UnresolvedRelation [ws67], [], false, false, false",
            "log_location":"(write) log_w_sql_orc (line 3625)",
            "pass":true
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: 'InsertIntoStatement 'UnresolvedRelation [ws67], [], false, false, false",
            "log_location":"(write) log_w_sql_parquet (line 3626)",
            "pass":true
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws67; line 1 pos 14;",
            "write_value":"No output, find exception: <console>:28: error: not found: value foo",
            "log_location":"(read) log_w_df_r_df_avro (line 2958)",
            "pass":true
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: <console>:28: error: not found: value foo",
            "log_location":"(write) log_w_df_orc (line 13522)",
            "pass":true
        }
    },
    "70":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"33333333333.2222222222, find exception: java.lang.ArithmeticException: Decimal(expanded,33333333333.2222222222,21,10}) cannot be represented as Decimal(20, 10).",
            "log_location":"(write) log_w_sql_avro (line 2703)",
            "pass":true
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"33333333333.2222222222, find exception: java.lang.ArithmeticException: Decimal(expanded,33333333333.2222222222,21,10}) cannot be represented as Decimal(20, 10).",
            "log_location":"(write) log_w_sql_orc (line 3653)",
            "pass":true
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"33333333333.2222222222, find exception: java.lang.ArithmeticException: Decimal(expanded,33333333333.2222222222,21,10}) cannot be represented as Decimal(20, 10).",
            "log_location":"(write) log_w_sql_avro (line 2703)",
            "pass":true
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"33333333333.2222222222, find exception: java.lang.ArithmeticException: Decimal(expanded,33333333333.2222222222,21,10}) cannot be represented as Decimal(20, 10).",
            "log_location":"(write) log_w_sql_parquet (line 3654)",
            "pass":true
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"33333333333.2222222222, find exception: java.lang.ArithmeticException: Decimal(expanded,33333333333.2222222222,21,10}) cannot be represented as Decimal(20, 10).",
            "log_location":"(write) log_w_sql_orc (line 3653)",
            "pass":true
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"33333333333.2222222222, find exception: java.lang.ArithmeticException: Decimal(expanded,33333333333.2222222222,21,10}) cannot be represented as Decimal(20, 10).",
            "log_location":"(write) log_w_sql_parquet (line 3654)",
            "pass":true
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        }
    },
    "71":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws71; line 1 pos 14;",
            "write_value":"No output, find exception: <console>:28: error: not found: value rdd71",
            "log_location":"(read) log_w_df_r_df_orc (line 1332)",
            "pass":true
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: <console>:28: error: not found: value rdd71",
            "log_location":"(write) log_w_df_avro (line 6262)",
            "pass":true
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: <console>:28: error: not found: value rdd71",
            "log_location":"(write) log_w_df_parquet (line 13857)",
            "pass":true
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws71; line 1 pos 14;",
            "write_value":"No output, find exception: <console>:28: error: not found: value rdd71",
            "log_location":"(read) log_w_df_r_df_parquet (line 1334)",
            "pass":true
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws71; line 1 pos 14;",
            "write_value":"No output, find exception: <console>:28: error: not found: value rdd71",
            "log_location":"(read) log_w_df_r_df_avro (line 3022)",
            "pass":true
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: <console>:28: error: not found: value rdd71",
            "log_location":"(write) log_w_df_orc (line 13854)",
            "pass":true
        }
    },
    "83":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: mismatched input '\"' expecting {'(', 'ADD', 'AFTER', 'ALL', 'ALTER', 'ANALYZE', 'AND', 'ANTI', 'ANY', 'ARCHIVE', 'ARRAY', 'AS', 'ASC', 'AT', 'AUTHORIZATION', 'BETWEEN', 'BOTH', 'BUCKET', 'BUCKETS', 'BY', 'CACHE', 'CASCADE', 'CASE', 'CAST', 'CHANGE', 'CHECK', 'CLEAR', 'CLUSTER', 'CLUSTERED', 'CODEGEN', 'COLLATE', 'COLLECTION', 'COLUMN', 'COLUMNS', 'COMMENT', 'COMMIT', 'COMPACT', 'COMPACTIONS', 'COMPUTE', 'CONCATENATE', 'CONSTRAINT', 'COST', 'CREATE', 'CROSS', 'CUBE', 'CURRENT', 'CURRENT_DATE', 'CURRENT_TIME', 'CURRENT_TIMESTAMP', 'CURRENT_USER', 'DAY', 'DATA', 'DATABASE', DATABASES, 'DBPROPERTIES', 'DEFINED', 'DELETE', 'DELIMITED', 'DESC', 'DESCRIBE', 'DFS', 'DIRECTORIES', 'DIRECTORY', 'DISTINCT', 'DISTRIBUTE', 'DIV', 'DROP', 'ELSE', 'END', 'ESCAPE', 'ESCAPED', 'EXCEPT', 'EXCHANGE', 'EXISTS', 'EXPLAIN', 'EXPORT', 'EXTENDED', 'EXTERNAL', 'EXTRACT', 'FALSE', 'FETCH', 'FIELDS', 'FILTER', 'FILEFORMAT', 'FIRST', 'FOLLOWING', 'FOR', 'FOREIGN', 'FORMAT', 'FORMATTED', 'FROM', 'FULL', 'FUNCTION', 'FUNCTIONS', 'GLOBAL', 'GRANT', 'GROUP', 'GROUPING', 'HAVING', 'HOUR', 'IF', 'IGNORE', 'IMPORT', 'IN', 'INDEX', 'INDEXES', 'INNER', 'INPATH', 'INPUTFORMAT', 'INSERT', 'INTERSECT', 'INTERVAL', 'INTO', 'IS', 'ITEMS', 'JOIN', 'KEYS', 'LAST', 'LATERAL', 'LAZY', 'LEADING', 'LEFT', 'LIKE', 'LIMIT', 'LINES', 'LIST', 'LOAD', 'LOCAL', 'LOCATION', 'LOCK', 'LOCKS', 'LOGICAL', 'MACRO', 'MAP', 'MATCHED', 'MERGE', 'MINUTE', 'MONTH', 'MSCK', 'NAMESPACE', 'NAMESPACES', 'NATURAL', 'NO', NOT, 'NULL', 'NULLS', 'OF', 'ON', 'ONLY', 'OPTION', 'OPTIONS', 'OR', 'ORDER', 'OUT', 'OUTER', 'OUTPUTFORMAT', 'OVER', 'OVERLAPS', 'OVERLAY', 'OVERWRITE', 'PARTITION', 'PARTITIONED', 'PARTITIONS', 'PERCENT', 'PIVOT', 'PLACING', 'POSITION', 'PRECEDING', 'PRIMARY', 'PRINCIPALS', 'PROPERTIES', 'PURGE', 'QUERY', 'RANGE', 'RECORDREADER', 'RECORDWRITER', 'RECOVER', 'REDUCE', 'REFERENCES', 'REFRESH', 'RENAME', 'REPAIR', 'REPLACE', 'RESET', 'RESPECT', 'RESTRICT', 'REVOKE', 'RIGHT', RLIKE, 'ROLE', 'ROLES', 'ROLLBACK', 'ROLLUP', 'ROW', 'ROWS', 'SECOND', 'SCHEMA', 'SELECT', 'SEMI', 'SEPARATED', 'SERDE', 'SERDEPROPERTIES', 'SESSION_USER', 'SET', 'MINUS', 'SETS', 'SHOW', 'SKEWED', 'SOME', 'SORT', 'SORTED', 'START', 'STATISTICS', 'STORED', 'STRATIFY', 'STRUCT', 'SUBSTR', 'SUBSTRING', 'SYNC', 'TABLE', 'TABLES', 'TABLESAMPLE', 'TBLPROPERTIES', TEMPORARY, 'TERMINATED', 'THEN', 'TIME', 'TO', 'TOUCH', 'TRAILING', 'TRANSACTION', 'TRANSACTIONS', 'TRANSFORM', 'TRIM', 'TRUE', 'TRUNCATE', 'TRY_CAST', 'TYPE', 'UNARCHIVE', 'UNBOUNDED', 'UNCACHE', 'UNION', 'UNIQUE', 'UNKNOWN', 'UNLOCK', 'UNSET', 'UPDATE', 'USE', 'USER', 'USING', 'VALUES', 'VIEW', 'VIEWS', 'WHEN', 'WHERE', 'WINDOW', 'WITH', 'YEAR', 'ZONE', '+', '-', '*', '~', STRING, BIGINT_LITERAL, SMALLINT_LITERAL, TINYINT_LITERAL, INTEGER_VALUE, EXPONENT_VALUE, DECIMAL_VALUE, FLOAT_LITERAL, DOUBLE_LITERAL, BIGDECIMAL_LITERAL, IDENTIFIER, BACKQUOTED_IDENTIFIER}(line 1, pos 28)",
            "log_location":"(write) log_w_sql_avro (line 3094)",
            "pass":true
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws83; line 1 pos 14;",
            "write_value":"No output, find exception: <console>:1: error: unclosed string literal",
            "log_location":"(read) log_w_df_r_df_orc (line 1460)",
            "pass":true
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: mismatched input '\"' expecting {'(', 'ADD', 'AFTER', 'ALL', 'ALTER', 'ANALYZE', 'AND', 'ANTI', 'ANY', 'ARCHIVE', 'ARRAY', 'AS', 'ASC', 'AT', 'AUTHORIZATION', 'BETWEEN', 'BOTH', 'BUCKET', 'BUCKETS', 'BY', 'CACHE', 'CASCADE', 'CASE', 'CAST', 'CHANGE', 'CHECK', 'CLEAR', 'CLUSTER', 'CLUSTERED', 'CODEGEN', 'COLLATE', 'COLLECTION', 'COLUMN', 'COLUMNS', 'COMMENT', 'COMMIT', 'COMPACT', 'COMPACTIONS', 'COMPUTE', 'CONCATENATE', 'CONSTRAINT', 'COST', 'CREATE', 'CROSS', 'CUBE', 'CURRENT', 'CURRENT_DATE', 'CURRENT_TIME', 'CURRENT_TIMESTAMP', 'CURRENT_USER', 'DAY', 'DATA', 'DATABASE', DATABASES, 'DBPROPERTIES', 'DEFINED', 'DELETE', 'DELIMITED', 'DESC', 'DESCRIBE', 'DFS', 'DIRECTORIES', 'DIRECTORY', 'DISTINCT', 'DISTRIBUTE', 'DIV', 'DROP', 'ELSE', 'END', 'ESCAPE', 'ESCAPED', 'EXCEPT', 'EXCHANGE', 'EXISTS', 'EXPLAIN', 'EXPORT', 'EXTENDED', 'EXTERNAL', 'EXTRACT', 'FALSE', 'FETCH', 'FIELDS', 'FILTER', 'FILEFORMAT', 'FIRST', 'FOLLOWING', 'FOR', 'FOREIGN', 'FORMAT', 'FORMATTED', 'FROM', 'FULL', 'FUNCTION', 'FUNCTIONS', 'GLOBAL', 'GRANT', 'GROUP', 'GROUPING', 'HAVING', 'HOUR', 'IF', 'IGNORE', 'IMPORT', 'IN', 'INDEX', 'INDEXES', 'INNER', 'INPATH', 'INPUTFORMAT', 'INSERT', 'INTERSECT', 'INTERVAL', 'INTO', 'IS', 'ITEMS', 'JOIN', 'KEYS', 'LAST', 'LATERAL', 'LAZY', 'LEADING', 'LEFT', 'LIKE', 'LIMIT', 'LINES', 'LIST', 'LOAD', 'LOCAL', 'LOCATION', 'LOCK', 'LOCKS', 'LOGICAL', 'MACRO', 'MAP', 'MATCHED', 'MERGE', 'MINUTE', 'MONTH', 'MSCK', 'NAMESPACE', 'NAMESPACES', 'NATURAL', 'NO', NOT, 'NULL', 'NULLS', 'OF', 'ON', 'ONLY', 'OPTION', 'OPTIONS', 'OR', 'ORDER', 'OUT', 'OUTER', 'OUTPUTFORMAT', 'OVER', 'OVERLAPS', 'OVERLAY', 'OVERWRITE', 'PARTITION', 'PARTITIONED', 'PARTITIONS', 'PERCENT', 'PIVOT', 'PLACING', 'POSITION', 'PRECEDING', 'PRIMARY', 'PRINCIPALS', 'PROPERTIES', 'PURGE', 'QUERY', 'RANGE', 'RECORDREADER', 'RECORDWRITER', 'RECOVER', 'REDUCE', 'REFERENCES', 'REFRESH', 'RENAME', 'REPAIR', 'REPLACE', 'RESET', 'RESPECT', 'RESTRICT', 'REVOKE', 'RIGHT', RLIKE, 'ROLE', 'ROLES', 'ROLLBACK', 'ROLLUP', 'ROW', 'ROWS', 'SECOND', 'SCHEMA', 'SELECT', 'SEMI', 'SEPARATED', 'SERDE', 'SERDEPROPERTIES', 'SESSION_USER', 'SET', 'MINUS', 'SETS', 'SHOW', 'SKEWED', 'SOME', 'SORT', 'SORTED', 'START', 'STATISTICS', 'STORED', 'STRATIFY', 'STRUCT', 'SUBSTR', 'SUBSTRING', 'SYNC', 'TABLE', 'TABLES', 'TABLESAMPLE', 'TBLPROPERTIES', TEMPORARY, 'TERMINATED', 'THEN', 'TIME', 'TO', 'TOUCH', 'TRAILING', 'TRANSACTION', 'TRANSACTIONS', 'TRANSFORM', 'TRIM', 'TRUE', 'TRUNCATE', 'TRY_CAST', 'TYPE', 'UNARCHIVE', 'UNBOUNDED', 'UNCACHE', 'UNION', 'UNIQUE', 'UNKNOWN', 'UNLOCK', 'UNSET', 'UPDATE', 'USE', 'USER', 'USING', 'VALUES', 'VIEW', 'VIEWS', 'WHEN', 'WHERE', 'WINDOW', 'WITH', 'YEAR', 'ZONE', '+', '-', '*', '~', STRING, BIGINT_LITERAL, SMALLINT_LITERAL, TINYINT_LITERAL, INTEGER_VALUE, EXPONENT_VALUE, DECIMAL_VALUE, FLOAT_LITERAL, DOUBLE_LITERAL, BIGDECIMAL_LITERAL, IDENTIFIER, BACKQUOTED_IDENTIFIER}(line 1, pos 28)",
            "log_location":"(write) log_w_sql_orc (line 4031)",
            "pass":true
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: <console>:1: error: unclosed string literal",
            "log_location":"(write) log_w_df_avro (line 6486)",
            "pass":true
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: mismatched input '\"' expecting {'(', 'ADD', 'AFTER', 'ALL', 'ALTER', 'ANALYZE', 'AND', 'ANTI', 'ANY', 'ARCHIVE', 'ARRAY', 'AS', 'ASC', 'AT', 'AUTHORIZATION', 'BETWEEN', 'BOTH', 'BUCKET', 'BUCKETS', 'BY', 'CACHE', 'CASCADE', 'CASE', 'CAST', 'CHANGE', 'CHECK', 'CLEAR', 'CLUSTER', 'CLUSTERED', 'CODEGEN', 'COLLATE', 'COLLECTION', 'COLUMN', 'COLUMNS', 'COMMENT', 'COMMIT', 'COMPACT', 'COMPACTIONS', 'COMPUTE', 'CONCATENATE', 'CONSTRAINT', 'COST', 'CREATE', 'CROSS', 'CUBE', 'CURRENT', 'CURRENT_DATE', 'CURRENT_TIME', 'CURRENT_TIMESTAMP', 'CURRENT_USER', 'DAY', 'DATA', 'DATABASE', DATABASES, 'DBPROPERTIES', 'DEFINED', 'DELETE', 'DELIMITED', 'DESC', 'DESCRIBE', 'DFS', 'DIRECTORIES', 'DIRECTORY', 'DISTINCT', 'DISTRIBUTE', 'DIV', 'DROP', 'ELSE', 'END', 'ESCAPE', 'ESCAPED', 'EXCEPT', 'EXCHANGE', 'EXISTS', 'EXPLAIN', 'EXPORT', 'EXTENDED', 'EXTERNAL', 'EXTRACT', 'FALSE', 'FETCH', 'FIELDS', 'FILTER', 'FILEFORMAT', 'FIRST', 'FOLLOWING', 'FOR', 'FOREIGN', 'FORMAT', 'FORMATTED', 'FROM', 'FULL', 'FUNCTION', 'FUNCTIONS', 'GLOBAL', 'GRANT', 'GROUP', 'GROUPING', 'HAVING', 'HOUR', 'IF', 'IGNORE', 'IMPORT', 'IN', 'INDEX', 'INDEXES', 'INNER', 'INPATH', 'INPUTFORMAT', 'INSERT', 'INTERSECT', 'INTERVAL', 'INTO', 'IS', 'ITEMS', 'JOIN', 'KEYS', 'LAST', 'LATERAL', 'LAZY', 'LEADING', 'LEFT', 'LIKE', 'LIMIT', 'LINES', 'LIST', 'LOAD', 'LOCAL', 'LOCATION', 'LOCK', 'LOCKS', 'LOGICAL', 'MACRO', 'MAP', 'MATCHED', 'MERGE', 'MINUTE', 'MONTH', 'MSCK', 'NAMESPACE', 'NAMESPACES', 'NATURAL', 'NO', NOT, 'NULL', 'NULLS', 'OF', 'ON', 'ONLY', 'OPTION', 'OPTIONS', 'OR', 'ORDER', 'OUT', 'OUTER', 'OUTPUTFORMAT', 'OVER', 'OVERLAPS', 'OVERLAY', 'OVERWRITE', 'PARTITION', 'PARTITIONED', 'PARTITIONS', 'PERCENT', 'PIVOT', 'PLACING', 'POSITION', 'PRECEDING', 'PRIMARY', 'PRINCIPALS', 'PROPERTIES', 'PURGE', 'QUERY', 'RANGE', 'RECORDREADER', 'RECORDWRITER', 'RECOVER', 'REDUCE', 'REFERENCES', 'REFRESH', 'RENAME', 'REPAIR', 'REPLACE', 'RESET', 'RESPECT', 'RESTRICT', 'REVOKE', 'RIGHT', RLIKE, 'ROLE', 'ROLES', 'ROLLBACK', 'ROLLUP', 'ROW', 'ROWS', 'SECOND', 'SCHEMA', 'SELECT', 'SEMI', 'SEPARATED', 'SERDE', 'SERDEPROPERTIES', 'SESSION_USER', 'SET', 'MINUS', 'SETS', 'SHOW', 'SKEWED', 'SOME', 'SORT', 'SORTED', 'START', 'STATISTICS', 'STORED', 'STRATIFY', 'STRUCT', 'SUBSTR', 'SUBSTRING', 'SYNC', 'TABLE', 'TABLES', 'TABLESAMPLE', 'TBLPROPERTIES', TEMPORARY, 'TERMINATED', 'THEN', 'TIME', 'TO', 'TOUCH', 'TRAILING', 'TRANSACTION', 'TRANSACTIONS', 'TRANSFORM', 'TRIM', 'TRUE', 'TRUNCATE', 'TRY_CAST', 'TYPE', 'UNARCHIVE', 'UNBOUNDED', 'UNCACHE', 'UNION', 'UNIQUE', 'UNKNOWN', 'UNLOCK', 'UNSET', 'UPDATE', 'USE', 'USER', 'USING', 'VALUES', 'VIEW', 'VIEWS', 'WHEN', 'WHERE', 'WINDOW', 'WITH', 'YEAR', 'ZONE', '+', '-', '*', '~', STRING, BIGINT_LITERAL, SMALLINT_LITERAL, TINYINT_LITERAL, INTEGER_VALUE, EXPONENT_VALUE, DECIMAL_VALUE, FLOAT_LITERAL, DOUBLE_LITERAL, BIGDECIMAL_LITERAL, IDENTIFIER, BACKQUOTED_IDENTIFIER}(line 1, pos 28)",
            "log_location":"(write) log_w_sql_avro (line 3094)",
            "pass":true
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: <console>:1: error: unclosed string literal",
            "log_location":"(write) log_w_df_parquet (line 14081)",
            "pass":true
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: mismatched input '\"' expecting {'(', 'ADD', 'AFTER', 'ALL', 'ALTER', 'ANALYZE', 'AND', 'ANTI', 'ANY', 'ARCHIVE', 'ARRAY', 'AS', 'ASC', 'AT', 'AUTHORIZATION', 'BETWEEN', 'BOTH', 'BUCKET', 'BUCKETS', 'BY', 'CACHE', 'CASCADE', 'CASE', 'CAST', 'CHANGE', 'CHECK', 'CLEAR', 'CLUSTER', 'CLUSTERED', 'CODEGEN', 'COLLATE', 'COLLECTION', 'COLUMN', 'COLUMNS', 'COMMENT', 'COMMIT', 'COMPACT', 'COMPACTIONS', 'COMPUTE', 'CONCATENATE', 'CONSTRAINT', 'COST', 'CREATE', 'CROSS', 'CUBE', 'CURRENT', 'CURRENT_DATE', 'CURRENT_TIME', 'CURRENT_TIMESTAMP', 'CURRENT_USER', 'DAY', 'DATA', 'DATABASE', DATABASES, 'DBPROPERTIES', 'DEFINED', 'DELETE', 'DELIMITED', 'DESC', 'DESCRIBE', 'DFS', 'DIRECTORIES', 'DIRECTORY', 'DISTINCT', 'DISTRIBUTE', 'DIV', 'DROP', 'ELSE', 'END', 'ESCAPE', 'ESCAPED', 'EXCEPT', 'EXCHANGE', 'EXISTS', 'EXPLAIN', 'EXPORT', 'EXTENDED', 'EXTERNAL', 'EXTRACT', 'FALSE', 'FETCH', 'FIELDS', 'FILTER', 'FILEFORMAT', 'FIRST', 'FOLLOWING', 'FOR', 'FOREIGN', 'FORMAT', 'FORMATTED', 'FROM', 'FULL', 'FUNCTION', 'FUNCTIONS', 'GLOBAL', 'GRANT', 'GROUP', 'GROUPING', 'HAVING', 'HOUR', 'IF', 'IGNORE', 'IMPORT', 'IN', 'INDEX', 'INDEXES', 'INNER', 'INPATH', 'INPUTFORMAT', 'INSERT', 'INTERSECT', 'INTERVAL', 'INTO', 'IS', 'ITEMS', 'JOIN', 'KEYS', 'LAST', 'LATERAL', 'LAZY', 'LEADING', 'LEFT', 'LIKE', 'LIMIT', 'LINES', 'LIST', 'LOAD', 'LOCAL', 'LOCATION', 'LOCK', 'LOCKS', 'LOGICAL', 'MACRO', 'MAP', 'MATCHED', 'MERGE', 'MINUTE', 'MONTH', 'MSCK', 'NAMESPACE', 'NAMESPACES', 'NATURAL', 'NO', NOT, 'NULL', 'NULLS', 'OF', 'ON', 'ONLY', 'OPTION', 'OPTIONS', 'OR', 'ORDER', 'OUT', 'OUTER', 'OUTPUTFORMAT', 'OVER', 'OVERLAPS', 'OVERLAY', 'OVERWRITE', 'PARTITION', 'PARTITIONED', 'PARTITIONS', 'PERCENT', 'PIVOT', 'PLACING', 'POSITION', 'PRECEDING', 'PRIMARY', 'PRINCIPALS', 'PROPERTIES', 'PURGE', 'QUERY', 'RANGE', 'RECORDREADER', 'RECORDWRITER', 'RECOVER', 'REDUCE', 'REFERENCES', 'REFRESH', 'RENAME', 'REPAIR', 'REPLACE', 'RESET', 'RESPECT', 'RESTRICT', 'REVOKE', 'RIGHT', RLIKE, 'ROLE', 'ROLES', 'ROLLBACK', 'ROLLUP', 'ROW', 'ROWS', 'SECOND', 'SCHEMA', 'SELECT', 'SEMI', 'SEPARATED', 'SERDE', 'SERDEPROPERTIES', 'SESSION_USER', 'SET', 'MINUS', 'SETS', 'SHOW', 'SKEWED', 'SOME', 'SORT', 'SORTED', 'START', 'STATISTICS', 'STORED', 'STRATIFY', 'STRUCT', 'SUBSTR', 'SUBSTRING', 'SYNC', 'TABLE', 'TABLES', 'TABLESAMPLE', 'TBLPROPERTIES', TEMPORARY, 'TERMINATED', 'THEN', 'TIME', 'TO', 'TOUCH', 'TRAILING', 'TRANSACTION', 'TRANSACTIONS', 'TRANSFORM', 'TRIM', 'TRUE', 'TRUNCATE', 'TRY_CAST', 'TYPE', 'UNARCHIVE', 'UNBOUNDED', 'UNCACHE', 'UNION', 'UNIQUE', 'UNKNOWN', 'UNLOCK', 'UNSET', 'UPDATE', 'USE', 'USER', 'USING', 'VALUES', 'VIEW', 'VIEWS', 'WHEN', 'WHERE', 'WINDOW', 'WITH', 'YEAR', 'ZONE', '+', '-', '*', '~', STRING, BIGINT_LITERAL, SMALLINT_LITERAL, TINYINT_LITERAL, INTEGER_VALUE, EXPONENT_VALUE, DECIMAL_VALUE, FLOAT_LITERAL, DOUBLE_LITERAL, BIGDECIMAL_LITERAL, IDENTIFIER, BACKQUOTED_IDENTIFIER}(line 1, pos 28)",
            "log_location":"(write) log_w_sql_parquet (line 4032)",
            "pass":true
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws83; line 1 pos 14;",
            "write_value":"No output, find exception: <console>:1: error: unclosed string literal",
            "log_location":"(read) log_w_df_r_df_parquet (line 1462)",
            "pass":true
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: mismatched input '\"' expecting {'(', 'ADD', 'AFTER', 'ALL', 'ALTER', 'ANALYZE', 'AND', 'ANTI', 'ANY', 'ARCHIVE', 'ARRAY', 'AS', 'ASC', 'AT', 'AUTHORIZATION', 'BETWEEN', 'BOTH', 'BUCKET', 'BUCKETS', 'BY', 'CACHE', 'CASCADE', 'CASE', 'CAST', 'CHANGE', 'CHECK', 'CLEAR', 'CLUSTER', 'CLUSTERED', 'CODEGEN', 'COLLATE', 'COLLECTION', 'COLUMN', 'COLUMNS', 'COMMENT', 'COMMIT', 'COMPACT', 'COMPACTIONS', 'COMPUTE', 'CONCATENATE', 'CONSTRAINT', 'COST', 'CREATE', 'CROSS', 'CUBE', 'CURRENT', 'CURRENT_DATE', 'CURRENT_TIME', 'CURRENT_TIMESTAMP', 'CURRENT_USER', 'DAY', 'DATA', 'DATABASE', DATABASES, 'DBPROPERTIES', 'DEFINED', 'DELETE', 'DELIMITED', 'DESC', 'DESCRIBE', 'DFS', 'DIRECTORIES', 'DIRECTORY', 'DISTINCT', 'DISTRIBUTE', 'DIV', 'DROP', 'ELSE', 'END', 'ESCAPE', 'ESCAPED', 'EXCEPT', 'EXCHANGE', 'EXISTS', 'EXPLAIN', 'EXPORT', 'EXTENDED', 'EXTERNAL', 'EXTRACT', 'FALSE', 'FETCH', 'FIELDS', 'FILTER', 'FILEFORMAT', 'FIRST', 'FOLLOWING', 'FOR', 'FOREIGN', 'FORMAT', 'FORMATTED', 'FROM', 'FULL', 'FUNCTION', 'FUNCTIONS', 'GLOBAL', 'GRANT', 'GROUP', 'GROUPING', 'HAVING', 'HOUR', 'IF', 'IGNORE', 'IMPORT', 'IN', 'INDEX', 'INDEXES', 'INNER', 'INPATH', 'INPUTFORMAT', 'INSERT', 'INTERSECT', 'INTERVAL', 'INTO', 'IS', 'ITEMS', 'JOIN', 'KEYS', 'LAST', 'LATERAL', 'LAZY', 'LEADING', 'LEFT', 'LIKE', 'LIMIT', 'LINES', 'LIST', 'LOAD', 'LOCAL', 'LOCATION', 'LOCK', 'LOCKS', 'LOGICAL', 'MACRO', 'MAP', 'MATCHED', 'MERGE', 'MINUTE', 'MONTH', 'MSCK', 'NAMESPACE', 'NAMESPACES', 'NATURAL', 'NO', NOT, 'NULL', 'NULLS', 'OF', 'ON', 'ONLY', 'OPTION', 'OPTIONS', 'OR', 'ORDER', 'OUT', 'OUTER', 'OUTPUTFORMAT', 'OVER', 'OVERLAPS', 'OVERLAY', 'OVERWRITE', 'PARTITION', 'PARTITIONED', 'PARTITIONS', 'PERCENT', 'PIVOT', 'PLACING', 'POSITION', 'PRECEDING', 'PRIMARY', 'PRINCIPALS', 'PROPERTIES', 'PURGE', 'QUERY', 'RANGE', 'RECORDREADER', 'RECORDWRITER', 'RECOVER', 'REDUCE', 'REFERENCES', 'REFRESH', 'RENAME', 'REPAIR', 'REPLACE', 'RESET', 'RESPECT', 'RESTRICT', 'REVOKE', 'RIGHT', RLIKE, 'ROLE', 'ROLES', 'ROLLBACK', 'ROLLUP', 'ROW', 'ROWS', 'SECOND', 'SCHEMA', 'SELECT', 'SEMI', 'SEPARATED', 'SERDE', 'SERDEPROPERTIES', 'SESSION_USER', 'SET', 'MINUS', 'SETS', 'SHOW', 'SKEWED', 'SOME', 'SORT', 'SORTED', 'START', 'STATISTICS', 'STORED', 'STRATIFY', 'STRUCT', 'SUBSTR', 'SUBSTRING', 'SYNC', 'TABLE', 'TABLES', 'TABLESAMPLE', 'TBLPROPERTIES', TEMPORARY, 'TERMINATED', 'THEN', 'TIME', 'TO', 'TOUCH', 'TRAILING', 'TRANSACTION', 'TRANSACTIONS', 'TRANSFORM', 'TRIM', 'TRUE', 'TRUNCATE', 'TRY_CAST', 'TYPE', 'UNARCHIVE', 'UNBOUNDED', 'UNCACHE', 'UNION', 'UNIQUE', 'UNKNOWN', 'UNLOCK', 'UNSET', 'UPDATE', 'USE', 'USER', 'USING', 'VALUES', 'VIEW', 'VIEWS', 'WHEN', 'WHERE', 'WINDOW', 'WITH', 'YEAR', 'ZONE', '+', '-', '*', '~', STRING, BIGINT_LITERAL, SMALLINT_LITERAL, TINYINT_LITERAL, INTEGER_VALUE, EXPONENT_VALUE, DECIMAL_VALUE, FLOAT_LITERAL, DOUBLE_LITERAL, BIGDECIMAL_LITERAL, IDENTIFIER, BACKQUOTED_IDENTIFIER}(line 1, pos 28)",
            "log_location":"(write) log_w_sql_orc (line 4031)",
            "pass":true
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: mismatched input '\"' expecting {'(', 'ADD', 'AFTER', 'ALL', 'ALTER', 'ANALYZE', 'AND', 'ANTI', 'ANY', 'ARCHIVE', 'ARRAY', 'AS', 'ASC', 'AT', 'AUTHORIZATION', 'BETWEEN', 'BOTH', 'BUCKET', 'BUCKETS', 'BY', 'CACHE', 'CASCADE', 'CASE', 'CAST', 'CHANGE', 'CHECK', 'CLEAR', 'CLUSTER', 'CLUSTERED', 'CODEGEN', 'COLLATE', 'COLLECTION', 'COLUMN', 'COLUMNS', 'COMMENT', 'COMMIT', 'COMPACT', 'COMPACTIONS', 'COMPUTE', 'CONCATENATE', 'CONSTRAINT', 'COST', 'CREATE', 'CROSS', 'CUBE', 'CURRENT', 'CURRENT_DATE', 'CURRENT_TIME', 'CURRENT_TIMESTAMP', 'CURRENT_USER', 'DAY', 'DATA', 'DATABASE', DATABASES, 'DBPROPERTIES', 'DEFINED', 'DELETE', 'DELIMITED', 'DESC', 'DESCRIBE', 'DFS', 'DIRECTORIES', 'DIRECTORY', 'DISTINCT', 'DISTRIBUTE', 'DIV', 'DROP', 'ELSE', 'END', 'ESCAPE', 'ESCAPED', 'EXCEPT', 'EXCHANGE', 'EXISTS', 'EXPLAIN', 'EXPORT', 'EXTENDED', 'EXTERNAL', 'EXTRACT', 'FALSE', 'FETCH', 'FIELDS', 'FILTER', 'FILEFORMAT', 'FIRST', 'FOLLOWING', 'FOR', 'FOREIGN', 'FORMAT', 'FORMATTED', 'FROM', 'FULL', 'FUNCTION', 'FUNCTIONS', 'GLOBAL', 'GRANT', 'GROUP', 'GROUPING', 'HAVING', 'HOUR', 'IF', 'IGNORE', 'IMPORT', 'IN', 'INDEX', 'INDEXES', 'INNER', 'INPATH', 'INPUTFORMAT', 'INSERT', 'INTERSECT', 'INTERVAL', 'INTO', 'IS', 'ITEMS', 'JOIN', 'KEYS', 'LAST', 'LATERAL', 'LAZY', 'LEADING', 'LEFT', 'LIKE', 'LIMIT', 'LINES', 'LIST', 'LOAD', 'LOCAL', 'LOCATION', 'LOCK', 'LOCKS', 'LOGICAL', 'MACRO', 'MAP', 'MATCHED', 'MERGE', 'MINUTE', 'MONTH', 'MSCK', 'NAMESPACE', 'NAMESPACES', 'NATURAL', 'NO', NOT, 'NULL', 'NULLS', 'OF', 'ON', 'ONLY', 'OPTION', 'OPTIONS', 'OR', 'ORDER', 'OUT', 'OUTER', 'OUTPUTFORMAT', 'OVER', 'OVERLAPS', 'OVERLAY', 'OVERWRITE', 'PARTITION', 'PARTITIONED', 'PARTITIONS', 'PERCENT', 'PIVOT', 'PLACING', 'POSITION', 'PRECEDING', 'PRIMARY', 'PRINCIPALS', 'PROPERTIES', 'PURGE', 'QUERY', 'RANGE', 'RECORDREADER', 'RECORDWRITER', 'RECOVER', 'REDUCE', 'REFERENCES', 'REFRESH', 'RENAME', 'REPAIR', 'REPLACE', 'RESET', 'RESPECT', 'RESTRICT', 'REVOKE', 'RIGHT', RLIKE, 'ROLE', 'ROLES', 'ROLLBACK', 'ROLLUP', 'ROW', 'ROWS', 'SECOND', 'SCHEMA', 'SELECT', 'SEMI', 'SEPARATED', 'SERDE', 'SERDEPROPERTIES', 'SESSION_USER', 'SET', 'MINUS', 'SETS', 'SHOW', 'SKEWED', 'SOME', 'SORT', 'SORTED', 'START', 'STATISTICS', 'STORED', 'STRATIFY', 'STRUCT', 'SUBSTR', 'SUBSTRING', 'SYNC', 'TABLE', 'TABLES', 'TABLESAMPLE', 'TBLPROPERTIES', TEMPORARY, 'TERMINATED', 'THEN', 'TIME', 'TO', 'TOUCH', 'TRAILING', 'TRANSACTION', 'TRANSACTIONS', 'TRANSFORM', 'TRIM', 'TRUE', 'TRUNCATE', 'TRY_CAST', 'TYPE', 'UNARCHIVE', 'UNBOUNDED', 'UNCACHE', 'UNION', 'UNIQUE', 'UNKNOWN', 'UNLOCK', 'UNSET', 'UPDATE', 'USE', 'USER', 'USING', 'VALUES', 'VIEW', 'VIEWS', 'WHEN', 'WHERE', 'WINDOW', 'WITH', 'YEAR', 'ZONE', '+', '-', '*', '~', STRING, BIGINT_LITERAL, SMALLINT_LITERAL, TINYINT_LITERAL, INTEGER_VALUE, EXPONENT_VALUE, DECIMAL_VALUE, FLOAT_LITERAL, DOUBLE_LITERAL, BIGDECIMAL_LITERAL, IDENTIFIER, BACKQUOTED_IDENTIFIER}(line 1, pos 28)",
            "log_location":"(write) log_w_sql_parquet (line 4032)",
            "pass":true
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws83; line 1 pos 14;",
            "write_value":"No output, find exception: <console>:1: error: unclosed string literal",
            "log_location":"(read) log_w_df_r_df_avro (line 3150)",
            "pass":true
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: <console>:1: error: unclosed string literal",
            "log_location":"(write) log_w_df_orc (line 14078)",
            "pass":true
        }
    },
    "96":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: mismatched input '\"' expecting {'(', 'ADD', 'AFTER', 'ALL', 'ALTER', 'ANALYZE', 'AND', 'ANTI', 'ANY', 'ARCHIVE', 'ARRAY', 'AS', 'ASC', 'AT', 'AUTHORIZATION', 'BETWEEN', 'BOTH', 'BUCKET', 'BUCKETS', 'BY', 'CACHE', 'CASCADE', 'CASE', 'CAST', 'CHANGE', 'CHECK', 'CLEAR', 'CLUSTER', 'CLUSTERED', 'CODEGEN', 'COLLATE', 'COLLECTION', 'COLUMN', 'COLUMNS', 'COMMENT', 'COMMIT', 'COMPACT', 'COMPACTIONS', 'COMPUTE', 'CONCATENATE', 'CONSTRAINT', 'COST', 'CREATE', 'CROSS', 'CUBE', 'CURRENT', 'CURRENT_DATE', 'CURRENT_TIME', 'CURRENT_TIMESTAMP', 'CURRENT_USER', 'DAY', 'DATA', 'DATABASE', DATABASES, 'DBPROPERTIES', 'DEFINED', 'DELETE', 'DELIMITED', 'DESC', 'DESCRIBE', 'DFS', 'DIRECTORIES', 'DIRECTORY', 'DISTINCT', 'DISTRIBUTE', 'DIV', 'DROP', 'ELSE', 'END', 'ESCAPE', 'ESCAPED', 'EXCEPT', 'EXCHANGE', 'EXISTS', 'EXPLAIN', 'EXPORT', 'EXTENDED', 'EXTERNAL', 'EXTRACT', 'FALSE', 'FETCH', 'FIELDS', 'FILTER', 'FILEFORMAT', 'FIRST', 'FOLLOWING', 'FOR', 'FOREIGN', 'FORMAT', 'FORMATTED', 'FROM', 'FULL', 'FUNCTION', 'FUNCTIONS', 'GLOBAL', 'GRANT', 'GROUP', 'GROUPING', 'HAVING', 'HOUR', 'IF', 'IGNORE', 'IMPORT', 'IN', 'INDEX', 'INDEXES', 'INNER', 'INPATH', 'INPUTFORMAT', 'INSERT', 'INTERSECT', 'INTERVAL', 'INTO', 'IS', 'ITEMS', 'JOIN', 'KEYS', 'LAST', 'LATERAL', 'LAZY', 'LEADING', 'LEFT', 'LIKE', 'LIMIT', 'LINES', 'LIST', 'LOAD', 'LOCAL', 'LOCATION', 'LOCK', 'LOCKS', 'LOGICAL', 'MACRO', 'MAP', 'MATCHED', 'MERGE', 'MINUTE', 'MONTH', 'MSCK', 'NAMESPACE', 'NAMESPACES', 'NATURAL', 'NO', NOT, 'NULL', 'NULLS', 'OF', 'ON', 'ONLY', 'OPTION', 'OPTIONS', 'OR', 'ORDER', 'OUT', 'OUTER', 'OUTPUTFORMAT', 'OVER', 'OVERLAPS', 'OVERLAY', 'OVERWRITE', 'PARTITION', 'PARTITIONED', 'PARTITIONS', 'PERCENT', 'PIVOT', 'PLACING', 'POSITION', 'PRECEDING', 'PRIMARY', 'PRINCIPALS', 'PROPERTIES', 'PURGE', 'QUERY', 'RANGE', 'RECORDREADER', 'RECORDWRITER', 'RECOVER', 'REDUCE', 'REFERENCES', 'REFRESH', 'RENAME', 'REPAIR', 'REPLACE', 'RESET', 'RESPECT', 'RESTRICT', 'REVOKE', 'RIGHT', RLIKE, 'ROLE', 'ROLES', 'ROLLBACK', 'ROLLUP', 'ROW', 'ROWS', 'SECOND', 'SCHEMA', 'SELECT', 'SEMI', 'SEPARATED', 'SERDE', 'SERDEPROPERTIES', 'SESSION_USER', 'SET', 'MINUS', 'SETS', 'SHOW', 'SKEWED', 'SOME', 'SORT', 'SORTED', 'START', 'STATISTICS', 'STORED', 'STRATIFY', 'STRUCT', 'SUBSTR', 'SUBSTRING', 'SYNC', 'TABLE', 'TABLES', 'TABLESAMPLE', 'TBLPROPERTIES', TEMPORARY, 'TERMINATED', 'THEN', 'TIME', 'TO', 'TOUCH', 'TRAILING', 'TRANSACTION', 'TRANSACTIONS', 'TRANSFORM', 'TRIM', 'TRUE', 'TRUNCATE', 'TRY_CAST', 'TYPE', 'UNARCHIVE', 'UNBOUNDED', 'UNCACHE', 'UNION', 'UNIQUE', 'UNKNOWN', 'UNLOCK', 'UNSET', 'UPDATE', 'USE', 'USER', 'USING', 'VALUES', 'VIEW', 'VIEWS', 'WHEN', 'WHERE', 'WINDOW', 'WITH', 'YEAR', 'ZONE', '+', '-', '*', '~', STRING, BIGINT_LITERAL, SMALLINT_LITERAL, TINYINT_LITERAL, INTEGER_VALUE, EXPONENT_VALUE, DECIMAL_VALUE, FLOAT_LITERAL, DOUBLE_LITERAL, BIGDECIMAL_LITERAL, IDENTIFIER, BACKQUOTED_IDENTIFIER}(line 1, pos 28)",
            "log_location":"(write) log_w_sql_avro (line 3222)",
            "pass":true
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws96; line 1 pos 14;",
            "write_value":"No output, find exception: <console>:1: error: unclosed string literal",
            "log_location":"(read) log_w_df_r_df_orc (line 1980)",
            "pass":true
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: mismatched input '\"' expecting {'(', 'ADD', 'AFTER', 'ALL', 'ALTER', 'ANALYZE', 'AND', 'ANTI', 'ANY', 'ARCHIVE', 'ARRAY', 'AS', 'ASC', 'AT', 'AUTHORIZATION', 'BETWEEN', 'BOTH', 'BUCKET', 'BUCKETS', 'BY', 'CACHE', 'CASCADE', 'CASE', 'CAST', 'CHANGE', 'CHECK', 'CLEAR', 'CLUSTER', 'CLUSTERED', 'CODEGEN', 'COLLATE', 'COLLECTION', 'COLUMN', 'COLUMNS', 'COMMENT', 'COMMIT', 'COMPACT', 'COMPACTIONS', 'COMPUTE', 'CONCATENATE', 'CONSTRAINT', 'COST', 'CREATE', 'CROSS', 'CUBE', 'CURRENT', 'CURRENT_DATE', 'CURRENT_TIME', 'CURRENT_TIMESTAMP', 'CURRENT_USER', 'DAY', 'DATA', 'DATABASE', DATABASES, 'DBPROPERTIES', 'DEFINED', 'DELETE', 'DELIMITED', 'DESC', 'DESCRIBE', 'DFS', 'DIRECTORIES', 'DIRECTORY', 'DISTINCT', 'DISTRIBUTE', 'DIV', 'DROP', 'ELSE', 'END', 'ESCAPE', 'ESCAPED', 'EXCEPT', 'EXCHANGE', 'EXISTS', 'EXPLAIN', 'EXPORT', 'EXTENDED', 'EXTERNAL', 'EXTRACT', 'FALSE', 'FETCH', 'FIELDS', 'FILTER', 'FILEFORMAT', 'FIRST', 'FOLLOWING', 'FOR', 'FOREIGN', 'FORMAT', 'FORMATTED', 'FROM', 'FULL', 'FUNCTION', 'FUNCTIONS', 'GLOBAL', 'GRANT', 'GROUP', 'GROUPING', 'HAVING', 'HOUR', 'IF', 'IGNORE', 'IMPORT', 'IN', 'INDEX', 'INDEXES', 'INNER', 'INPATH', 'INPUTFORMAT', 'INSERT', 'INTERSECT', 'INTERVAL', 'INTO', 'IS', 'ITEMS', 'JOIN', 'KEYS', 'LAST', 'LATERAL', 'LAZY', 'LEADING', 'LEFT', 'LIKE', 'LIMIT', 'LINES', 'LIST', 'LOAD', 'LOCAL', 'LOCATION', 'LOCK', 'LOCKS', 'LOGICAL', 'MACRO', 'MAP', 'MATCHED', 'MERGE', 'MINUTE', 'MONTH', 'MSCK', 'NAMESPACE', 'NAMESPACES', 'NATURAL', 'NO', NOT, 'NULL', 'NULLS', 'OF', 'ON', 'ONLY', 'OPTION', 'OPTIONS', 'OR', 'ORDER', 'OUT', 'OUTER', 'OUTPUTFORMAT', 'OVER', 'OVERLAPS', 'OVERLAY', 'OVERWRITE', 'PARTITION', 'PARTITIONED', 'PARTITIONS', 'PERCENT', 'PIVOT', 'PLACING', 'POSITION', 'PRECEDING', 'PRIMARY', 'PRINCIPALS', 'PROPERTIES', 'PURGE', 'QUERY', 'RANGE', 'RECORDREADER', 'RECORDWRITER', 'RECOVER', 'REDUCE', 'REFERENCES', 'REFRESH', 'RENAME', 'REPAIR', 'REPLACE', 'RESET', 'RESPECT', 'RESTRICT', 'REVOKE', 'RIGHT', RLIKE, 'ROLE', 'ROLES', 'ROLLBACK', 'ROLLUP', 'ROW', 'ROWS', 'SECOND', 'SCHEMA', 'SELECT', 'SEMI', 'SEPARATED', 'SERDE', 'SERDEPROPERTIES', 'SESSION_USER', 'SET', 'MINUS', 'SETS', 'SHOW', 'SKEWED', 'SOME', 'SORT', 'SORTED', 'START', 'STATISTICS', 'STORED', 'STRATIFY', 'STRUCT', 'SUBSTR', 'SUBSTRING', 'SYNC', 'TABLE', 'TABLES', 'TABLESAMPLE', 'TBLPROPERTIES', TEMPORARY, 'TERMINATED', 'THEN', 'TIME', 'TO', 'TOUCH', 'TRAILING', 'TRANSACTION', 'TRANSACTIONS', 'TRANSFORM', 'TRIM', 'TRUE', 'TRUNCATE', 'TRY_CAST', 'TYPE', 'UNARCHIVE', 'UNBOUNDED', 'UNCACHE', 'UNION', 'UNIQUE', 'UNKNOWN', 'UNLOCK', 'UNSET', 'UPDATE', 'USE', 'USER', 'USING', 'VALUES', 'VIEW', 'VIEWS', 'WHEN', 'WHERE', 'WINDOW', 'WITH', 'YEAR', 'ZONE', '+', '-', '*', '~', STRING, BIGINT_LITERAL, SMALLINT_LITERAL, TINYINT_LITERAL, INTEGER_VALUE, EXPONENT_VALUE, DECIMAL_VALUE, FLOAT_LITERAL, DOUBLE_LITERAL, BIGDECIMAL_LITERAL, IDENTIFIER, BACKQUOTED_IDENTIFIER}(line 1, pos 28)",
            "log_location":"(write) log_w_sql_orc (line 4146)",
            "pass":true
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: <console>:1: error: unclosed string literal",
            "log_location":"(write) log_w_df_avro (line 6809)",
            "pass":true
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: mismatched input '\"' expecting {'(', 'ADD', 'AFTER', 'ALL', 'ALTER', 'ANALYZE', 'AND', 'ANTI', 'ANY', 'ARCHIVE', 'ARRAY', 'AS', 'ASC', 'AT', 'AUTHORIZATION', 'BETWEEN', 'BOTH', 'BUCKET', 'BUCKETS', 'BY', 'CACHE', 'CASCADE', 'CASE', 'CAST', 'CHANGE', 'CHECK', 'CLEAR', 'CLUSTER', 'CLUSTERED', 'CODEGEN', 'COLLATE', 'COLLECTION', 'COLUMN', 'COLUMNS', 'COMMENT', 'COMMIT', 'COMPACT', 'COMPACTIONS', 'COMPUTE', 'CONCATENATE', 'CONSTRAINT', 'COST', 'CREATE', 'CROSS', 'CUBE', 'CURRENT', 'CURRENT_DATE', 'CURRENT_TIME', 'CURRENT_TIMESTAMP', 'CURRENT_USER', 'DAY', 'DATA', 'DATABASE', DATABASES, 'DBPROPERTIES', 'DEFINED', 'DELETE', 'DELIMITED', 'DESC', 'DESCRIBE', 'DFS', 'DIRECTORIES', 'DIRECTORY', 'DISTINCT', 'DISTRIBUTE', 'DIV', 'DROP', 'ELSE', 'END', 'ESCAPE', 'ESCAPED', 'EXCEPT', 'EXCHANGE', 'EXISTS', 'EXPLAIN', 'EXPORT', 'EXTENDED', 'EXTERNAL', 'EXTRACT', 'FALSE', 'FETCH', 'FIELDS', 'FILTER', 'FILEFORMAT', 'FIRST', 'FOLLOWING', 'FOR', 'FOREIGN', 'FORMAT', 'FORMATTED', 'FROM', 'FULL', 'FUNCTION', 'FUNCTIONS', 'GLOBAL', 'GRANT', 'GROUP', 'GROUPING', 'HAVING', 'HOUR', 'IF', 'IGNORE', 'IMPORT', 'IN', 'INDEX', 'INDEXES', 'INNER', 'INPATH', 'INPUTFORMAT', 'INSERT', 'INTERSECT', 'INTERVAL', 'INTO', 'IS', 'ITEMS', 'JOIN', 'KEYS', 'LAST', 'LATERAL', 'LAZY', 'LEADING', 'LEFT', 'LIKE', 'LIMIT', 'LINES', 'LIST', 'LOAD', 'LOCAL', 'LOCATION', 'LOCK', 'LOCKS', 'LOGICAL', 'MACRO', 'MAP', 'MATCHED', 'MERGE', 'MINUTE', 'MONTH', 'MSCK', 'NAMESPACE', 'NAMESPACES', 'NATURAL', 'NO', NOT, 'NULL', 'NULLS', 'OF', 'ON', 'ONLY', 'OPTION', 'OPTIONS', 'OR', 'ORDER', 'OUT', 'OUTER', 'OUTPUTFORMAT', 'OVER', 'OVERLAPS', 'OVERLAY', 'OVERWRITE', 'PARTITION', 'PARTITIONED', 'PARTITIONS', 'PERCENT', 'PIVOT', 'PLACING', 'POSITION', 'PRECEDING', 'PRIMARY', 'PRINCIPALS', 'PROPERTIES', 'PURGE', 'QUERY', 'RANGE', 'RECORDREADER', 'RECORDWRITER', 'RECOVER', 'REDUCE', 'REFERENCES', 'REFRESH', 'RENAME', 'REPAIR', 'REPLACE', 'RESET', 'RESPECT', 'RESTRICT', 'REVOKE', 'RIGHT', RLIKE, 'ROLE', 'ROLES', 'ROLLBACK', 'ROLLUP', 'ROW', 'ROWS', 'SECOND', 'SCHEMA', 'SELECT', 'SEMI', 'SEPARATED', 'SERDE', 'SERDEPROPERTIES', 'SESSION_USER', 'SET', 'MINUS', 'SETS', 'SHOW', 'SKEWED', 'SOME', 'SORT', 'SORTED', 'START', 'STATISTICS', 'STORED', 'STRATIFY', 'STRUCT', 'SUBSTR', 'SUBSTRING', 'SYNC', 'TABLE', 'TABLES', 'TABLESAMPLE', 'TBLPROPERTIES', TEMPORARY, 'TERMINATED', 'THEN', 'TIME', 'TO', 'TOUCH', 'TRAILING', 'TRANSACTION', 'TRANSACTIONS', 'TRANSFORM', 'TRIM', 'TRUE', 'TRUNCATE', 'TRY_CAST', 'TYPE', 'UNARCHIVE', 'UNBOUNDED', 'UNCACHE', 'UNION', 'UNIQUE', 'UNKNOWN', 'UNLOCK', 'UNSET', 'UPDATE', 'USE', 'USER', 'USING', 'VALUES', 'VIEW', 'VIEWS', 'WHEN', 'WHERE', 'WINDOW', 'WITH', 'YEAR', 'ZONE', '+', '-', '*', '~', STRING, BIGINT_LITERAL, SMALLINT_LITERAL, TINYINT_LITERAL, INTEGER_VALUE, EXPONENT_VALUE, DECIMAL_VALUE, FLOAT_LITERAL, DOUBLE_LITERAL, BIGDECIMAL_LITERAL, IDENTIFIER, BACKQUOTED_IDENTIFIER}(line 1, pos 28)",
            "log_location":"(write) log_w_sql_avro (line 3222)",
            "pass":true
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: <console>:1: error: unclosed string literal",
            "log_location":"(write) log_w_df_parquet (line 14404)",
            "pass":true
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: mismatched input '\"' expecting {'(', 'ADD', 'AFTER', 'ALL', 'ALTER', 'ANALYZE', 'AND', 'ANTI', 'ANY', 'ARCHIVE', 'ARRAY', 'AS', 'ASC', 'AT', 'AUTHORIZATION', 'BETWEEN', 'BOTH', 'BUCKET', 'BUCKETS', 'BY', 'CACHE', 'CASCADE', 'CASE', 'CAST', 'CHANGE', 'CHECK', 'CLEAR', 'CLUSTER', 'CLUSTERED', 'CODEGEN', 'COLLATE', 'COLLECTION', 'COLUMN', 'COLUMNS', 'COMMENT', 'COMMIT', 'COMPACT', 'COMPACTIONS', 'COMPUTE', 'CONCATENATE', 'CONSTRAINT', 'COST', 'CREATE', 'CROSS', 'CUBE', 'CURRENT', 'CURRENT_DATE', 'CURRENT_TIME', 'CURRENT_TIMESTAMP', 'CURRENT_USER', 'DAY', 'DATA', 'DATABASE', DATABASES, 'DBPROPERTIES', 'DEFINED', 'DELETE', 'DELIMITED', 'DESC', 'DESCRIBE', 'DFS', 'DIRECTORIES', 'DIRECTORY', 'DISTINCT', 'DISTRIBUTE', 'DIV', 'DROP', 'ELSE', 'END', 'ESCAPE', 'ESCAPED', 'EXCEPT', 'EXCHANGE', 'EXISTS', 'EXPLAIN', 'EXPORT', 'EXTENDED', 'EXTERNAL', 'EXTRACT', 'FALSE', 'FETCH', 'FIELDS', 'FILTER', 'FILEFORMAT', 'FIRST', 'FOLLOWING', 'FOR', 'FOREIGN', 'FORMAT', 'FORMATTED', 'FROM', 'FULL', 'FUNCTION', 'FUNCTIONS', 'GLOBAL', 'GRANT', 'GROUP', 'GROUPING', 'HAVING', 'HOUR', 'IF', 'IGNORE', 'IMPORT', 'IN', 'INDEX', 'INDEXES', 'INNER', 'INPATH', 'INPUTFORMAT', 'INSERT', 'INTERSECT', 'INTERVAL', 'INTO', 'IS', 'ITEMS', 'JOIN', 'KEYS', 'LAST', 'LATERAL', 'LAZY', 'LEADING', 'LEFT', 'LIKE', 'LIMIT', 'LINES', 'LIST', 'LOAD', 'LOCAL', 'LOCATION', 'LOCK', 'LOCKS', 'LOGICAL', 'MACRO', 'MAP', 'MATCHED', 'MERGE', 'MINUTE', 'MONTH', 'MSCK', 'NAMESPACE', 'NAMESPACES', 'NATURAL', 'NO', NOT, 'NULL', 'NULLS', 'OF', 'ON', 'ONLY', 'OPTION', 'OPTIONS', 'OR', 'ORDER', 'OUT', 'OUTER', 'OUTPUTFORMAT', 'OVER', 'OVERLAPS', 'OVERLAY', 'OVERWRITE', 'PARTITION', 'PARTITIONED', 'PARTITIONS', 'PERCENT', 'PIVOT', 'PLACING', 'POSITION', 'PRECEDING', 'PRIMARY', 'PRINCIPALS', 'PROPERTIES', 'PURGE', 'QUERY', 'RANGE', 'RECORDREADER', 'RECORDWRITER', 'RECOVER', 'REDUCE', 'REFERENCES', 'REFRESH', 'RENAME', 'REPAIR', 'REPLACE', 'RESET', 'RESPECT', 'RESTRICT', 'REVOKE', 'RIGHT', RLIKE, 'ROLE', 'ROLES', 'ROLLBACK', 'ROLLUP', 'ROW', 'ROWS', 'SECOND', 'SCHEMA', 'SELECT', 'SEMI', 'SEPARATED', 'SERDE', 'SERDEPROPERTIES', 'SESSION_USER', 'SET', 'MINUS', 'SETS', 'SHOW', 'SKEWED', 'SOME', 'SORT', 'SORTED', 'START', 'STATISTICS', 'STORED', 'STRATIFY', 'STRUCT', 'SUBSTR', 'SUBSTRING', 'SYNC', 'TABLE', 'TABLES', 'TABLESAMPLE', 'TBLPROPERTIES', TEMPORARY, 'TERMINATED', 'THEN', 'TIME', 'TO', 'TOUCH', 'TRAILING', 'TRANSACTION', 'TRANSACTIONS', 'TRANSFORM', 'TRIM', 'TRUE', 'TRUNCATE', 'TRY_CAST', 'TYPE', 'UNARCHIVE', 'UNBOUNDED', 'UNCACHE', 'UNION', 'UNIQUE', 'UNKNOWN', 'UNLOCK', 'UNSET', 'UPDATE', 'USE', 'USER', 'USING', 'VALUES', 'VIEW', 'VIEWS', 'WHEN', 'WHERE', 'WINDOW', 'WITH', 'YEAR', 'ZONE', '+', '-', '*', '~', STRING, BIGINT_LITERAL, SMALLINT_LITERAL, TINYINT_LITERAL, INTEGER_VALUE, EXPONENT_VALUE, DECIMAL_VALUE, FLOAT_LITERAL, DOUBLE_LITERAL, BIGDECIMAL_LITERAL, IDENTIFIER, BACKQUOTED_IDENTIFIER}(line 1, pos 28)",
            "log_location":"(write) log_w_sql_parquet (line 4147)",
            "pass":true
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws96; line 1 pos 14;",
            "write_value":"No output, find exception: <console>:1: error: unclosed string literal",
            "log_location":"(read) log_w_df_r_df_parquet (line 1982)",
            "pass":true
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: mismatched input '\"' expecting {'(', 'ADD', 'AFTER', 'ALL', 'ALTER', 'ANALYZE', 'AND', 'ANTI', 'ANY', 'ARCHIVE', 'ARRAY', 'AS', 'ASC', 'AT', 'AUTHORIZATION', 'BETWEEN', 'BOTH', 'BUCKET', 'BUCKETS', 'BY', 'CACHE', 'CASCADE', 'CASE', 'CAST', 'CHANGE', 'CHECK', 'CLEAR', 'CLUSTER', 'CLUSTERED', 'CODEGEN', 'COLLATE', 'COLLECTION', 'COLUMN', 'COLUMNS', 'COMMENT', 'COMMIT', 'COMPACT', 'COMPACTIONS', 'COMPUTE', 'CONCATENATE', 'CONSTRAINT', 'COST', 'CREATE', 'CROSS', 'CUBE', 'CURRENT', 'CURRENT_DATE', 'CURRENT_TIME', 'CURRENT_TIMESTAMP', 'CURRENT_USER', 'DAY', 'DATA', 'DATABASE', DATABASES, 'DBPROPERTIES', 'DEFINED', 'DELETE', 'DELIMITED', 'DESC', 'DESCRIBE', 'DFS', 'DIRECTORIES', 'DIRECTORY', 'DISTINCT', 'DISTRIBUTE', 'DIV', 'DROP', 'ELSE', 'END', 'ESCAPE', 'ESCAPED', 'EXCEPT', 'EXCHANGE', 'EXISTS', 'EXPLAIN', 'EXPORT', 'EXTENDED', 'EXTERNAL', 'EXTRACT', 'FALSE', 'FETCH', 'FIELDS', 'FILTER', 'FILEFORMAT', 'FIRST', 'FOLLOWING', 'FOR', 'FOREIGN', 'FORMAT', 'FORMATTED', 'FROM', 'FULL', 'FUNCTION', 'FUNCTIONS', 'GLOBAL', 'GRANT', 'GROUP', 'GROUPING', 'HAVING', 'HOUR', 'IF', 'IGNORE', 'IMPORT', 'IN', 'INDEX', 'INDEXES', 'INNER', 'INPATH', 'INPUTFORMAT', 'INSERT', 'INTERSECT', 'INTERVAL', 'INTO', 'IS', 'ITEMS', 'JOIN', 'KEYS', 'LAST', 'LATERAL', 'LAZY', 'LEADING', 'LEFT', 'LIKE', 'LIMIT', 'LINES', 'LIST', 'LOAD', 'LOCAL', 'LOCATION', 'LOCK', 'LOCKS', 'LOGICAL', 'MACRO', 'MAP', 'MATCHED', 'MERGE', 'MINUTE', 'MONTH', 'MSCK', 'NAMESPACE', 'NAMESPACES', 'NATURAL', 'NO', NOT, 'NULL', 'NULLS', 'OF', 'ON', 'ONLY', 'OPTION', 'OPTIONS', 'OR', 'ORDER', 'OUT', 'OUTER', 'OUTPUTFORMAT', 'OVER', 'OVERLAPS', 'OVERLAY', 'OVERWRITE', 'PARTITION', 'PARTITIONED', 'PARTITIONS', 'PERCENT', 'PIVOT', 'PLACING', 'POSITION', 'PRECEDING', 'PRIMARY', 'PRINCIPALS', 'PROPERTIES', 'PURGE', 'QUERY', 'RANGE', 'RECORDREADER', 'RECORDWRITER', 'RECOVER', 'REDUCE', 'REFERENCES', 'REFRESH', 'RENAME', 'REPAIR', 'REPLACE', 'RESET', 'RESPECT', 'RESTRICT', 'REVOKE', 'RIGHT', RLIKE, 'ROLE', 'ROLES', 'ROLLBACK', 'ROLLUP', 'ROW', 'ROWS', 'SECOND', 'SCHEMA', 'SELECT', 'SEMI', 'SEPARATED', 'SERDE', 'SERDEPROPERTIES', 'SESSION_USER', 'SET', 'MINUS', 'SETS', 'SHOW', 'SKEWED', 'SOME', 'SORT', 'SORTED', 'START', 'STATISTICS', 'STORED', 'STRATIFY', 'STRUCT', 'SUBSTR', 'SUBSTRING', 'SYNC', 'TABLE', 'TABLES', 'TABLESAMPLE', 'TBLPROPERTIES', TEMPORARY, 'TERMINATED', 'THEN', 'TIME', 'TO', 'TOUCH', 'TRAILING', 'TRANSACTION', 'TRANSACTIONS', 'TRANSFORM', 'TRIM', 'TRUE', 'TRUNCATE', 'TRY_CAST', 'TYPE', 'UNARCHIVE', 'UNBOUNDED', 'UNCACHE', 'UNION', 'UNIQUE', 'UNKNOWN', 'UNLOCK', 'UNSET', 'UPDATE', 'USE', 'USER', 'USING', 'VALUES', 'VIEW', 'VIEWS', 'WHEN', 'WHERE', 'WINDOW', 'WITH', 'YEAR', 'ZONE', '+', '-', '*', '~', STRING, BIGINT_LITERAL, SMALLINT_LITERAL, TINYINT_LITERAL, INTEGER_VALUE, EXPONENT_VALUE, DECIMAL_VALUE, FLOAT_LITERAL, DOUBLE_LITERAL, BIGDECIMAL_LITERAL, IDENTIFIER, BACKQUOTED_IDENTIFIER}(line 1, pos 28)",
            "log_location":"(write) log_w_sql_orc (line 4146)",
            "pass":true
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: mismatched input '\"' expecting {'(', 'ADD', 'AFTER', 'ALL', 'ALTER', 'ANALYZE', 'AND', 'ANTI', 'ANY', 'ARCHIVE', 'ARRAY', 'AS', 'ASC', 'AT', 'AUTHORIZATION', 'BETWEEN', 'BOTH', 'BUCKET', 'BUCKETS', 'BY', 'CACHE', 'CASCADE', 'CASE', 'CAST', 'CHANGE', 'CHECK', 'CLEAR', 'CLUSTER', 'CLUSTERED', 'CODEGEN', 'COLLATE', 'COLLECTION', 'COLUMN', 'COLUMNS', 'COMMENT', 'COMMIT', 'COMPACT', 'COMPACTIONS', 'COMPUTE', 'CONCATENATE', 'CONSTRAINT', 'COST', 'CREATE', 'CROSS', 'CUBE', 'CURRENT', 'CURRENT_DATE', 'CURRENT_TIME', 'CURRENT_TIMESTAMP', 'CURRENT_USER', 'DAY', 'DATA', 'DATABASE', DATABASES, 'DBPROPERTIES', 'DEFINED', 'DELETE', 'DELIMITED', 'DESC', 'DESCRIBE', 'DFS', 'DIRECTORIES', 'DIRECTORY', 'DISTINCT', 'DISTRIBUTE', 'DIV', 'DROP', 'ELSE', 'END', 'ESCAPE', 'ESCAPED', 'EXCEPT', 'EXCHANGE', 'EXISTS', 'EXPLAIN', 'EXPORT', 'EXTENDED', 'EXTERNAL', 'EXTRACT', 'FALSE', 'FETCH', 'FIELDS', 'FILTER', 'FILEFORMAT', 'FIRST', 'FOLLOWING', 'FOR', 'FOREIGN', 'FORMAT', 'FORMATTED', 'FROM', 'FULL', 'FUNCTION', 'FUNCTIONS', 'GLOBAL', 'GRANT', 'GROUP', 'GROUPING', 'HAVING', 'HOUR', 'IF', 'IGNORE', 'IMPORT', 'IN', 'INDEX', 'INDEXES', 'INNER', 'INPATH', 'INPUTFORMAT', 'INSERT', 'INTERSECT', 'INTERVAL', 'INTO', 'IS', 'ITEMS', 'JOIN', 'KEYS', 'LAST', 'LATERAL', 'LAZY', 'LEADING', 'LEFT', 'LIKE', 'LIMIT', 'LINES', 'LIST', 'LOAD', 'LOCAL', 'LOCATION', 'LOCK', 'LOCKS', 'LOGICAL', 'MACRO', 'MAP', 'MATCHED', 'MERGE', 'MINUTE', 'MONTH', 'MSCK', 'NAMESPACE', 'NAMESPACES', 'NATURAL', 'NO', NOT, 'NULL', 'NULLS', 'OF', 'ON', 'ONLY', 'OPTION', 'OPTIONS', 'OR', 'ORDER', 'OUT', 'OUTER', 'OUTPUTFORMAT', 'OVER', 'OVERLAPS', 'OVERLAY', 'OVERWRITE', 'PARTITION', 'PARTITIONED', 'PARTITIONS', 'PERCENT', 'PIVOT', 'PLACING', 'POSITION', 'PRECEDING', 'PRIMARY', 'PRINCIPALS', 'PROPERTIES', 'PURGE', 'QUERY', 'RANGE', 'RECORDREADER', 'RECORDWRITER', 'RECOVER', 'REDUCE', 'REFERENCES', 'REFRESH', 'RENAME', 'REPAIR', 'REPLACE', 'RESET', 'RESPECT', 'RESTRICT', 'REVOKE', 'RIGHT', RLIKE, 'ROLE', 'ROLES', 'ROLLBACK', 'ROLLUP', 'ROW', 'ROWS', 'SECOND', 'SCHEMA', 'SELECT', 'SEMI', 'SEPARATED', 'SERDE', 'SERDEPROPERTIES', 'SESSION_USER', 'SET', 'MINUS', 'SETS', 'SHOW', 'SKEWED', 'SOME', 'SORT', 'SORTED', 'START', 'STATISTICS', 'STORED', 'STRATIFY', 'STRUCT', 'SUBSTR', 'SUBSTRING', 'SYNC', 'TABLE', 'TABLES', 'TABLESAMPLE', 'TBLPROPERTIES', TEMPORARY, 'TERMINATED', 'THEN', 'TIME', 'TO', 'TOUCH', 'TRAILING', 'TRANSACTION', 'TRANSACTIONS', 'TRANSFORM', 'TRIM', 'TRUE', 'TRUNCATE', 'TRY_CAST', 'TYPE', 'UNARCHIVE', 'UNBOUNDED', 'UNCACHE', 'UNION', 'UNIQUE', 'UNKNOWN', 'UNLOCK', 'UNSET', 'UPDATE', 'USE', 'USER', 'USING', 'VALUES', 'VIEW', 'VIEWS', 'WHEN', 'WHERE', 'WINDOW', 'WITH', 'YEAR', 'ZONE', '+', '-', '*', '~', STRING, BIGINT_LITERAL, SMALLINT_LITERAL, TINYINT_LITERAL, INTEGER_VALUE, EXPONENT_VALUE, DECIMAL_VALUE, FLOAT_LITERAL, DOUBLE_LITERAL, BIGDECIMAL_LITERAL, IDENTIFIER, BACKQUOTED_IDENTIFIER}(line 1, pos 28)",
            "log_location":"(write) log_w_sql_parquet (line 4147)",
            "pass":true
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws96; line 1 pos 14;",
            "write_value":"No output, find exception: <console>:1: error: unclosed string literal",
            "log_location":"(read) log_w_df_r_df_avro (line 3670)",
            "pass":true
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: <console>:1: error: unclosed string literal",
            "log_location":"(write) log_w_df_orc (line 14401)",
            "pass":true
        }
    },
    "97":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"bbbbbbbbbbb, find exception: java.lang.RuntimeException: Exceeds char/varchar type length limitation: 10",
            "log_location":"(write) log_w_sql_avro (line 3239)",
            "pass":true
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws97; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.AnalysisException: char/varchar type can only be used in the table schema. You can set spark.sql.legacy.charVarcharAsString to true, so that Spark treat them as string type as same as Spark 3.0 and earlier",
            "log_location":"(read) log_w_df_r_df_orc (line 2020)",
            "pass":true
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"bbbbbbbbbbb, find exception: java.lang.RuntimeException: Exceeds char/varchar type length limitation: 10",
            "log_location":"(write) log_w_sql_orc (line 4162)",
            "pass":true
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.AnalysisException: char/varchar type can only be used in the table schema. You can set spark.sql.legacy.charVarcharAsString to true, so that Spark treat them as string type as same as Spark 3.0 and earlier",
            "log_location":"(write) log_w_df_avro (line 6838)",
            "pass":true
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"bbbbbbbbbbb, find exception: java.lang.RuntimeException: Exceeds char/varchar type length limitation: 10",
            "log_location":"(write) log_w_sql_avro (line 3239)",
            "pass":true
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.AnalysisException: char/varchar type can only be used in the table schema. You can set spark.sql.legacy.charVarcharAsString to true, so that Spark treat them as string type as same as Spark 3.0 and earlier",
            "log_location":"(write) log_w_df_parquet (line 14433)",
            "pass":true
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"bbbbbbbbbbb, find exception: java.lang.RuntimeException: Exceeds char/varchar type length limitation: 10",
            "log_location":"(write) log_w_sql_parquet (line 4163)",
            "pass":true
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws97; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.AnalysisException: char/varchar type can only be used in the table schema. You can set spark.sql.legacy.charVarcharAsString to true, so that Spark treat them as string type as same as Spark 3.0 and earlier",
            "log_location":"(read) log_w_df_r_df_parquet (line 2022)",
            "pass":true
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"bbbbbbbbbbb, find exception: java.lang.RuntimeException: Exceeds char/varchar type length limitation: 10",
            "log_location":"(write) log_w_sql_orc (line 4162)",
            "pass":true
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"bbbbbbbbbbb, find exception: java.lang.RuntimeException: Exceeds char/varchar type length limitation: 10",
            "log_location":"(write) log_w_sql_parquet (line 4163)",
            "pass":true
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws97; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.AnalysisException: char/varchar type can only be used in the table schema. You can set spark.sql.legacy.charVarcharAsString to true, so that Spark treat them as string type as same as Spark 3.0 and earlier",
            "log_location":"(read) log_w_df_r_df_avro (line 3710)",
            "pass":true
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.AnalysisException: char/varchar type can only be used in the table schema. You can set spark.sql.legacy.charVarcharAsString to true, so that Spark treat them as string type as same as Spark 3.0 and earlier",
            "log_location":"(write) log_w_df_orc (line 14430)",
            "pass":true
        }
    },
    "110":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: mismatched input '\"' expecting {'(', 'ADD', 'AFTER', 'ALL', 'ALTER', 'ANALYZE', 'AND', 'ANTI', 'ANY', 'ARCHIVE', 'ARRAY', 'AS', 'ASC', 'AT', 'AUTHORIZATION', 'BETWEEN', 'BOTH', 'BUCKET', 'BUCKETS', 'BY', 'CACHE', 'CASCADE', 'CASE', 'CAST', 'CHANGE', 'CHECK', 'CLEAR', 'CLUSTER', 'CLUSTERED', 'CODEGEN', 'COLLATE', 'COLLECTION', 'COLUMN', 'COLUMNS', 'COMMENT', 'COMMIT', 'COMPACT', 'COMPACTIONS', 'COMPUTE', 'CONCATENATE', 'CONSTRAINT', 'COST', 'CREATE', 'CROSS', 'CUBE', 'CURRENT', 'CURRENT_DATE', 'CURRENT_TIME', 'CURRENT_TIMESTAMP', 'CURRENT_USER', 'DAY', 'DATA', 'DATABASE', DATABASES, 'DBPROPERTIES', 'DEFINED', 'DELETE', 'DELIMITED', 'DESC', 'DESCRIBE', 'DFS', 'DIRECTORIES', 'DIRECTORY', 'DISTINCT', 'DISTRIBUTE', 'DIV', 'DROP', 'ELSE', 'END', 'ESCAPE', 'ESCAPED', 'EXCEPT', 'EXCHANGE', 'EXISTS', 'EXPLAIN', 'EXPORT', 'EXTENDED', 'EXTERNAL', 'EXTRACT', 'FALSE', 'FETCH', 'FIELDS', 'FILTER', 'FILEFORMAT', 'FIRST', 'FOLLOWING', 'FOR', 'FOREIGN', 'FORMAT', 'FORMATTED', 'FROM', 'FULL', 'FUNCTION', 'FUNCTIONS', 'GLOBAL', 'GRANT', 'GROUP', 'GROUPING', 'HAVING', 'HOUR', 'IF', 'IGNORE', 'IMPORT', 'IN', 'INDEX', 'INDEXES', 'INNER', 'INPATH', 'INPUTFORMAT', 'INSERT', 'INTERSECT', 'INTERVAL', 'INTO', 'IS', 'ITEMS', 'JOIN', 'KEYS', 'LAST', 'LATERAL', 'LAZY', 'LEADING', 'LEFT', 'LIKE', 'LIMIT', 'LINES', 'LIST', 'LOAD', 'LOCAL', 'LOCATION', 'LOCK', 'LOCKS', 'LOGICAL', 'MACRO', 'MAP', 'MATCHED', 'MERGE', 'MINUTE', 'MONTH', 'MSCK', 'NAMESPACE', 'NAMESPACES', 'NATURAL', 'NO', NOT, 'NULL', 'NULLS', 'OF', 'ON', 'ONLY', 'OPTION', 'OPTIONS', 'OR', 'ORDER', 'OUT', 'OUTER', 'OUTPUTFORMAT', 'OVER', 'OVERLAPS', 'OVERLAY', 'OVERWRITE', 'PARTITION', 'PARTITIONED', 'PARTITIONS', 'PERCENT', 'PIVOT', 'PLACING', 'POSITION', 'PRECEDING', 'PRIMARY', 'PRINCIPALS', 'PROPERTIES', 'PURGE', 'QUERY', 'RANGE', 'RECORDREADER', 'RECORDWRITER', 'RECOVER', 'REDUCE', 'REFERENCES', 'REFRESH', 'RENAME', 'REPAIR', 'REPLACE', 'RESET', 'RESPECT', 'RESTRICT', 'REVOKE', 'RIGHT', RLIKE, 'ROLE', 'ROLES', 'ROLLBACK', 'ROLLUP', 'ROW', 'ROWS', 'SECOND', 'SCHEMA', 'SELECT', 'SEMI', 'SEPARATED', 'SERDE', 'SERDEPROPERTIES', 'SESSION_USER', 'SET', 'MINUS', 'SETS', 'SHOW', 'SKEWED', 'SOME', 'SORT', 'SORTED', 'START', 'STATISTICS', 'STORED', 'STRATIFY', 'STRUCT', 'SUBSTR', 'SUBSTRING', 'SYNC', 'TABLE', 'TABLES', 'TABLESAMPLE', 'TBLPROPERTIES', TEMPORARY, 'TERMINATED', 'THEN', 'TIME', 'TO', 'TOUCH', 'TRAILING', 'TRANSACTION', 'TRANSACTIONS', 'TRANSFORM', 'TRIM', 'TRUE', 'TRUNCATE', 'TRY_CAST', 'TYPE', 'UNARCHIVE', 'UNBOUNDED', 'UNCACHE', 'UNION', 'UNIQUE', 'UNKNOWN', 'UNLOCK', 'UNSET', 'UPDATE', 'USE', 'USER', 'USING', 'VALUES', 'VIEW', 'VIEWS', 'WHEN', 'WHERE', 'WINDOW', 'WITH', 'YEAR', 'ZONE', '+', '-', '*', '~', STRING, BIGINT_LITERAL, SMALLINT_LITERAL, TINYINT_LITERAL, INTEGER_VALUE, EXPONENT_VALUE, DECIMAL_VALUE, FLOAT_LITERAL, DOUBLE_LITERAL, BIGDECIMAL_LITERAL, IDENTIFIER, BACKQUOTED_IDENTIFIER}(line 1, pos 30)",
            "log_location":"(write) log_w_sql_avro (line 3697)",
            "pass":true
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws110; line 1 pos 14;",
            "write_value":"No output, find exception: <console>:1: error: unclosed string literal",
            "log_location":"(read) log_w_df_r_df_orc (line 2540)",
            "pass":true
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: mismatched input '\"' expecting {'(', 'ADD', 'AFTER', 'ALL', 'ALTER', 'ANALYZE', 'AND', 'ANTI', 'ANY', 'ARCHIVE', 'ARRAY', 'AS', 'ASC', 'AT', 'AUTHORIZATION', 'BETWEEN', 'BOTH', 'BUCKET', 'BUCKETS', 'BY', 'CACHE', 'CASCADE', 'CASE', 'CAST', 'CHANGE', 'CHECK', 'CLEAR', 'CLUSTER', 'CLUSTERED', 'CODEGEN', 'COLLATE', 'COLLECTION', 'COLUMN', 'COLUMNS', 'COMMENT', 'COMMIT', 'COMPACT', 'COMPACTIONS', 'COMPUTE', 'CONCATENATE', 'CONSTRAINT', 'COST', 'CREATE', 'CROSS', 'CUBE', 'CURRENT', 'CURRENT_DATE', 'CURRENT_TIME', 'CURRENT_TIMESTAMP', 'CURRENT_USER', 'DAY', 'DATA', 'DATABASE', DATABASES, 'DBPROPERTIES', 'DEFINED', 'DELETE', 'DELIMITED', 'DESC', 'DESCRIBE', 'DFS', 'DIRECTORIES', 'DIRECTORY', 'DISTINCT', 'DISTRIBUTE', 'DIV', 'DROP', 'ELSE', 'END', 'ESCAPE', 'ESCAPED', 'EXCEPT', 'EXCHANGE', 'EXISTS', 'EXPLAIN', 'EXPORT', 'EXTENDED', 'EXTERNAL', 'EXTRACT', 'FALSE', 'FETCH', 'FIELDS', 'FILTER', 'FILEFORMAT', 'FIRST', 'FOLLOWING', 'FOR', 'FOREIGN', 'FORMAT', 'FORMATTED', 'FROM', 'FULL', 'FUNCTION', 'FUNCTIONS', 'GLOBAL', 'GRANT', 'GROUP', 'GROUPING', 'HAVING', 'HOUR', 'IF', 'IGNORE', 'IMPORT', 'IN', 'INDEX', 'INDEXES', 'INNER', 'INPATH', 'INPUTFORMAT', 'INSERT', 'INTERSECT', 'INTERVAL', 'INTO', 'IS', 'ITEMS', 'JOIN', 'KEYS', 'LAST', 'LATERAL', 'LAZY', 'LEADING', 'LEFT', 'LIKE', 'LIMIT', 'LINES', 'LIST', 'LOAD', 'LOCAL', 'LOCATION', 'LOCK', 'LOCKS', 'LOGICAL', 'MACRO', 'MAP', 'MATCHED', 'MERGE', 'MINUTE', 'MONTH', 'MSCK', 'NAMESPACE', 'NAMESPACES', 'NATURAL', 'NO', NOT, 'NULL', 'NULLS', 'OF', 'ON', 'ONLY', 'OPTION', 'OPTIONS', 'OR', 'ORDER', 'OUT', 'OUTER', 'OUTPUTFORMAT', 'OVER', 'OVERLAPS', 'OVERLAY', 'OVERWRITE', 'PARTITION', 'PARTITIONED', 'PARTITIONS', 'PERCENT', 'PIVOT', 'PLACING', 'POSITION', 'PRECEDING', 'PRIMARY', 'PRINCIPALS', 'PROPERTIES', 'PURGE', 'QUERY', 'RANGE', 'RECORDREADER', 'RECORDWRITER', 'RECOVER', 'REDUCE', 'REFERENCES', 'REFRESH', 'RENAME', 'REPAIR', 'REPLACE', 'RESET', 'RESPECT', 'RESTRICT', 'REVOKE', 'RIGHT', RLIKE, 'ROLE', 'ROLES', 'ROLLBACK', 'ROLLUP', 'ROW', 'ROWS', 'SECOND', 'SCHEMA', 'SELECT', 'SEMI', 'SEPARATED', 'SERDE', 'SERDEPROPERTIES', 'SESSION_USER', 'SET', 'MINUS', 'SETS', 'SHOW', 'SKEWED', 'SOME', 'SORT', 'SORTED', 'START', 'STATISTICS', 'STORED', 'STRATIFY', 'STRUCT', 'SUBSTR', 'SUBSTRING', 'SYNC', 'TABLE', 'TABLES', 'TABLESAMPLE', 'TBLPROPERTIES', TEMPORARY, 'TERMINATED', 'THEN', 'TIME', 'TO', 'TOUCH', 'TRAILING', 'TRANSACTION', 'TRANSACTIONS', 'TRANSFORM', 'TRIM', 'TRUE', 'TRUNCATE', 'TRY_CAST', 'TYPE', 'UNARCHIVE', 'UNBOUNDED', 'UNCACHE', 'UNION', 'UNIQUE', 'UNKNOWN', 'UNLOCK', 'UNSET', 'UPDATE', 'USE', 'USER', 'USING', 'VALUES', 'VIEW', 'VIEWS', 'WHEN', 'WHERE', 'WINDOW', 'WITH', 'YEAR', 'ZONE', '+', '-', '*', '~', STRING, BIGINT_LITERAL, SMALLINT_LITERAL, TINYINT_LITERAL, INTEGER_VALUE, EXPONENT_VALUE, DECIMAL_VALUE, FLOAT_LITERAL, DOUBLE_LITERAL, BIGDECIMAL_LITERAL, IDENTIFIER, BACKQUOTED_IDENTIFIER}(line 1, pos 30)",
            "log_location":"(write) log_w_sql_orc (line 4599)",
            "pass":true
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: <console>:1: error: unclosed string literal",
            "log_location":"(write) log_w_df_avro (line 7157)",
            "pass":true
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: mismatched input '\"' expecting {'(', 'ADD', 'AFTER', 'ALL', 'ALTER', 'ANALYZE', 'AND', 'ANTI', 'ANY', 'ARCHIVE', 'ARRAY', 'AS', 'ASC', 'AT', 'AUTHORIZATION', 'BETWEEN', 'BOTH', 'BUCKET', 'BUCKETS', 'BY', 'CACHE', 'CASCADE', 'CASE', 'CAST', 'CHANGE', 'CHECK', 'CLEAR', 'CLUSTER', 'CLUSTERED', 'CODEGEN', 'COLLATE', 'COLLECTION', 'COLUMN', 'COLUMNS', 'COMMENT', 'COMMIT', 'COMPACT', 'COMPACTIONS', 'COMPUTE', 'CONCATENATE', 'CONSTRAINT', 'COST', 'CREATE', 'CROSS', 'CUBE', 'CURRENT', 'CURRENT_DATE', 'CURRENT_TIME', 'CURRENT_TIMESTAMP', 'CURRENT_USER', 'DAY', 'DATA', 'DATABASE', DATABASES, 'DBPROPERTIES', 'DEFINED', 'DELETE', 'DELIMITED', 'DESC', 'DESCRIBE', 'DFS', 'DIRECTORIES', 'DIRECTORY', 'DISTINCT', 'DISTRIBUTE', 'DIV', 'DROP', 'ELSE', 'END', 'ESCAPE', 'ESCAPED', 'EXCEPT', 'EXCHANGE', 'EXISTS', 'EXPLAIN', 'EXPORT', 'EXTENDED', 'EXTERNAL', 'EXTRACT', 'FALSE', 'FETCH', 'FIELDS', 'FILTER', 'FILEFORMAT', 'FIRST', 'FOLLOWING', 'FOR', 'FOREIGN', 'FORMAT', 'FORMATTED', 'FROM', 'FULL', 'FUNCTION', 'FUNCTIONS', 'GLOBAL', 'GRANT', 'GROUP', 'GROUPING', 'HAVING', 'HOUR', 'IF', 'IGNORE', 'IMPORT', 'IN', 'INDEX', 'INDEXES', 'INNER', 'INPATH', 'INPUTFORMAT', 'INSERT', 'INTERSECT', 'INTERVAL', 'INTO', 'IS', 'ITEMS', 'JOIN', 'KEYS', 'LAST', 'LATERAL', 'LAZY', 'LEADING', 'LEFT', 'LIKE', 'LIMIT', 'LINES', 'LIST', 'LOAD', 'LOCAL', 'LOCATION', 'LOCK', 'LOCKS', 'LOGICAL', 'MACRO', 'MAP', 'MATCHED', 'MERGE', 'MINUTE', 'MONTH', 'MSCK', 'NAMESPACE', 'NAMESPACES', 'NATURAL', 'NO', NOT, 'NULL', 'NULLS', 'OF', 'ON', 'ONLY', 'OPTION', 'OPTIONS', 'OR', 'ORDER', 'OUT', 'OUTER', 'OUTPUTFORMAT', 'OVER', 'OVERLAPS', 'OVERLAY', 'OVERWRITE', 'PARTITION', 'PARTITIONED', 'PARTITIONS', 'PERCENT', 'PIVOT', 'PLACING', 'POSITION', 'PRECEDING', 'PRIMARY', 'PRINCIPALS', 'PROPERTIES', 'PURGE', 'QUERY', 'RANGE', 'RECORDREADER', 'RECORDWRITER', 'RECOVER', 'REDUCE', 'REFERENCES', 'REFRESH', 'RENAME', 'REPAIR', 'REPLACE', 'RESET', 'RESPECT', 'RESTRICT', 'REVOKE', 'RIGHT', RLIKE, 'ROLE', 'ROLES', 'ROLLBACK', 'ROLLUP', 'ROW', 'ROWS', 'SECOND', 'SCHEMA', 'SELECT', 'SEMI', 'SEPARATED', 'SERDE', 'SERDEPROPERTIES', 'SESSION_USER', 'SET', 'MINUS', 'SETS', 'SHOW', 'SKEWED', 'SOME', 'SORT', 'SORTED', 'START', 'STATISTICS', 'STORED', 'STRATIFY', 'STRUCT', 'SUBSTR', 'SUBSTRING', 'SYNC', 'TABLE', 'TABLES', 'TABLESAMPLE', 'TBLPROPERTIES', TEMPORARY, 'TERMINATED', 'THEN', 'TIME', 'TO', 'TOUCH', 'TRAILING', 'TRANSACTION', 'TRANSACTIONS', 'TRANSFORM', 'TRIM', 'TRUE', 'TRUNCATE', 'TRY_CAST', 'TYPE', 'UNARCHIVE', 'UNBOUNDED', 'UNCACHE', 'UNION', 'UNIQUE', 'UNKNOWN', 'UNLOCK', 'UNSET', 'UPDATE', 'USE', 'USER', 'USING', 'VALUES', 'VIEW', 'VIEWS', 'WHEN', 'WHERE', 'WINDOW', 'WITH', 'YEAR', 'ZONE', '+', '-', '*', '~', STRING, BIGINT_LITERAL, SMALLINT_LITERAL, TINYINT_LITERAL, INTEGER_VALUE, EXPONENT_VALUE, DECIMAL_VALUE, FLOAT_LITERAL, DOUBLE_LITERAL, BIGDECIMAL_LITERAL, IDENTIFIER, BACKQUOTED_IDENTIFIER}(line 1, pos 30)",
            "log_location":"(write) log_w_sql_avro (line 3697)",
            "pass":true
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: <console>:1: error: unclosed string literal",
            "log_location":"(write) log_w_df_parquet (line 14752)",
            "pass":true
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: mismatched input '\"' expecting {'(', 'ADD', 'AFTER', 'ALL', 'ALTER', 'ANALYZE', 'AND', 'ANTI', 'ANY', 'ARCHIVE', 'ARRAY', 'AS', 'ASC', 'AT', 'AUTHORIZATION', 'BETWEEN', 'BOTH', 'BUCKET', 'BUCKETS', 'BY', 'CACHE', 'CASCADE', 'CASE', 'CAST', 'CHANGE', 'CHECK', 'CLEAR', 'CLUSTER', 'CLUSTERED', 'CODEGEN', 'COLLATE', 'COLLECTION', 'COLUMN', 'COLUMNS', 'COMMENT', 'COMMIT', 'COMPACT', 'COMPACTIONS', 'COMPUTE', 'CONCATENATE', 'CONSTRAINT', 'COST', 'CREATE', 'CROSS', 'CUBE', 'CURRENT', 'CURRENT_DATE', 'CURRENT_TIME', 'CURRENT_TIMESTAMP', 'CURRENT_USER', 'DAY', 'DATA', 'DATABASE', DATABASES, 'DBPROPERTIES', 'DEFINED', 'DELETE', 'DELIMITED', 'DESC', 'DESCRIBE', 'DFS', 'DIRECTORIES', 'DIRECTORY', 'DISTINCT', 'DISTRIBUTE', 'DIV', 'DROP', 'ELSE', 'END', 'ESCAPE', 'ESCAPED', 'EXCEPT', 'EXCHANGE', 'EXISTS', 'EXPLAIN', 'EXPORT', 'EXTENDED', 'EXTERNAL', 'EXTRACT', 'FALSE', 'FETCH', 'FIELDS', 'FILTER', 'FILEFORMAT', 'FIRST', 'FOLLOWING', 'FOR', 'FOREIGN', 'FORMAT', 'FORMATTED', 'FROM', 'FULL', 'FUNCTION', 'FUNCTIONS', 'GLOBAL', 'GRANT', 'GROUP', 'GROUPING', 'HAVING', 'HOUR', 'IF', 'IGNORE', 'IMPORT', 'IN', 'INDEX', 'INDEXES', 'INNER', 'INPATH', 'INPUTFORMAT', 'INSERT', 'INTERSECT', 'INTERVAL', 'INTO', 'IS', 'ITEMS', 'JOIN', 'KEYS', 'LAST', 'LATERAL', 'LAZY', 'LEADING', 'LEFT', 'LIKE', 'LIMIT', 'LINES', 'LIST', 'LOAD', 'LOCAL', 'LOCATION', 'LOCK', 'LOCKS', 'LOGICAL', 'MACRO', 'MAP', 'MATCHED', 'MERGE', 'MINUTE', 'MONTH', 'MSCK', 'NAMESPACE', 'NAMESPACES', 'NATURAL', 'NO', NOT, 'NULL', 'NULLS', 'OF', 'ON', 'ONLY', 'OPTION', 'OPTIONS', 'OR', 'ORDER', 'OUT', 'OUTER', 'OUTPUTFORMAT', 'OVER', 'OVERLAPS', 'OVERLAY', 'OVERWRITE', 'PARTITION', 'PARTITIONED', 'PARTITIONS', 'PERCENT', 'PIVOT', 'PLACING', 'POSITION', 'PRECEDING', 'PRIMARY', 'PRINCIPALS', 'PROPERTIES', 'PURGE', 'QUERY', 'RANGE', 'RECORDREADER', 'RECORDWRITER', 'RECOVER', 'REDUCE', 'REFERENCES', 'REFRESH', 'RENAME', 'REPAIR', 'REPLACE', 'RESET', 'RESPECT', 'RESTRICT', 'REVOKE', 'RIGHT', RLIKE, 'ROLE', 'ROLES', 'ROLLBACK', 'ROLLUP', 'ROW', 'ROWS', 'SECOND', 'SCHEMA', 'SELECT', 'SEMI', 'SEPARATED', 'SERDE', 'SERDEPROPERTIES', 'SESSION_USER', 'SET', 'MINUS', 'SETS', 'SHOW', 'SKEWED', 'SOME', 'SORT', 'SORTED', 'START', 'STATISTICS', 'STORED', 'STRATIFY', 'STRUCT', 'SUBSTR', 'SUBSTRING', 'SYNC', 'TABLE', 'TABLES', 'TABLESAMPLE', 'TBLPROPERTIES', TEMPORARY, 'TERMINATED', 'THEN', 'TIME', 'TO', 'TOUCH', 'TRAILING', 'TRANSACTION', 'TRANSACTIONS', 'TRANSFORM', 'TRIM', 'TRUE', 'TRUNCATE', 'TRY_CAST', 'TYPE', 'UNARCHIVE', 'UNBOUNDED', 'UNCACHE', 'UNION', 'UNIQUE', 'UNKNOWN', 'UNLOCK', 'UNSET', 'UPDATE', 'USE', 'USER', 'USING', 'VALUES', 'VIEW', 'VIEWS', 'WHEN', 'WHERE', 'WINDOW', 'WITH', 'YEAR', 'ZONE', '+', '-', '*', '~', STRING, BIGINT_LITERAL, SMALLINT_LITERAL, TINYINT_LITERAL, INTEGER_VALUE, EXPONENT_VALUE, DECIMAL_VALUE, FLOAT_LITERAL, DOUBLE_LITERAL, BIGDECIMAL_LITERAL, IDENTIFIER, BACKQUOTED_IDENTIFIER}(line 1, pos 30)",
            "log_location":"(write) log_w_sql_parquet (line 4600)",
            "pass":true
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws110; line 1 pos 14;",
            "write_value":"No output, find exception: <console>:1: error: unclosed string literal",
            "log_location":"(read) log_w_df_r_df_parquet (line 2542)",
            "pass":true
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: mismatched input '\"' expecting {'(', 'ADD', 'AFTER', 'ALL', 'ALTER', 'ANALYZE', 'AND', 'ANTI', 'ANY', 'ARCHIVE', 'ARRAY', 'AS', 'ASC', 'AT', 'AUTHORIZATION', 'BETWEEN', 'BOTH', 'BUCKET', 'BUCKETS', 'BY', 'CACHE', 'CASCADE', 'CASE', 'CAST', 'CHANGE', 'CHECK', 'CLEAR', 'CLUSTER', 'CLUSTERED', 'CODEGEN', 'COLLATE', 'COLLECTION', 'COLUMN', 'COLUMNS', 'COMMENT', 'COMMIT', 'COMPACT', 'COMPACTIONS', 'COMPUTE', 'CONCATENATE', 'CONSTRAINT', 'COST', 'CREATE', 'CROSS', 'CUBE', 'CURRENT', 'CURRENT_DATE', 'CURRENT_TIME', 'CURRENT_TIMESTAMP', 'CURRENT_USER', 'DAY', 'DATA', 'DATABASE', DATABASES, 'DBPROPERTIES', 'DEFINED', 'DELETE', 'DELIMITED', 'DESC', 'DESCRIBE', 'DFS', 'DIRECTORIES', 'DIRECTORY', 'DISTINCT', 'DISTRIBUTE', 'DIV', 'DROP', 'ELSE', 'END', 'ESCAPE', 'ESCAPED', 'EXCEPT', 'EXCHANGE', 'EXISTS', 'EXPLAIN', 'EXPORT', 'EXTENDED', 'EXTERNAL', 'EXTRACT', 'FALSE', 'FETCH', 'FIELDS', 'FILTER', 'FILEFORMAT', 'FIRST', 'FOLLOWING', 'FOR', 'FOREIGN', 'FORMAT', 'FORMATTED', 'FROM', 'FULL', 'FUNCTION', 'FUNCTIONS', 'GLOBAL', 'GRANT', 'GROUP', 'GROUPING', 'HAVING', 'HOUR', 'IF', 'IGNORE', 'IMPORT', 'IN', 'INDEX', 'INDEXES', 'INNER', 'INPATH', 'INPUTFORMAT', 'INSERT', 'INTERSECT', 'INTERVAL', 'INTO', 'IS', 'ITEMS', 'JOIN', 'KEYS', 'LAST', 'LATERAL', 'LAZY', 'LEADING', 'LEFT', 'LIKE', 'LIMIT', 'LINES', 'LIST', 'LOAD', 'LOCAL', 'LOCATION', 'LOCK', 'LOCKS', 'LOGICAL', 'MACRO', 'MAP', 'MATCHED', 'MERGE', 'MINUTE', 'MONTH', 'MSCK', 'NAMESPACE', 'NAMESPACES', 'NATURAL', 'NO', NOT, 'NULL', 'NULLS', 'OF', 'ON', 'ONLY', 'OPTION', 'OPTIONS', 'OR', 'ORDER', 'OUT', 'OUTER', 'OUTPUTFORMAT', 'OVER', 'OVERLAPS', 'OVERLAY', 'OVERWRITE', 'PARTITION', 'PARTITIONED', 'PARTITIONS', 'PERCENT', 'PIVOT', 'PLACING', 'POSITION', 'PRECEDING', 'PRIMARY', 'PRINCIPALS', 'PROPERTIES', 'PURGE', 'QUERY', 'RANGE', 'RECORDREADER', 'RECORDWRITER', 'RECOVER', 'REDUCE', 'REFERENCES', 'REFRESH', 'RENAME', 'REPAIR', 'REPLACE', 'RESET', 'RESPECT', 'RESTRICT', 'REVOKE', 'RIGHT', RLIKE, 'ROLE', 'ROLES', 'ROLLBACK', 'ROLLUP', 'ROW', 'ROWS', 'SECOND', 'SCHEMA', 'SELECT', 'SEMI', 'SEPARATED', 'SERDE', 'SERDEPROPERTIES', 'SESSION_USER', 'SET', 'MINUS', 'SETS', 'SHOW', 'SKEWED', 'SOME', 'SORT', 'SORTED', 'START', 'STATISTICS', 'STORED', 'STRATIFY', 'STRUCT', 'SUBSTR', 'SUBSTRING', 'SYNC', 'TABLE', 'TABLES', 'TABLESAMPLE', 'TBLPROPERTIES', TEMPORARY, 'TERMINATED', 'THEN', 'TIME', 'TO', 'TOUCH', 'TRAILING', 'TRANSACTION', 'TRANSACTIONS', 'TRANSFORM', 'TRIM', 'TRUE', 'TRUNCATE', 'TRY_CAST', 'TYPE', 'UNARCHIVE', 'UNBOUNDED', 'UNCACHE', 'UNION', 'UNIQUE', 'UNKNOWN', 'UNLOCK', 'UNSET', 'UPDATE', 'USE', 'USER', 'USING', 'VALUES', 'VIEW', 'VIEWS', 'WHEN', 'WHERE', 'WINDOW', 'WITH', 'YEAR', 'ZONE', '+', '-', '*', '~', STRING, BIGINT_LITERAL, SMALLINT_LITERAL, TINYINT_LITERAL, INTEGER_VALUE, EXPONENT_VALUE, DECIMAL_VALUE, FLOAT_LITERAL, DOUBLE_LITERAL, BIGDECIMAL_LITERAL, IDENTIFIER, BACKQUOTED_IDENTIFIER}(line 1, pos 30)",
            "log_location":"(write) log_w_sql_orc (line 4599)",
            "pass":true
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: mismatched input '\"' expecting {'(', 'ADD', 'AFTER', 'ALL', 'ALTER', 'ANALYZE', 'AND', 'ANTI', 'ANY', 'ARCHIVE', 'ARRAY', 'AS', 'ASC', 'AT', 'AUTHORIZATION', 'BETWEEN', 'BOTH', 'BUCKET', 'BUCKETS', 'BY', 'CACHE', 'CASCADE', 'CASE', 'CAST', 'CHANGE', 'CHECK', 'CLEAR', 'CLUSTER', 'CLUSTERED', 'CODEGEN', 'COLLATE', 'COLLECTION', 'COLUMN', 'COLUMNS', 'COMMENT', 'COMMIT', 'COMPACT', 'COMPACTIONS', 'COMPUTE', 'CONCATENATE', 'CONSTRAINT', 'COST', 'CREATE', 'CROSS', 'CUBE', 'CURRENT', 'CURRENT_DATE', 'CURRENT_TIME', 'CURRENT_TIMESTAMP', 'CURRENT_USER', 'DAY', 'DATA', 'DATABASE', DATABASES, 'DBPROPERTIES', 'DEFINED', 'DELETE', 'DELIMITED', 'DESC', 'DESCRIBE', 'DFS', 'DIRECTORIES', 'DIRECTORY', 'DISTINCT', 'DISTRIBUTE', 'DIV', 'DROP', 'ELSE', 'END', 'ESCAPE', 'ESCAPED', 'EXCEPT', 'EXCHANGE', 'EXISTS', 'EXPLAIN', 'EXPORT', 'EXTENDED', 'EXTERNAL', 'EXTRACT', 'FALSE', 'FETCH', 'FIELDS', 'FILTER', 'FILEFORMAT', 'FIRST', 'FOLLOWING', 'FOR', 'FOREIGN', 'FORMAT', 'FORMATTED', 'FROM', 'FULL', 'FUNCTION', 'FUNCTIONS', 'GLOBAL', 'GRANT', 'GROUP', 'GROUPING', 'HAVING', 'HOUR', 'IF', 'IGNORE', 'IMPORT', 'IN', 'INDEX', 'INDEXES', 'INNER', 'INPATH', 'INPUTFORMAT', 'INSERT', 'INTERSECT', 'INTERVAL', 'INTO', 'IS', 'ITEMS', 'JOIN', 'KEYS', 'LAST', 'LATERAL', 'LAZY', 'LEADING', 'LEFT', 'LIKE', 'LIMIT', 'LINES', 'LIST', 'LOAD', 'LOCAL', 'LOCATION', 'LOCK', 'LOCKS', 'LOGICAL', 'MACRO', 'MAP', 'MATCHED', 'MERGE', 'MINUTE', 'MONTH', 'MSCK', 'NAMESPACE', 'NAMESPACES', 'NATURAL', 'NO', NOT, 'NULL', 'NULLS', 'OF', 'ON', 'ONLY', 'OPTION', 'OPTIONS', 'OR', 'ORDER', 'OUT', 'OUTER', 'OUTPUTFORMAT', 'OVER', 'OVERLAPS', 'OVERLAY', 'OVERWRITE', 'PARTITION', 'PARTITIONED', 'PARTITIONS', 'PERCENT', 'PIVOT', 'PLACING', 'POSITION', 'PRECEDING', 'PRIMARY', 'PRINCIPALS', 'PROPERTIES', 'PURGE', 'QUERY', 'RANGE', 'RECORDREADER', 'RECORDWRITER', 'RECOVER', 'REDUCE', 'REFERENCES', 'REFRESH', 'RENAME', 'REPAIR', 'REPLACE', 'RESET', 'RESPECT', 'RESTRICT', 'REVOKE', 'RIGHT', RLIKE, 'ROLE', 'ROLES', 'ROLLBACK', 'ROLLUP', 'ROW', 'ROWS', 'SECOND', 'SCHEMA', 'SELECT', 'SEMI', 'SEPARATED', 'SERDE', 'SERDEPROPERTIES', 'SESSION_USER', 'SET', 'MINUS', 'SETS', 'SHOW', 'SKEWED', 'SOME', 'SORT', 'SORTED', 'START', 'STATISTICS', 'STORED', 'STRATIFY', 'STRUCT', 'SUBSTR', 'SUBSTRING', 'SYNC', 'TABLE', 'TABLES', 'TABLESAMPLE', 'TBLPROPERTIES', TEMPORARY, 'TERMINATED', 'THEN', 'TIME', 'TO', 'TOUCH', 'TRAILING', 'TRANSACTION', 'TRANSACTIONS', 'TRANSFORM', 'TRIM', 'TRUE', 'TRUNCATE', 'TRY_CAST', 'TYPE', 'UNARCHIVE', 'UNBOUNDED', 'UNCACHE', 'UNION', 'UNIQUE', 'UNKNOWN', 'UNLOCK', 'UNSET', 'UPDATE', 'USE', 'USER', 'USING', 'VALUES', 'VIEW', 'VIEWS', 'WHEN', 'WHERE', 'WINDOW', 'WITH', 'YEAR', 'ZONE', '+', '-', '*', '~', STRING, BIGINT_LITERAL, SMALLINT_LITERAL, TINYINT_LITERAL, INTEGER_VALUE, EXPONENT_VALUE, DECIMAL_VALUE, FLOAT_LITERAL, DOUBLE_LITERAL, BIGDECIMAL_LITERAL, IDENTIFIER, BACKQUOTED_IDENTIFIER}(line 1, pos 30)",
            "log_location":"(write) log_w_sql_parquet (line 4600)",
            "pass":true
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws110; line 1 pos 14;",
            "write_value":"No output, find exception: <console>:1: error: unclosed string literal",
            "log_location":"(read) log_w_df_r_df_avro (line 4230)",
            "pass":true
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: <console>:1: error: unclosed string literal",
            "log_location":"(write) log_w_df_orc (line 14749)",
            "pass":true
        }
    },
    "111":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"bbbbbb, find exception: java.lang.RuntimeException: Exceeds char/varchar type length limitation: 5",
            "log_location":"(write) log_w_sql_avro (line 3714)",
            "pass":true
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws111; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.AnalysisException: char/varchar type can only be used in the table schema. You can set spark.sql.legacy.charVarcharAsString to true, so that Spark treat them as string type as same as Spark 3.0 and earlier",
            "log_location":"(read) log_w_df_r_df_orc (line 2580)",
            "pass":true
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"bbbbbb, find exception: java.lang.RuntimeException: Exceeds char/varchar type length limitation: 5",
            "log_location":"(write) log_w_sql_orc (line 4615)",
            "pass":true
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.AnalysisException: char/varchar type can only be used in the table schema. You can set spark.sql.legacy.charVarcharAsString to true, so that Spark treat them as string type as same as Spark 3.0 and earlier",
            "log_location":"(write) log_w_df_avro (line 7186)",
            "pass":true
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"bbbbbb, find exception: java.lang.RuntimeException: Exceeds char/varchar type length limitation: 5",
            "log_location":"(write) log_w_sql_avro (line 3714)",
            "pass":true
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.AnalysisException: char/varchar type can only be used in the table schema. You can set spark.sql.legacy.charVarcharAsString to true, so that Spark treat them as string type as same as Spark 3.0 and earlier",
            "log_location":"(write) log_w_df_parquet (line 14781)",
            "pass":true
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"bbbbbb, find exception: java.lang.RuntimeException: Exceeds char/varchar type length limitation: 5",
            "log_location":"(write) log_w_sql_parquet (line 4616)",
            "pass":true
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws111; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.AnalysisException: char/varchar type can only be used in the table schema. You can set spark.sql.legacy.charVarcharAsString to true, so that Spark treat them as string type as same as Spark 3.0 and earlier",
            "log_location":"(read) log_w_df_r_df_parquet (line 2582)",
            "pass":true
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"bbbbbb, find exception: java.lang.RuntimeException: Exceeds char/varchar type length limitation: 5",
            "log_location":"(write) log_w_sql_orc (line 4615)",
            "pass":true
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"bbbbbb, find exception: java.lang.RuntimeException: Exceeds char/varchar type length limitation: 5",
            "log_location":"(write) log_w_sql_parquet (line 4616)",
            "pass":true
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws111; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.AnalysisException: char/varchar type can only be used in the table schema. You can set spark.sql.legacy.charVarcharAsString to true, so that Spark treat them as string type as same as Spark 3.0 and earlier",
            "log_location":"(read) log_w_df_r_df_avro (line 4270)",
            "pass":true
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.AnalysisException: char/varchar type can only be used in the table schema. You can set spark.sql.legacy.charVarcharAsString to true, so that Spark treat them as string type as same as Spark 3.0 and earlier",
            "log_location":"(write) log_w_df_orc (line 14778)",
            "pass":true
        }
    },
    "117":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"spark, find exception: Error in query: Cannot write incompatible data to table '`default`.`ws117`':",
            "log_location":"(write) log_w_sql_avro (line 4103)",
            "pass":true
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws117; line 1 pos 14;",
            "write_value":"No output, find exception: <console>:1: error: unclosed character literal (or use \" for string literal \"spark\")",
            "log_location":"(read) log_w_df_r_df_orc (line 2660)",
            "pass":true
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"spark, find exception: Error in query: Cannot write incompatible data to table '`default`.`ws117`':",
            "log_location":"(write) log_w_sql_orc (line 4990)",
            "pass":true
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: <console>:1: error: unclosed character literal (or use \" for string literal \"spark\")",
            "log_location":"(write) log_w_df_avro (line 7300)",
            "pass":true
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"spark, find exception: Error in query: Cannot write incompatible data to table '`default`.`ws117`':",
            "log_location":"(write) log_w_sql_avro (line 4103)",
            "pass":true
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: <console>:1: error: unclosed character literal (or use \" for string literal \"spark\")",
            "log_location":"(write) log_w_df_parquet (line 14895)",
            "pass":true
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"spark, find exception: Error in query: Cannot write incompatible data to table '`default`.`ws117`':",
            "log_location":"(write) log_w_sql_parquet (line 4991)",
            "pass":true
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws117; line 1 pos 14;",
            "write_value":"No output, find exception: <console>:1: error: unclosed character literal (or use \" for string literal \"spark\")",
            "log_location":"(read) log_w_df_r_df_parquet (line 2662)",
            "pass":true
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"spark, find exception: Error in query: Cannot write incompatible data to table '`default`.`ws117`':",
            "log_location":"(write) log_w_sql_orc (line 4990)",
            "pass":true
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"spark, find exception: Error in query: Cannot write incompatible data to table '`default`.`ws117`':",
            "log_location":"(write) log_w_sql_parquet (line 4991)",
            "pass":true
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws117; line 1 pos 14;",
            "write_value":"No output, find exception: <console>:1: error: unclosed character literal (or use \" for string literal \"spark\")",
            "log_location":"(read) log_w_df_r_df_avro (line 4350)",
            "pass":true
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: <console>:1: error: unclosed character literal (or use \" for string literal \"spark\")",
            "log_location":"(write) log_w_df_orc (line 14892)",
            "pass":true
        }
    },
    "118":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: contains illegal character for hexBinary: 0spark(line 1, pos 30)",
            "log_location":"(write) log_w_sql_avro (line 4119)",
            "pass":true
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws118; line 1 pos 14;",
            "write_value":"No output, find exception: <console>:1: error: ')' expected but symbol literal found.",
            "log_location":"(read) log_w_df_r_df_orc (line 2700)",
            "pass":true
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: contains illegal character for hexBinary: 0spark(line 1, pos 30)",
            "log_location":"(write) log_w_sql_orc (line 5005)",
            "pass":true
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: <console>:1: error: ')' expected but symbol literal found.",
            "log_location":"(write) log_w_df_avro (line 7323)",
            "pass":true
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: contains illegal character for hexBinary: 0spark(line 1, pos 30)",
            "log_location":"(write) log_w_sql_avro (line 4119)",
            "pass":true
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: <console>:1: error: ')' expected but symbol literal found.",
            "log_location":"(write) log_w_df_parquet (line 14918)",
            "pass":true
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: contains illegal character for hexBinary: 0spark(line 1, pos 30)",
            "log_location":"(write) log_w_sql_parquet (line 5006)",
            "pass":true
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws118; line 1 pos 14;",
            "write_value":"No output, find exception: <console>:1: error: ')' expected but symbol literal found.",
            "log_location":"(read) log_w_df_r_df_parquet (line 2702)",
            "pass":true
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: contains illegal character for hexBinary: 0spark(line 1, pos 30)",
            "log_location":"(write) log_w_sql_orc (line 5005)",
            "pass":true
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: contains illegal character for hexBinary: 0spark(line 1, pos 30)",
            "log_location":"(write) log_w_sql_parquet (line 5006)",
            "pass":true
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws118; line 1 pos 14;",
            "write_value":"No output, find exception: <console>:1: error: ')' expected but symbol literal found.",
            "log_location":"(read) log_w_df_r_df_avro (line 4390)",
            "pass":true
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: <console>:1: error: ')' expected but symbol literal found.",
            "log_location":"(write) log_w_df_orc (line 14915)",
            "pass":true
        }
    },
    "119":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"25, find exception: Error in query: Cannot write incompatible data to table '`default`.`ws119`':",
            "log_location":"(write) log_w_sql_avro (line 4133)",
            "pass":true
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws119; line 1 pos 14;",
            "write_value":"No output, find exception: java.lang.RuntimeException: Error while encoding: java.lang.RuntimeException: java.lang.Integer is not a valid external type for schema of binary",
            "log_location":"(read) log_w_df_r_df_orc (line 2740)",
            "pass":true
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"25, find exception: Error in query: Cannot write incompatible data to table '`default`.`ws119`':",
            "log_location":"(write) log_w_sql_orc (line 5018)",
            "pass":true
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.RuntimeException: Error while encoding: java.lang.RuntimeException: java.lang.Integer is not a valid external type for schema of binary",
            "log_location":"(write) log_w_df_avro (line 7359)",
            "pass":true
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"25, find exception: Error in query: Cannot write incompatible data to table '`default`.`ws119`':",
            "log_location":"(write) log_w_sql_avro (line 4133)",
            "pass":true
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.RuntimeException: Error while encoding: java.lang.RuntimeException: java.lang.Integer is not a valid external type for schema of binary",
            "log_location":"(write) log_w_df_parquet (line 14954)",
            "pass":true
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"25, find exception: Error in query: Cannot write incompatible data to table '`default`.`ws119`':",
            "log_location":"(write) log_w_sql_parquet (line 5019)",
            "pass":true
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws119; line 1 pos 14;",
            "write_value":"No output, find exception: java.lang.RuntimeException: Error while encoding: java.lang.RuntimeException: java.lang.Integer is not a valid external type for schema of binary",
            "log_location":"(read) log_w_df_r_df_parquet (line 2742)",
            "pass":true
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"25, find exception: Error in query: Cannot write incompatible data to table '`default`.`ws119`':",
            "log_location":"(write) log_w_sql_orc (line 5018)",
            "pass":true
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"25, find exception: Error in query: Cannot write incompatible data to table '`default`.`ws119`':",
            "log_location":"(write) log_w_sql_parquet (line 5019)",
            "pass":true
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws119; line 1 pos 14;",
            "write_value":"No output, find exception: java.lang.RuntimeException: Error while encoding: java.lang.RuntimeException: java.lang.Integer is not a valid external type for schema of binary",
            "log_location":"(read) log_w_df_r_df_avro (line 4430)",
            "pass":true
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.RuntimeException: Error while encoding: java.lang.RuntimeException: java.lang.Integer is not a valid external type for schema of binary",
            "log_location":"(write) log_w_df_orc (line 14951)",
            "pass":true
        }
    },
    "120":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"1.1, find exception: Error in query: Cannot write incompatible data to table '`default`.`ws120`':",
            "log_location":"(write) log_w_sql_avro (line 4143)",
            "pass":true
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws120; line 1 pos 14;",
            "write_value":"No output, find exception: java.lang.RuntimeException: Error while encoding: java.lang.RuntimeException: java.lang.Double is not a valid external type for schema of binary",
            "log_location":"(read) log_w_df_r_df_orc (line 2780)",
            "pass":true
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"1.1, find exception: Error in query: Cannot write incompatible data to table '`default`.`ws120`':",
            "log_location":"(write) log_w_sql_orc (line 5027)",
            "pass":true
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.RuntimeException: Error while encoding: java.lang.RuntimeException: java.lang.Double is not a valid external type for schema of binary",
            "log_location":"(write) log_w_df_avro (line 7844)",
            "pass":true
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"1.1, find exception: Error in query: Cannot write incompatible data to table '`default`.`ws120`':",
            "log_location":"(write) log_w_sql_avro (line 4143)",
            "pass":true
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.RuntimeException: Error while encoding: java.lang.RuntimeException: java.lang.Double is not a valid external type for schema of binary",
            "log_location":"(write) log_w_df_parquet (line 15439)",
            "pass":true
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"1.1, find exception: Error in query: Cannot write incompatible data to table '`default`.`ws120`':",
            "log_location":"(write) log_w_sql_parquet (line 5028)",
            "pass":true
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws120; line 1 pos 14;",
            "write_value":"No output, find exception: java.lang.RuntimeException: Error while encoding: java.lang.RuntimeException: java.lang.Double is not a valid external type for schema of binary",
            "log_location":"(read) log_w_df_r_df_parquet (line 2782)",
            "pass":true
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"1.1, find exception: Error in query: Cannot write incompatible data to table '`default`.`ws120`':",
            "log_location":"(write) log_w_sql_orc (line 5027)",
            "pass":true
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"1.1, find exception: Error in query: Cannot write incompatible data to table '`default`.`ws120`':",
            "log_location":"(write) log_w_sql_parquet (line 5028)",
            "pass":true
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws120; line 1 pos 14;",
            "write_value":"No output, find exception: java.lang.RuntimeException: Error while encoding: java.lang.RuntimeException: java.lang.Double is not a valid external type for schema of binary",
            "log_location":"(read) log_w_df_r_df_avro (line 4470)",
            "pass":true
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.RuntimeException: Error while encoding: java.lang.RuntimeException: java.lang.Double is not a valid external type for schema of binary",
            "log_location":"(write) log_w_df_orc (line 15436)",
            "pass":true
        }
    },
    "121":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: 'InsertIntoStatement 'UnresolvedRelation [ws121], [], false, false, false",
            "log_location":"(write) log_w_sql_avro (line 4156)",
            "pass":true
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws121; line 1 pos 14;",
            "write_value":"No output, find exception: <console>:1: error: Invalid literal number",
            "log_location":"(read) log_w_df_r_df_orc (line 2820)",
            "pass":true
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: 'InsertIntoStatement 'UnresolvedRelation [ws121], [], false, false, false",
            "log_location":"(write) log_w_sql_orc (line 5039)",
            "pass":true
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: <console>:1: error: Invalid literal number",
            "log_location":"(write) log_w_df_avro (line 8319)",
            "pass":true
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: 'InsertIntoStatement 'UnresolvedRelation [ws121], [], false, false, false",
            "log_location":"(write) log_w_sql_avro (line 4156)",
            "pass":true
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: <console>:1: error: Invalid literal number",
            "log_location":"(write) log_w_df_parquet (line 15914)",
            "pass":true
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: 'InsertIntoStatement 'UnresolvedRelation [ws121], [], false, false, false",
            "log_location":"(write) log_w_sql_parquet (line 5040)",
            "pass":true
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws121; line 1 pos 14;",
            "write_value":"No output, find exception: <console>:1: error: Invalid literal number",
            "log_location":"(read) log_w_df_r_df_parquet (line 2822)",
            "pass":true
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: 'InsertIntoStatement 'UnresolvedRelation [ws121], [], false, false, false",
            "log_location":"(write) log_w_sql_orc (line 5039)",
            "pass":true
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: 'InsertIntoStatement 'UnresolvedRelation [ws121], [], false, false, false",
            "log_location":"(write) log_w_sql_parquet (line 5040)",
            "pass":true
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws121; line 1 pos 14;",
            "write_value":"No output, find exception: <console>:1: error: Invalid literal number",
            "log_location":"(read) log_w_df_r_df_avro (line 4510)",
            "pass":true
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: <console>:1: error: Invalid literal number",
            "log_location":"(write) log_w_df_orc (line 15911)",
            "pass":true
        }
    },
    "122":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: mismatched input '<EOF>' expecting {'(', 'ADD', 'AFTER', 'ALL', 'ALTER', 'ANALYZE', 'AND', 'ANTI', 'ANY', 'ARCHIVE', 'ARRAY', 'AS', 'ASC', 'AT', 'AUTHORIZATION', 'BETWEEN', 'BOTH', 'BUCKET', 'BUCKETS', 'BY', 'CACHE', 'CASCADE', 'CASE', 'CAST', 'CHANGE', 'CHECK', 'CLEAR', 'CLUSTER', 'CLUSTERED', 'CODEGEN', 'COLLATE', 'COLLECTION', 'COLUMN', 'COLUMNS', 'COMMENT', 'COMMIT', 'COMPACT', 'COMPACTIONS', 'COMPUTE', 'CONCATENATE', 'CONSTRAINT', 'COST', 'CREATE', 'CROSS', 'CUBE', 'CURRENT', 'CURRENT_DATE', 'CURRENT_TIME', 'CURRENT_TIMESTAMP', 'CURRENT_USER', 'DAY', 'DATA', 'DATABASE', DATABASES, 'DBPROPERTIES', 'DEFINED', 'DELETE', 'DELIMITED', 'DESC', 'DESCRIBE', 'DFS', 'DIRECTORIES', 'DIRECTORY', 'DISTINCT', 'DISTRIBUTE', 'DIV', 'DROP', 'ELSE', 'END', 'ESCAPE', 'ESCAPED', 'EXCEPT', 'EXCHANGE', 'EXISTS', 'EXPLAIN', 'EXPORT', 'EXTENDED', 'EXTERNAL', 'EXTRACT', 'FALSE', 'FETCH', 'FIELDS', 'FILTER', 'FILEFORMAT', 'FIRST', 'FOLLOWING', 'FOR', 'FOREIGN', 'FORMAT', 'FORMATTED', 'FROM', 'FULL', 'FUNCTION', 'FUNCTIONS', 'GLOBAL', 'GRANT', 'GROUP', 'GROUPING', 'HAVING', 'HOUR', 'IF', 'IGNORE', 'IMPORT', 'IN', 'INDEX', 'INDEXES', 'INNER', 'INPATH', 'INPUTFORMAT', 'INSERT', 'INTERSECT', 'INTERVAL', 'INTO', 'IS', 'ITEMS', 'JOIN', 'KEYS', 'LAST', 'LATERAL', 'LAZY', 'LEADING', 'LEFT', 'LIKE', 'LIMIT', 'LINES', 'LIST', 'LOAD', 'LOCAL', 'LOCATION', 'LOCK', 'LOCKS', 'LOGICAL', 'MACRO', 'MAP', 'MATCHED', 'MERGE', 'MINUTE', 'MONTH', 'MSCK', 'NAMESPACE', 'NAMESPACES', 'NATURAL', 'NO', NOT, 'NULL', 'NULLS', 'OF', 'ON', 'ONLY', 'OPTION', 'OPTIONS', 'OR', 'ORDER', 'OUT', 'OUTER', 'OUTPUTFORMAT', 'OVER', 'OVERLAPS', 'OVERLAY', 'OVERWRITE', 'PARTITION', 'PARTITIONED', 'PARTITIONS', 'PERCENT', 'PIVOT', 'PLACING', 'POSITION', 'PRECEDING', 'PRIMARY', 'PRINCIPALS', 'PROPERTIES', 'PURGE', 'QUERY', 'RANGE', 'RECORDREADER', 'RECORDWRITER', 'RECOVER', 'REDUCE', 'REFERENCES', 'REFRESH', 'RENAME', 'REPAIR', 'REPLACE', 'RESET', 'RESPECT', 'RESTRICT', 'REVOKE', 'RIGHT', RLIKE, 'ROLE', 'ROLES', 'ROLLBACK', 'ROLLUP', 'ROW', 'ROWS', 'SECOND', 'SCHEMA', 'SELECT', 'SEMI', 'SEPARATED', 'SERDE', 'SERDEPROPERTIES', 'SESSION_USER', 'SET', 'MINUS', 'SETS', 'SHOW', 'SKEWED', 'SOME', 'SORT', 'SORTED', 'START', 'STATISTICS', 'STORED', 'STRATIFY', 'STRUCT', 'SUBSTR', 'SUBSTRING', 'SYNC', 'TABLE', 'TABLES', 'TABLESAMPLE', 'TBLPROPERTIES', TEMPORARY, 'TERMINATED', 'THEN', 'TIME', 'TO', 'TOUCH', 'TRAILING', 'TRANSACTION', 'TRANSACTIONS', 'TRANSFORM', 'TRIM', 'TRUE', 'TRUNCATE', 'TRY_CAST', 'TYPE', 'UNARCHIVE', 'UNBOUNDED', 'UNCACHE', 'UNION', 'UNIQUE', 'UNKNOWN', 'UNLOCK', 'UNSET', 'UPDATE', 'USE', 'USER', 'USING', 'VALUES', 'VIEW', 'VIEWS', 'WHEN', 'WHERE', 'WINDOW', 'WITH', 'YEAR', 'ZONE', '+', '-', '*', '~', STRING, BIGINT_LITERAL, SMALLINT_LITERAL, TINYINT_LITERAL, INTEGER_VALUE, EXPONENT_VALUE, DECIMAL_VALUE, FLOAT_LITERAL, DOUBLE_LITERAL, BIGDECIMAL_LITERAL, IDENTIFIER, BACKQUOTED_IDENTIFIER}(line 1, pos 30)",
            "log_location":"(write) log_w_sql_avro (line 4174)",
            "pass":true
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws122; line 1 pos 14;",
            "write_value":"No output, find exception: <console>:1: error: illegal start of simple expression",
            "log_location":"(read) log_w_df_r_df_orc (line 2860)",
            "pass":true
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: mismatched input '<EOF>' expecting {'(', 'ADD', 'AFTER', 'ALL', 'ALTER', 'ANALYZE', 'AND', 'ANTI', 'ANY', 'ARCHIVE', 'ARRAY', 'AS', 'ASC', 'AT', 'AUTHORIZATION', 'BETWEEN', 'BOTH', 'BUCKET', 'BUCKETS', 'BY', 'CACHE', 'CASCADE', 'CASE', 'CAST', 'CHANGE', 'CHECK', 'CLEAR', 'CLUSTER', 'CLUSTERED', 'CODEGEN', 'COLLATE', 'COLLECTION', 'COLUMN', 'COLUMNS', 'COMMENT', 'COMMIT', 'COMPACT', 'COMPACTIONS', 'COMPUTE', 'CONCATENATE', 'CONSTRAINT', 'COST', 'CREATE', 'CROSS', 'CUBE', 'CURRENT', 'CURRENT_DATE', 'CURRENT_TIME', 'CURRENT_TIMESTAMP', 'CURRENT_USER', 'DAY', 'DATA', 'DATABASE', DATABASES, 'DBPROPERTIES', 'DEFINED', 'DELETE', 'DELIMITED', 'DESC', 'DESCRIBE', 'DFS', 'DIRECTORIES', 'DIRECTORY', 'DISTINCT', 'DISTRIBUTE', 'DIV', 'DROP', 'ELSE', 'END', 'ESCAPE', 'ESCAPED', 'EXCEPT', 'EXCHANGE', 'EXISTS', 'EXPLAIN', 'EXPORT', 'EXTENDED', 'EXTERNAL', 'EXTRACT', 'FALSE', 'FETCH', 'FIELDS', 'FILTER', 'FILEFORMAT', 'FIRST', 'FOLLOWING', 'FOR', 'FOREIGN', 'FORMAT', 'FORMATTED', 'FROM', 'FULL', 'FUNCTION', 'FUNCTIONS', 'GLOBAL', 'GRANT', 'GROUP', 'GROUPING', 'HAVING', 'HOUR', 'IF', 'IGNORE', 'IMPORT', 'IN', 'INDEX', 'INDEXES', 'INNER', 'INPATH', 'INPUTFORMAT', 'INSERT', 'INTERSECT', 'INTERVAL', 'INTO', 'IS', 'ITEMS', 'JOIN', 'KEYS', 'LAST', 'LATERAL', 'LAZY', 'LEADING', 'LEFT', 'LIKE', 'LIMIT', 'LINES', 'LIST', 'LOAD', 'LOCAL', 'LOCATION', 'LOCK', 'LOCKS', 'LOGICAL', 'MACRO', 'MAP', 'MATCHED', 'MERGE', 'MINUTE', 'MONTH', 'MSCK', 'NAMESPACE', 'NAMESPACES', 'NATURAL', 'NO', NOT, 'NULL', 'NULLS', 'OF', 'ON', 'ONLY', 'OPTION', 'OPTIONS', 'OR', 'ORDER', 'OUT', 'OUTER', 'OUTPUTFORMAT', 'OVER', 'OVERLAPS', 'OVERLAY', 'OVERWRITE', 'PARTITION', 'PARTITIONED', 'PARTITIONS', 'PERCENT', 'PIVOT', 'PLACING', 'POSITION', 'PRECEDING', 'PRIMARY', 'PRINCIPALS', 'PROPERTIES', 'PURGE', 'QUERY', 'RANGE', 'RECORDREADER', 'RECORDWRITER', 'RECOVER', 'REDUCE', 'REFERENCES', 'REFRESH', 'RENAME', 'REPAIR', 'REPLACE', 'RESET', 'RESPECT', 'RESTRICT', 'REVOKE', 'RIGHT', RLIKE, 'ROLE', 'ROLES', 'ROLLBACK', 'ROLLUP', 'ROW', 'ROWS', 'SECOND', 'SCHEMA', 'SELECT', 'SEMI', 'SEPARATED', 'SERDE', 'SERDEPROPERTIES', 'SESSION_USER', 'SET', 'MINUS', 'SETS', 'SHOW', 'SKEWED', 'SOME', 'SORT', 'SORTED', 'START', 'STATISTICS', 'STORED', 'STRATIFY', 'STRUCT', 'SUBSTR', 'SUBSTRING', 'SYNC', 'TABLE', 'TABLES', 'TABLESAMPLE', 'TBLPROPERTIES', TEMPORARY, 'TERMINATED', 'THEN', 'TIME', 'TO', 'TOUCH', 'TRAILING', 'TRANSACTION', 'TRANSACTIONS', 'TRANSFORM', 'TRIM', 'TRUE', 'TRUNCATE', 'TRY_CAST', 'TYPE', 'UNARCHIVE', 'UNBOUNDED', 'UNCACHE', 'UNION', 'UNIQUE', 'UNKNOWN', 'UNLOCK', 'UNSET', 'UPDATE', 'USE', 'USER', 'USING', 'VALUES', 'VIEW', 'VIEWS', 'WHEN', 'WHERE', 'WINDOW', 'WITH', 'YEAR', 'ZONE', '+', '-', '*', '~', STRING, BIGINT_LITERAL, SMALLINT_LITERAL, TINYINT_LITERAL, INTEGER_VALUE, EXPONENT_VALUE, DECIMAL_VALUE, FLOAT_LITERAL, DOUBLE_LITERAL, BIGDECIMAL_LITERAL, IDENTIFIER, BACKQUOTED_IDENTIFIER}(line 1, pos 30)",
            "log_location":"(write) log_w_sql_orc (line 5056)",
            "pass":true
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: <console>:1: error: illegal start of simple expression",
            "log_location":"(write) log_w_df_avro (line 8342)",
            "pass":true
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: mismatched input '<EOF>' expecting {'(', 'ADD', 'AFTER', 'ALL', 'ALTER', 'ANALYZE', 'AND', 'ANTI', 'ANY', 'ARCHIVE', 'ARRAY', 'AS', 'ASC', 'AT', 'AUTHORIZATION', 'BETWEEN', 'BOTH', 'BUCKET', 'BUCKETS', 'BY', 'CACHE', 'CASCADE', 'CASE', 'CAST', 'CHANGE', 'CHECK', 'CLEAR', 'CLUSTER', 'CLUSTERED', 'CODEGEN', 'COLLATE', 'COLLECTION', 'COLUMN', 'COLUMNS', 'COMMENT', 'COMMIT', 'COMPACT', 'COMPACTIONS', 'COMPUTE', 'CONCATENATE', 'CONSTRAINT', 'COST', 'CREATE', 'CROSS', 'CUBE', 'CURRENT', 'CURRENT_DATE', 'CURRENT_TIME', 'CURRENT_TIMESTAMP', 'CURRENT_USER', 'DAY', 'DATA', 'DATABASE', DATABASES, 'DBPROPERTIES', 'DEFINED', 'DELETE', 'DELIMITED', 'DESC', 'DESCRIBE', 'DFS', 'DIRECTORIES', 'DIRECTORY', 'DISTINCT', 'DISTRIBUTE', 'DIV', 'DROP', 'ELSE', 'END', 'ESCAPE', 'ESCAPED', 'EXCEPT', 'EXCHANGE', 'EXISTS', 'EXPLAIN', 'EXPORT', 'EXTENDED', 'EXTERNAL', 'EXTRACT', 'FALSE', 'FETCH', 'FIELDS', 'FILTER', 'FILEFORMAT', 'FIRST', 'FOLLOWING', 'FOR', 'FOREIGN', 'FORMAT', 'FORMATTED', 'FROM', 'FULL', 'FUNCTION', 'FUNCTIONS', 'GLOBAL', 'GRANT', 'GROUP', 'GROUPING', 'HAVING', 'HOUR', 'IF', 'IGNORE', 'IMPORT', 'IN', 'INDEX', 'INDEXES', 'INNER', 'INPATH', 'INPUTFORMAT', 'INSERT', 'INTERSECT', 'INTERVAL', 'INTO', 'IS', 'ITEMS', 'JOIN', 'KEYS', 'LAST', 'LATERAL', 'LAZY', 'LEADING', 'LEFT', 'LIKE', 'LIMIT', 'LINES', 'LIST', 'LOAD', 'LOCAL', 'LOCATION', 'LOCK', 'LOCKS', 'LOGICAL', 'MACRO', 'MAP', 'MATCHED', 'MERGE', 'MINUTE', 'MONTH', 'MSCK', 'NAMESPACE', 'NAMESPACES', 'NATURAL', 'NO', NOT, 'NULL', 'NULLS', 'OF', 'ON', 'ONLY', 'OPTION', 'OPTIONS', 'OR', 'ORDER', 'OUT', 'OUTER', 'OUTPUTFORMAT', 'OVER', 'OVERLAPS', 'OVERLAY', 'OVERWRITE', 'PARTITION', 'PARTITIONED', 'PARTITIONS', 'PERCENT', 'PIVOT', 'PLACING', 'POSITION', 'PRECEDING', 'PRIMARY', 'PRINCIPALS', 'PROPERTIES', 'PURGE', 'QUERY', 'RANGE', 'RECORDREADER', 'RECORDWRITER', 'RECOVER', 'REDUCE', 'REFERENCES', 'REFRESH', 'RENAME', 'REPAIR', 'REPLACE', 'RESET', 'RESPECT', 'RESTRICT', 'REVOKE', 'RIGHT', RLIKE, 'ROLE', 'ROLES', 'ROLLBACK', 'ROLLUP', 'ROW', 'ROWS', 'SECOND', 'SCHEMA', 'SELECT', 'SEMI', 'SEPARATED', 'SERDE', 'SERDEPROPERTIES', 'SESSION_USER', 'SET', 'MINUS', 'SETS', 'SHOW', 'SKEWED', 'SOME', 'SORT', 'SORTED', 'START', 'STATISTICS', 'STORED', 'STRATIFY', 'STRUCT', 'SUBSTR', 'SUBSTRING', 'SYNC', 'TABLE', 'TABLES', 'TABLESAMPLE', 'TBLPROPERTIES', TEMPORARY, 'TERMINATED', 'THEN', 'TIME', 'TO', 'TOUCH', 'TRAILING', 'TRANSACTION', 'TRANSACTIONS', 'TRANSFORM', 'TRIM', 'TRUE', 'TRUNCATE', 'TRY_CAST', 'TYPE', 'UNARCHIVE', 'UNBOUNDED', 'UNCACHE', 'UNION', 'UNIQUE', 'UNKNOWN', 'UNLOCK', 'UNSET', 'UPDATE', 'USE', 'USER', 'USING', 'VALUES', 'VIEW', 'VIEWS', 'WHEN', 'WHERE', 'WINDOW', 'WITH', 'YEAR', 'ZONE', '+', '-', '*', '~', STRING, BIGINT_LITERAL, SMALLINT_LITERAL, TINYINT_LITERAL, INTEGER_VALUE, EXPONENT_VALUE, DECIMAL_VALUE, FLOAT_LITERAL, DOUBLE_LITERAL, BIGDECIMAL_LITERAL, IDENTIFIER, BACKQUOTED_IDENTIFIER}(line 1, pos 30)",
            "log_location":"(write) log_w_sql_avro (line 4174)",
            "pass":true
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: <console>:1: error: illegal start of simple expression",
            "log_location":"(write) log_w_df_parquet (line 15937)",
            "pass":true
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: mismatched input '<EOF>' expecting {'(', 'ADD', 'AFTER', 'ALL', 'ALTER', 'ANALYZE', 'AND', 'ANTI', 'ANY', 'ARCHIVE', 'ARRAY', 'AS', 'ASC', 'AT', 'AUTHORIZATION', 'BETWEEN', 'BOTH', 'BUCKET', 'BUCKETS', 'BY', 'CACHE', 'CASCADE', 'CASE', 'CAST', 'CHANGE', 'CHECK', 'CLEAR', 'CLUSTER', 'CLUSTERED', 'CODEGEN', 'COLLATE', 'COLLECTION', 'COLUMN', 'COLUMNS', 'COMMENT', 'COMMIT', 'COMPACT', 'COMPACTIONS', 'COMPUTE', 'CONCATENATE', 'CONSTRAINT', 'COST', 'CREATE', 'CROSS', 'CUBE', 'CURRENT', 'CURRENT_DATE', 'CURRENT_TIME', 'CURRENT_TIMESTAMP', 'CURRENT_USER', 'DAY', 'DATA', 'DATABASE', DATABASES, 'DBPROPERTIES', 'DEFINED', 'DELETE', 'DELIMITED', 'DESC', 'DESCRIBE', 'DFS', 'DIRECTORIES', 'DIRECTORY', 'DISTINCT', 'DISTRIBUTE', 'DIV', 'DROP', 'ELSE', 'END', 'ESCAPE', 'ESCAPED', 'EXCEPT', 'EXCHANGE', 'EXISTS', 'EXPLAIN', 'EXPORT', 'EXTENDED', 'EXTERNAL', 'EXTRACT', 'FALSE', 'FETCH', 'FIELDS', 'FILTER', 'FILEFORMAT', 'FIRST', 'FOLLOWING', 'FOR', 'FOREIGN', 'FORMAT', 'FORMATTED', 'FROM', 'FULL', 'FUNCTION', 'FUNCTIONS', 'GLOBAL', 'GRANT', 'GROUP', 'GROUPING', 'HAVING', 'HOUR', 'IF', 'IGNORE', 'IMPORT', 'IN', 'INDEX', 'INDEXES', 'INNER', 'INPATH', 'INPUTFORMAT', 'INSERT', 'INTERSECT', 'INTERVAL', 'INTO', 'IS', 'ITEMS', 'JOIN', 'KEYS', 'LAST', 'LATERAL', 'LAZY', 'LEADING', 'LEFT', 'LIKE', 'LIMIT', 'LINES', 'LIST', 'LOAD', 'LOCAL', 'LOCATION', 'LOCK', 'LOCKS', 'LOGICAL', 'MACRO', 'MAP', 'MATCHED', 'MERGE', 'MINUTE', 'MONTH', 'MSCK', 'NAMESPACE', 'NAMESPACES', 'NATURAL', 'NO', NOT, 'NULL', 'NULLS', 'OF', 'ON', 'ONLY', 'OPTION', 'OPTIONS', 'OR', 'ORDER', 'OUT', 'OUTER', 'OUTPUTFORMAT', 'OVER', 'OVERLAPS', 'OVERLAY', 'OVERWRITE', 'PARTITION', 'PARTITIONED', 'PARTITIONS', 'PERCENT', 'PIVOT', 'PLACING', 'POSITION', 'PRECEDING', 'PRIMARY', 'PRINCIPALS', 'PROPERTIES', 'PURGE', 'QUERY', 'RANGE', 'RECORDREADER', 'RECORDWRITER', 'RECOVER', 'REDUCE', 'REFERENCES', 'REFRESH', 'RENAME', 'REPAIR', 'REPLACE', 'RESET', 'RESPECT', 'RESTRICT', 'REVOKE', 'RIGHT', RLIKE, 'ROLE', 'ROLES', 'ROLLBACK', 'ROLLUP', 'ROW', 'ROWS', 'SECOND', 'SCHEMA', 'SELECT', 'SEMI', 'SEPARATED', 'SERDE', 'SERDEPROPERTIES', 'SESSION_USER', 'SET', 'MINUS', 'SETS', 'SHOW', 'SKEWED', 'SOME', 'SORT', 'SORTED', 'START', 'STATISTICS', 'STORED', 'STRATIFY', 'STRUCT', 'SUBSTR', 'SUBSTRING', 'SYNC', 'TABLE', 'TABLES', 'TABLESAMPLE', 'TBLPROPERTIES', TEMPORARY, 'TERMINATED', 'THEN', 'TIME', 'TO', 'TOUCH', 'TRAILING', 'TRANSACTION', 'TRANSACTIONS', 'TRANSFORM', 'TRIM', 'TRUE', 'TRUNCATE', 'TRY_CAST', 'TYPE', 'UNARCHIVE', 'UNBOUNDED', 'UNCACHE', 'UNION', 'UNIQUE', 'UNKNOWN', 'UNLOCK', 'UNSET', 'UPDATE', 'USE', 'USER', 'USING', 'VALUES', 'VIEW', 'VIEWS', 'WHEN', 'WHERE', 'WINDOW', 'WITH', 'YEAR', 'ZONE', '+', '-', '*', '~', STRING, BIGINT_LITERAL, SMALLINT_LITERAL, TINYINT_LITERAL, INTEGER_VALUE, EXPONENT_VALUE, DECIMAL_VALUE, FLOAT_LITERAL, DOUBLE_LITERAL, BIGDECIMAL_LITERAL, IDENTIFIER, BACKQUOTED_IDENTIFIER}(line 1, pos 30)",
            "log_location":"(write) log_w_sql_parquet (line 5057)",
            "pass":true
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws122; line 1 pos 14;",
            "write_value":"No output, find exception: <console>:1: error: illegal start of simple expression",
            "log_location":"(read) log_w_df_r_df_parquet (line 2862)",
            "pass":true
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: mismatched input '<EOF>' expecting {'(', 'ADD', 'AFTER', 'ALL', 'ALTER', 'ANALYZE', 'AND', 'ANTI', 'ANY', 'ARCHIVE', 'ARRAY', 'AS', 'ASC', 'AT', 'AUTHORIZATION', 'BETWEEN', 'BOTH', 'BUCKET', 'BUCKETS', 'BY', 'CACHE', 'CASCADE', 'CASE', 'CAST', 'CHANGE', 'CHECK', 'CLEAR', 'CLUSTER', 'CLUSTERED', 'CODEGEN', 'COLLATE', 'COLLECTION', 'COLUMN', 'COLUMNS', 'COMMENT', 'COMMIT', 'COMPACT', 'COMPACTIONS', 'COMPUTE', 'CONCATENATE', 'CONSTRAINT', 'COST', 'CREATE', 'CROSS', 'CUBE', 'CURRENT', 'CURRENT_DATE', 'CURRENT_TIME', 'CURRENT_TIMESTAMP', 'CURRENT_USER', 'DAY', 'DATA', 'DATABASE', DATABASES, 'DBPROPERTIES', 'DEFINED', 'DELETE', 'DELIMITED', 'DESC', 'DESCRIBE', 'DFS', 'DIRECTORIES', 'DIRECTORY', 'DISTINCT', 'DISTRIBUTE', 'DIV', 'DROP', 'ELSE', 'END', 'ESCAPE', 'ESCAPED', 'EXCEPT', 'EXCHANGE', 'EXISTS', 'EXPLAIN', 'EXPORT', 'EXTENDED', 'EXTERNAL', 'EXTRACT', 'FALSE', 'FETCH', 'FIELDS', 'FILTER', 'FILEFORMAT', 'FIRST', 'FOLLOWING', 'FOR', 'FOREIGN', 'FORMAT', 'FORMATTED', 'FROM', 'FULL', 'FUNCTION', 'FUNCTIONS', 'GLOBAL', 'GRANT', 'GROUP', 'GROUPING', 'HAVING', 'HOUR', 'IF', 'IGNORE', 'IMPORT', 'IN', 'INDEX', 'INDEXES', 'INNER', 'INPATH', 'INPUTFORMAT', 'INSERT', 'INTERSECT', 'INTERVAL', 'INTO', 'IS', 'ITEMS', 'JOIN', 'KEYS', 'LAST', 'LATERAL', 'LAZY', 'LEADING', 'LEFT', 'LIKE', 'LIMIT', 'LINES', 'LIST', 'LOAD', 'LOCAL', 'LOCATION', 'LOCK', 'LOCKS', 'LOGICAL', 'MACRO', 'MAP', 'MATCHED', 'MERGE', 'MINUTE', 'MONTH', 'MSCK', 'NAMESPACE', 'NAMESPACES', 'NATURAL', 'NO', NOT, 'NULL', 'NULLS', 'OF', 'ON', 'ONLY', 'OPTION', 'OPTIONS', 'OR', 'ORDER', 'OUT', 'OUTER', 'OUTPUTFORMAT', 'OVER', 'OVERLAPS', 'OVERLAY', 'OVERWRITE', 'PARTITION', 'PARTITIONED', 'PARTITIONS', 'PERCENT', 'PIVOT', 'PLACING', 'POSITION', 'PRECEDING', 'PRIMARY', 'PRINCIPALS', 'PROPERTIES', 'PURGE', 'QUERY', 'RANGE', 'RECORDREADER', 'RECORDWRITER', 'RECOVER', 'REDUCE', 'REFERENCES', 'REFRESH', 'RENAME', 'REPAIR', 'REPLACE', 'RESET', 'RESPECT', 'RESTRICT', 'REVOKE', 'RIGHT', RLIKE, 'ROLE', 'ROLES', 'ROLLBACK', 'ROLLUP', 'ROW', 'ROWS', 'SECOND', 'SCHEMA', 'SELECT', 'SEMI', 'SEPARATED', 'SERDE', 'SERDEPROPERTIES', 'SESSION_USER', 'SET', 'MINUS', 'SETS', 'SHOW', 'SKEWED', 'SOME', 'SORT', 'SORTED', 'START', 'STATISTICS', 'STORED', 'STRATIFY', 'STRUCT', 'SUBSTR', 'SUBSTRING', 'SYNC', 'TABLE', 'TABLES', 'TABLESAMPLE', 'TBLPROPERTIES', TEMPORARY, 'TERMINATED', 'THEN', 'TIME', 'TO', 'TOUCH', 'TRAILING', 'TRANSACTION', 'TRANSACTIONS', 'TRANSFORM', 'TRIM', 'TRUE', 'TRUNCATE', 'TRY_CAST', 'TYPE', 'UNARCHIVE', 'UNBOUNDED', 'UNCACHE', 'UNION', 'UNIQUE', 'UNKNOWN', 'UNLOCK', 'UNSET', 'UPDATE', 'USE', 'USER', 'USING', 'VALUES', 'VIEW', 'VIEWS', 'WHEN', 'WHERE', 'WINDOW', 'WITH', 'YEAR', 'ZONE', '+', '-', '*', '~', STRING, BIGINT_LITERAL, SMALLINT_LITERAL, TINYINT_LITERAL, INTEGER_VALUE, EXPONENT_VALUE, DECIMAL_VALUE, FLOAT_LITERAL, DOUBLE_LITERAL, BIGDECIMAL_LITERAL, IDENTIFIER, BACKQUOTED_IDENTIFIER}(line 1, pos 30)",
            "log_location":"(write) log_w_sql_orc (line 5056)",
            "pass":true
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: mismatched input '<EOF>' expecting {'(', 'ADD', 'AFTER', 'ALL', 'ALTER', 'ANALYZE', 'AND', 'ANTI', 'ANY', 'ARCHIVE', 'ARRAY', 'AS', 'ASC', 'AT', 'AUTHORIZATION', 'BETWEEN', 'BOTH', 'BUCKET', 'BUCKETS', 'BY', 'CACHE', 'CASCADE', 'CASE', 'CAST', 'CHANGE', 'CHECK', 'CLEAR', 'CLUSTER', 'CLUSTERED', 'CODEGEN', 'COLLATE', 'COLLECTION', 'COLUMN', 'COLUMNS', 'COMMENT', 'COMMIT', 'COMPACT', 'COMPACTIONS', 'COMPUTE', 'CONCATENATE', 'CONSTRAINT', 'COST', 'CREATE', 'CROSS', 'CUBE', 'CURRENT', 'CURRENT_DATE', 'CURRENT_TIME', 'CURRENT_TIMESTAMP', 'CURRENT_USER', 'DAY', 'DATA', 'DATABASE', DATABASES, 'DBPROPERTIES', 'DEFINED', 'DELETE', 'DELIMITED', 'DESC', 'DESCRIBE', 'DFS', 'DIRECTORIES', 'DIRECTORY', 'DISTINCT', 'DISTRIBUTE', 'DIV', 'DROP', 'ELSE', 'END', 'ESCAPE', 'ESCAPED', 'EXCEPT', 'EXCHANGE', 'EXISTS', 'EXPLAIN', 'EXPORT', 'EXTENDED', 'EXTERNAL', 'EXTRACT', 'FALSE', 'FETCH', 'FIELDS', 'FILTER', 'FILEFORMAT', 'FIRST', 'FOLLOWING', 'FOR', 'FOREIGN', 'FORMAT', 'FORMATTED', 'FROM', 'FULL', 'FUNCTION', 'FUNCTIONS', 'GLOBAL', 'GRANT', 'GROUP', 'GROUPING', 'HAVING', 'HOUR', 'IF', 'IGNORE', 'IMPORT', 'IN', 'INDEX', 'INDEXES', 'INNER', 'INPATH', 'INPUTFORMAT', 'INSERT', 'INTERSECT', 'INTERVAL', 'INTO', 'IS', 'ITEMS', 'JOIN', 'KEYS', 'LAST', 'LATERAL', 'LAZY', 'LEADING', 'LEFT', 'LIKE', 'LIMIT', 'LINES', 'LIST', 'LOAD', 'LOCAL', 'LOCATION', 'LOCK', 'LOCKS', 'LOGICAL', 'MACRO', 'MAP', 'MATCHED', 'MERGE', 'MINUTE', 'MONTH', 'MSCK', 'NAMESPACE', 'NAMESPACES', 'NATURAL', 'NO', NOT, 'NULL', 'NULLS', 'OF', 'ON', 'ONLY', 'OPTION', 'OPTIONS', 'OR', 'ORDER', 'OUT', 'OUTER', 'OUTPUTFORMAT', 'OVER', 'OVERLAPS', 'OVERLAY', 'OVERWRITE', 'PARTITION', 'PARTITIONED', 'PARTITIONS', 'PERCENT', 'PIVOT', 'PLACING', 'POSITION', 'PRECEDING', 'PRIMARY', 'PRINCIPALS', 'PROPERTIES', 'PURGE', 'QUERY', 'RANGE', 'RECORDREADER', 'RECORDWRITER', 'RECOVER', 'REDUCE', 'REFERENCES', 'REFRESH', 'RENAME', 'REPAIR', 'REPLACE', 'RESET', 'RESPECT', 'RESTRICT', 'REVOKE', 'RIGHT', RLIKE, 'ROLE', 'ROLES', 'ROLLBACK', 'ROLLUP', 'ROW', 'ROWS', 'SECOND', 'SCHEMA', 'SELECT', 'SEMI', 'SEPARATED', 'SERDE', 'SERDEPROPERTIES', 'SESSION_USER', 'SET', 'MINUS', 'SETS', 'SHOW', 'SKEWED', 'SOME', 'SORT', 'SORTED', 'START', 'STATISTICS', 'STORED', 'STRATIFY', 'STRUCT', 'SUBSTR', 'SUBSTRING', 'SYNC', 'TABLE', 'TABLES', 'TABLESAMPLE', 'TBLPROPERTIES', TEMPORARY, 'TERMINATED', 'THEN', 'TIME', 'TO', 'TOUCH', 'TRAILING', 'TRANSACTION', 'TRANSACTIONS', 'TRANSFORM', 'TRIM', 'TRUE', 'TRUNCATE', 'TRY_CAST', 'TYPE', 'UNARCHIVE', 'UNBOUNDED', 'UNCACHE', 'UNION', 'UNIQUE', 'UNKNOWN', 'UNLOCK', 'UNSET', 'UPDATE', 'USE', 'USER', 'USING', 'VALUES', 'VIEW', 'VIEWS', 'WHEN', 'WHERE', 'WINDOW', 'WITH', 'YEAR', 'ZONE', '+', '-', '*', '~', STRING, BIGINT_LITERAL, SMALLINT_LITERAL, TINYINT_LITERAL, INTEGER_VALUE, EXPONENT_VALUE, DECIMAL_VALUE, FLOAT_LITERAL, DOUBLE_LITERAL, BIGDECIMAL_LITERAL, IDENTIFIER, BACKQUOTED_IDENTIFIER}(line 1, pos 30)",
            "log_location":"(write) log_w_sql_parquet (line 5057)",
            "pass":true
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws122; line 1 pos 14;",
            "write_value":"No output, find exception: <console>:1: error: illegal start of simple expression",
            "log_location":"(read) log_w_df_r_df_avro (line 4550)",
            "pass":true
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: <console>:1: error: illegal start of simple expression",
            "log_location":"(write) log_w_df_orc (line 15934)",
            "pass":true
        }
    },
    "123":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Literals of type 'B' are currently not supported.(line 1, pos 30)",
            "log_location":"(write) log_w_sql_avro (line 4194)",
            "pass":true
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws123; line 1 pos 14;",
            "write_value":"No output, find exception: <console>:1: error: empty character literal",
            "log_location":"(read) log_w_df_r_df_orc (line 2900)",
            "pass":true
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Literals of type 'B' are currently not supported.(line 1, pos 30)",
            "log_location":"(write) log_w_sql_orc (line 5075)",
            "pass":true
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: <console>:1: error: empty character literal",
            "log_location":"(write) log_w_df_avro (line 8365)",
            "pass":true
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Literals of type 'B' are currently not supported.(line 1, pos 30)",
            "log_location":"(write) log_w_sql_avro (line 4194)",
            "pass":true
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: <console>:1: error: empty character literal",
            "log_location":"(write) log_w_df_parquet (line 15960)",
            "pass":true
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Literals of type 'B' are currently not supported.(line 1, pos 30)",
            "log_location":"(write) log_w_sql_parquet (line 5076)",
            "pass":true
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws123; line 1 pos 14;",
            "write_value":"No output, find exception: <console>:1: error: empty character literal",
            "log_location":"(read) log_w_df_r_df_parquet (line 2902)",
            "pass":true
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Literals of type 'B' are currently not supported.(line 1, pos 30)",
            "log_location":"(write) log_w_sql_orc (line 5075)",
            "pass":true
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Literals of type 'B' are currently not supported.(line 1, pos 30)",
            "log_location":"(write) log_w_sql_parquet (line 5076)",
            "pass":true
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws123; line 1 pos 14;",
            "write_value":"No output, find exception: <console>:1: error: empty character literal",
            "log_location":"(read) log_w_df_r_df_avro (line 4590)",
            "pass":true
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: <console>:1: error: empty character literal",
            "log_location":"(write) log_w_df_orc (line 15957)",
            "pass":true
        }
    },
    "126":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: 'InsertIntoStatement 'UnresolvedRelation [ws126], [], false, false, false",
            "log_location":"(write) log_w_sql_avro (line 4229)",
            "pass":true
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws126; line 1 pos 14;",
            "write_value":"No output, find exception: <console>:28: error: not found: value tf",
            "log_location":"(read) log_w_df_r_df_orc (line 2956)",
            "pass":true
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: 'InsertIntoStatement 'UnresolvedRelation [ws126], [], false, false, false",
            "log_location":"(write) log_w_sql_orc (line 5107)",
            "pass":true
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: <console>:28: error: not found: value tf",
            "log_location":"(write) log_w_df_avro (line 8426)",
            "pass":true
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: 'InsertIntoStatement 'UnresolvedRelation [ws126], [], false, false, false",
            "log_location":"(write) log_w_sql_avro (line 4229)",
            "pass":true
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: <console>:28: error: not found: value tf",
            "log_location":"(write) log_w_df_parquet (line 16021)",
            "pass":true
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: 'InsertIntoStatement 'UnresolvedRelation [ws126], [], false, false, false",
            "log_location":"(write) log_w_sql_parquet (line 5108)",
            "pass":true
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws126; line 1 pos 14;",
            "write_value":"No output, find exception: <console>:28: error: not found: value tf",
            "log_location":"(read) log_w_df_r_df_parquet (line 2958)",
            "pass":true
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: 'InsertIntoStatement 'UnresolvedRelation [ws126], [], false, false, false",
            "log_location":"(write) log_w_sql_orc (line 5107)",
            "pass":true
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: 'InsertIntoStatement 'UnresolvedRelation [ws126], [], false, false, false",
            "log_location":"(write) log_w_sql_parquet (line 5108)",
            "pass":true
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws126; line 1 pos 14;",
            "write_value":"No output, find exception: <console>:28: error: not found: value tf",
            "log_location":"(read) log_w_df_r_df_avro (line 4646)",
            "pass":true
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: <console>:28: error: not found: value tf",
            "log_location":"(write) log_w_df_orc (line 16018)",
            "pass":true
        }
    },
    "127":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: mismatched input '<EOF>' expecting {'(', 'ADD', 'AFTER', 'ALL', 'ALTER', 'ANALYZE', 'AND', 'ANTI', 'ANY', 'ARCHIVE', 'ARRAY', 'AS', 'ASC', 'AT', 'AUTHORIZATION', 'BETWEEN', 'BOTH', 'BUCKET', 'BUCKETS', 'BY', 'CACHE', 'CASCADE', 'CASE', 'CAST', 'CHANGE', 'CHECK', 'CLEAR', 'CLUSTER', 'CLUSTERED', 'CODEGEN', 'COLLATE', 'COLLECTION', 'COLUMN', 'COLUMNS', 'COMMENT', 'COMMIT', 'COMPACT', 'COMPACTIONS', 'COMPUTE', 'CONCATENATE', 'CONSTRAINT', 'COST', 'CREATE', 'CROSS', 'CUBE', 'CURRENT', 'CURRENT_DATE', 'CURRENT_TIME', 'CURRENT_TIMESTAMP', 'CURRENT_USER', 'DAY', 'DATA', 'DATABASE', DATABASES, 'DBPROPERTIES', 'DEFINED', 'DELETE', 'DELIMITED', 'DESC', 'DESCRIBE', 'DFS', 'DIRECTORIES', 'DIRECTORY', 'DISTINCT', 'DISTRIBUTE', 'DIV', 'DROP', 'ELSE', 'END', 'ESCAPE', 'ESCAPED', 'EXCEPT', 'EXCHANGE', 'EXISTS', 'EXPLAIN', 'EXPORT', 'EXTENDED', 'EXTERNAL', 'EXTRACT', 'FALSE', 'FETCH', 'FIELDS', 'FILTER', 'FILEFORMAT', 'FIRST', 'FOLLOWING', 'FOR', 'FOREIGN', 'FORMAT', 'FORMATTED', 'FROM', 'FULL', 'FUNCTION', 'FUNCTIONS', 'GLOBAL', 'GRANT', 'GROUP', 'GROUPING', 'HAVING', 'HOUR', 'IF', 'IGNORE', 'IMPORT', 'IN', 'INDEX', 'INDEXES', 'INNER', 'INPATH', 'INPUTFORMAT', 'INSERT', 'INTERSECT', 'INTERVAL', 'INTO', 'IS', 'ITEMS', 'JOIN', 'KEYS', 'LAST', 'LATERAL', 'LAZY', 'LEADING', 'LEFT', 'LIKE', 'LIMIT', 'LINES', 'LIST', 'LOAD', 'LOCAL', 'LOCATION', 'LOCK', 'LOCKS', 'LOGICAL', 'MACRO', 'MAP', 'MATCHED', 'MERGE', 'MINUTE', 'MONTH', 'MSCK', 'NAMESPACE', 'NAMESPACES', 'NATURAL', 'NO', NOT, 'NULL', 'NULLS', 'OF', 'ON', 'ONLY', 'OPTION', 'OPTIONS', 'OR', 'ORDER', 'OUT', 'OUTER', 'OUTPUTFORMAT', 'OVER', 'OVERLAPS', 'OVERLAY', 'OVERWRITE', 'PARTITION', 'PARTITIONED', 'PARTITIONS', 'PERCENT', 'PIVOT', 'PLACING', 'POSITION', 'PRECEDING', 'PRIMARY', 'PRINCIPALS', 'PROPERTIES', 'PURGE', 'QUERY', 'RANGE', 'RECORDREADER', 'RECORDWRITER', 'RECOVER', 'REDUCE', 'REFERENCES', 'REFRESH', 'RENAME', 'REPAIR', 'REPLACE', 'RESET', 'RESPECT', 'RESTRICT', 'REVOKE', 'RIGHT', RLIKE, 'ROLE', 'ROLES', 'ROLLBACK', 'ROLLUP', 'ROW', 'ROWS', 'SECOND', 'SCHEMA', 'SELECT', 'SEMI', 'SEPARATED', 'SERDE', 'SERDEPROPERTIES', 'SESSION_USER', 'SET', 'MINUS', 'SETS', 'SHOW', 'SKEWED', 'SOME', 'SORT', 'SORTED', 'START', 'STATISTICS', 'STORED', 'STRATIFY', 'STRUCT', 'SUBSTR', 'SUBSTRING', 'SYNC', 'TABLE', 'TABLES', 'TABLESAMPLE', 'TBLPROPERTIES', TEMPORARY, 'TERMINATED', 'THEN', 'TIME', 'TO', 'TOUCH', 'TRAILING', 'TRANSACTION', 'TRANSACTIONS', 'TRANSFORM', 'TRIM', 'TRUE', 'TRUNCATE', 'TRY_CAST', 'TYPE', 'UNARCHIVE', 'UNBOUNDED', 'UNCACHE', 'UNION', 'UNIQUE', 'UNKNOWN', 'UNLOCK', 'UNSET', 'UPDATE', 'USE', 'USER', 'USING', 'VALUES', 'VIEW', 'VIEWS', 'WHEN', 'WHERE', 'WINDOW', 'WITH', 'YEAR', 'ZONE', '+', '-', '*', '~', STRING, BIGINT_LITERAL, SMALLINT_LITERAL, TINYINT_LITERAL, INTEGER_VALUE, EXPONENT_VALUE, DECIMAL_VALUE, FLOAT_LITERAL, DOUBLE_LITERAL, BIGDECIMAL_LITERAL, IDENTIFIER, BACKQUOTED_IDENTIFIER}(line 1, pos 30)",
            "log_location":"(write) log_w_sql_avro (line 4247)",
            "pass":true
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws127; line 1 pos 14;",
            "write_value":"No output, find exception: <console>:1: error: illegal start of simple expression",
            "log_location":"(read) log_w_df_r_df_orc (line 2996)",
            "pass":true
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: mismatched input '<EOF>' expecting {'(', 'ADD', 'AFTER', 'ALL', 'ALTER', 'ANALYZE', 'AND', 'ANTI', 'ANY', 'ARCHIVE', 'ARRAY', 'AS', 'ASC', 'AT', 'AUTHORIZATION', 'BETWEEN', 'BOTH', 'BUCKET', 'BUCKETS', 'BY', 'CACHE', 'CASCADE', 'CASE', 'CAST', 'CHANGE', 'CHECK', 'CLEAR', 'CLUSTER', 'CLUSTERED', 'CODEGEN', 'COLLATE', 'COLLECTION', 'COLUMN', 'COLUMNS', 'COMMENT', 'COMMIT', 'COMPACT', 'COMPACTIONS', 'COMPUTE', 'CONCATENATE', 'CONSTRAINT', 'COST', 'CREATE', 'CROSS', 'CUBE', 'CURRENT', 'CURRENT_DATE', 'CURRENT_TIME', 'CURRENT_TIMESTAMP', 'CURRENT_USER', 'DAY', 'DATA', 'DATABASE', DATABASES, 'DBPROPERTIES', 'DEFINED', 'DELETE', 'DELIMITED', 'DESC', 'DESCRIBE', 'DFS', 'DIRECTORIES', 'DIRECTORY', 'DISTINCT', 'DISTRIBUTE', 'DIV', 'DROP', 'ELSE', 'END', 'ESCAPE', 'ESCAPED', 'EXCEPT', 'EXCHANGE', 'EXISTS', 'EXPLAIN', 'EXPORT', 'EXTENDED', 'EXTERNAL', 'EXTRACT', 'FALSE', 'FETCH', 'FIELDS', 'FILTER', 'FILEFORMAT', 'FIRST', 'FOLLOWING', 'FOR', 'FOREIGN', 'FORMAT', 'FORMATTED', 'FROM', 'FULL', 'FUNCTION', 'FUNCTIONS', 'GLOBAL', 'GRANT', 'GROUP', 'GROUPING', 'HAVING', 'HOUR', 'IF', 'IGNORE', 'IMPORT', 'IN', 'INDEX', 'INDEXES', 'INNER', 'INPATH', 'INPUTFORMAT', 'INSERT', 'INTERSECT', 'INTERVAL', 'INTO', 'IS', 'ITEMS', 'JOIN', 'KEYS', 'LAST', 'LATERAL', 'LAZY', 'LEADING', 'LEFT', 'LIKE', 'LIMIT', 'LINES', 'LIST', 'LOAD', 'LOCAL', 'LOCATION', 'LOCK', 'LOCKS', 'LOGICAL', 'MACRO', 'MAP', 'MATCHED', 'MERGE', 'MINUTE', 'MONTH', 'MSCK', 'NAMESPACE', 'NAMESPACES', 'NATURAL', 'NO', NOT, 'NULL', 'NULLS', 'OF', 'ON', 'ONLY', 'OPTION', 'OPTIONS', 'OR', 'ORDER', 'OUT', 'OUTER', 'OUTPUTFORMAT', 'OVER', 'OVERLAPS', 'OVERLAY', 'OVERWRITE', 'PARTITION', 'PARTITIONED', 'PARTITIONS', 'PERCENT', 'PIVOT', 'PLACING', 'POSITION', 'PRECEDING', 'PRIMARY', 'PRINCIPALS', 'PROPERTIES', 'PURGE', 'QUERY', 'RANGE', 'RECORDREADER', 'RECORDWRITER', 'RECOVER', 'REDUCE', 'REFERENCES', 'REFRESH', 'RENAME', 'REPAIR', 'REPLACE', 'RESET', 'RESPECT', 'RESTRICT', 'REVOKE', 'RIGHT', RLIKE, 'ROLE', 'ROLES', 'ROLLBACK', 'ROLLUP', 'ROW', 'ROWS', 'SECOND', 'SCHEMA', 'SELECT', 'SEMI', 'SEPARATED', 'SERDE', 'SERDEPROPERTIES', 'SESSION_USER', 'SET', 'MINUS', 'SETS', 'SHOW', 'SKEWED', 'SOME', 'SORT', 'SORTED', 'START', 'STATISTICS', 'STORED', 'STRATIFY', 'STRUCT', 'SUBSTR', 'SUBSTRING', 'SYNC', 'TABLE', 'TABLES', 'TABLESAMPLE', 'TBLPROPERTIES', TEMPORARY, 'TERMINATED', 'THEN', 'TIME', 'TO', 'TOUCH', 'TRAILING', 'TRANSACTION', 'TRANSACTIONS', 'TRANSFORM', 'TRIM', 'TRUE', 'TRUNCATE', 'TRY_CAST', 'TYPE', 'UNARCHIVE', 'UNBOUNDED', 'UNCACHE', 'UNION', 'UNIQUE', 'UNKNOWN', 'UNLOCK', 'UNSET', 'UPDATE', 'USE', 'USER', 'USING', 'VALUES', 'VIEW', 'VIEWS', 'WHEN', 'WHERE', 'WINDOW', 'WITH', 'YEAR', 'ZONE', '+', '-', '*', '~', STRING, BIGINT_LITERAL, SMALLINT_LITERAL, TINYINT_LITERAL, INTEGER_VALUE, EXPONENT_VALUE, DECIMAL_VALUE, FLOAT_LITERAL, DOUBLE_LITERAL, BIGDECIMAL_LITERAL, IDENTIFIER, BACKQUOTED_IDENTIFIER}(line 1, pos 30)",
            "log_location":"(write) log_w_sql_orc (line 5124)",
            "pass":true
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: <console>:1: error: illegal start of simple expression",
            "log_location":"(write) log_w_df_avro (line 8450)",
            "pass":true
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: mismatched input '<EOF>' expecting {'(', 'ADD', 'AFTER', 'ALL', 'ALTER', 'ANALYZE', 'AND', 'ANTI', 'ANY', 'ARCHIVE', 'ARRAY', 'AS', 'ASC', 'AT', 'AUTHORIZATION', 'BETWEEN', 'BOTH', 'BUCKET', 'BUCKETS', 'BY', 'CACHE', 'CASCADE', 'CASE', 'CAST', 'CHANGE', 'CHECK', 'CLEAR', 'CLUSTER', 'CLUSTERED', 'CODEGEN', 'COLLATE', 'COLLECTION', 'COLUMN', 'COLUMNS', 'COMMENT', 'COMMIT', 'COMPACT', 'COMPACTIONS', 'COMPUTE', 'CONCATENATE', 'CONSTRAINT', 'COST', 'CREATE', 'CROSS', 'CUBE', 'CURRENT', 'CURRENT_DATE', 'CURRENT_TIME', 'CURRENT_TIMESTAMP', 'CURRENT_USER', 'DAY', 'DATA', 'DATABASE', DATABASES, 'DBPROPERTIES', 'DEFINED', 'DELETE', 'DELIMITED', 'DESC', 'DESCRIBE', 'DFS', 'DIRECTORIES', 'DIRECTORY', 'DISTINCT', 'DISTRIBUTE', 'DIV', 'DROP', 'ELSE', 'END', 'ESCAPE', 'ESCAPED', 'EXCEPT', 'EXCHANGE', 'EXISTS', 'EXPLAIN', 'EXPORT', 'EXTENDED', 'EXTERNAL', 'EXTRACT', 'FALSE', 'FETCH', 'FIELDS', 'FILTER', 'FILEFORMAT', 'FIRST', 'FOLLOWING', 'FOR', 'FOREIGN', 'FORMAT', 'FORMATTED', 'FROM', 'FULL', 'FUNCTION', 'FUNCTIONS', 'GLOBAL', 'GRANT', 'GROUP', 'GROUPING', 'HAVING', 'HOUR', 'IF', 'IGNORE', 'IMPORT', 'IN', 'INDEX', 'INDEXES', 'INNER', 'INPATH', 'INPUTFORMAT', 'INSERT', 'INTERSECT', 'INTERVAL', 'INTO', 'IS', 'ITEMS', 'JOIN', 'KEYS', 'LAST', 'LATERAL', 'LAZY', 'LEADING', 'LEFT', 'LIKE', 'LIMIT', 'LINES', 'LIST', 'LOAD', 'LOCAL', 'LOCATION', 'LOCK', 'LOCKS', 'LOGICAL', 'MACRO', 'MAP', 'MATCHED', 'MERGE', 'MINUTE', 'MONTH', 'MSCK', 'NAMESPACE', 'NAMESPACES', 'NATURAL', 'NO', NOT, 'NULL', 'NULLS', 'OF', 'ON', 'ONLY', 'OPTION', 'OPTIONS', 'OR', 'ORDER', 'OUT', 'OUTER', 'OUTPUTFORMAT', 'OVER', 'OVERLAPS', 'OVERLAY', 'OVERWRITE', 'PARTITION', 'PARTITIONED', 'PARTITIONS', 'PERCENT', 'PIVOT', 'PLACING', 'POSITION', 'PRECEDING', 'PRIMARY', 'PRINCIPALS', 'PROPERTIES', 'PURGE', 'QUERY', 'RANGE', 'RECORDREADER', 'RECORDWRITER', 'RECOVER', 'REDUCE', 'REFERENCES', 'REFRESH', 'RENAME', 'REPAIR', 'REPLACE', 'RESET', 'RESPECT', 'RESTRICT', 'REVOKE', 'RIGHT', RLIKE, 'ROLE', 'ROLES', 'ROLLBACK', 'ROLLUP', 'ROW', 'ROWS', 'SECOND', 'SCHEMA', 'SELECT', 'SEMI', 'SEPARATED', 'SERDE', 'SERDEPROPERTIES', 'SESSION_USER', 'SET', 'MINUS', 'SETS', 'SHOW', 'SKEWED', 'SOME', 'SORT', 'SORTED', 'START', 'STATISTICS', 'STORED', 'STRATIFY', 'STRUCT', 'SUBSTR', 'SUBSTRING', 'SYNC', 'TABLE', 'TABLES', 'TABLESAMPLE', 'TBLPROPERTIES', TEMPORARY, 'TERMINATED', 'THEN', 'TIME', 'TO', 'TOUCH', 'TRAILING', 'TRANSACTION', 'TRANSACTIONS', 'TRANSFORM', 'TRIM', 'TRUE', 'TRUNCATE', 'TRY_CAST', 'TYPE', 'UNARCHIVE', 'UNBOUNDED', 'UNCACHE', 'UNION', 'UNIQUE', 'UNKNOWN', 'UNLOCK', 'UNSET', 'UPDATE', 'USE', 'USER', 'USING', 'VALUES', 'VIEW', 'VIEWS', 'WHEN', 'WHERE', 'WINDOW', 'WITH', 'YEAR', 'ZONE', '+', '-', '*', '~', STRING, BIGINT_LITERAL, SMALLINT_LITERAL, TINYINT_LITERAL, INTEGER_VALUE, EXPONENT_VALUE, DECIMAL_VALUE, FLOAT_LITERAL, DOUBLE_LITERAL, BIGDECIMAL_LITERAL, IDENTIFIER, BACKQUOTED_IDENTIFIER}(line 1, pos 30)",
            "log_location":"(write) log_w_sql_avro (line 4247)",
            "pass":true
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: <console>:1: error: illegal start of simple expression",
            "log_location":"(write) log_w_df_parquet (line 16045)",
            "pass":true
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: mismatched input '<EOF>' expecting {'(', 'ADD', 'AFTER', 'ALL', 'ALTER', 'ANALYZE', 'AND', 'ANTI', 'ANY', 'ARCHIVE', 'ARRAY', 'AS', 'ASC', 'AT', 'AUTHORIZATION', 'BETWEEN', 'BOTH', 'BUCKET', 'BUCKETS', 'BY', 'CACHE', 'CASCADE', 'CASE', 'CAST', 'CHANGE', 'CHECK', 'CLEAR', 'CLUSTER', 'CLUSTERED', 'CODEGEN', 'COLLATE', 'COLLECTION', 'COLUMN', 'COLUMNS', 'COMMENT', 'COMMIT', 'COMPACT', 'COMPACTIONS', 'COMPUTE', 'CONCATENATE', 'CONSTRAINT', 'COST', 'CREATE', 'CROSS', 'CUBE', 'CURRENT', 'CURRENT_DATE', 'CURRENT_TIME', 'CURRENT_TIMESTAMP', 'CURRENT_USER', 'DAY', 'DATA', 'DATABASE', DATABASES, 'DBPROPERTIES', 'DEFINED', 'DELETE', 'DELIMITED', 'DESC', 'DESCRIBE', 'DFS', 'DIRECTORIES', 'DIRECTORY', 'DISTINCT', 'DISTRIBUTE', 'DIV', 'DROP', 'ELSE', 'END', 'ESCAPE', 'ESCAPED', 'EXCEPT', 'EXCHANGE', 'EXISTS', 'EXPLAIN', 'EXPORT', 'EXTENDED', 'EXTERNAL', 'EXTRACT', 'FALSE', 'FETCH', 'FIELDS', 'FILTER', 'FILEFORMAT', 'FIRST', 'FOLLOWING', 'FOR', 'FOREIGN', 'FORMAT', 'FORMATTED', 'FROM', 'FULL', 'FUNCTION', 'FUNCTIONS', 'GLOBAL', 'GRANT', 'GROUP', 'GROUPING', 'HAVING', 'HOUR', 'IF', 'IGNORE', 'IMPORT', 'IN', 'INDEX', 'INDEXES', 'INNER', 'INPATH', 'INPUTFORMAT', 'INSERT', 'INTERSECT', 'INTERVAL', 'INTO', 'IS', 'ITEMS', 'JOIN', 'KEYS', 'LAST', 'LATERAL', 'LAZY', 'LEADING', 'LEFT', 'LIKE', 'LIMIT', 'LINES', 'LIST', 'LOAD', 'LOCAL', 'LOCATION', 'LOCK', 'LOCKS', 'LOGICAL', 'MACRO', 'MAP', 'MATCHED', 'MERGE', 'MINUTE', 'MONTH', 'MSCK', 'NAMESPACE', 'NAMESPACES', 'NATURAL', 'NO', NOT, 'NULL', 'NULLS', 'OF', 'ON', 'ONLY', 'OPTION', 'OPTIONS', 'OR', 'ORDER', 'OUT', 'OUTER', 'OUTPUTFORMAT', 'OVER', 'OVERLAPS', 'OVERLAY', 'OVERWRITE', 'PARTITION', 'PARTITIONED', 'PARTITIONS', 'PERCENT', 'PIVOT', 'PLACING', 'POSITION', 'PRECEDING', 'PRIMARY', 'PRINCIPALS', 'PROPERTIES', 'PURGE', 'QUERY', 'RANGE', 'RECORDREADER', 'RECORDWRITER', 'RECOVER', 'REDUCE', 'REFERENCES', 'REFRESH', 'RENAME', 'REPAIR', 'REPLACE', 'RESET', 'RESPECT', 'RESTRICT', 'REVOKE', 'RIGHT', RLIKE, 'ROLE', 'ROLES', 'ROLLBACK', 'ROLLUP', 'ROW', 'ROWS', 'SECOND', 'SCHEMA', 'SELECT', 'SEMI', 'SEPARATED', 'SERDE', 'SERDEPROPERTIES', 'SESSION_USER', 'SET', 'MINUS', 'SETS', 'SHOW', 'SKEWED', 'SOME', 'SORT', 'SORTED', 'START', 'STATISTICS', 'STORED', 'STRATIFY', 'STRUCT', 'SUBSTR', 'SUBSTRING', 'SYNC', 'TABLE', 'TABLES', 'TABLESAMPLE', 'TBLPROPERTIES', TEMPORARY, 'TERMINATED', 'THEN', 'TIME', 'TO', 'TOUCH', 'TRAILING', 'TRANSACTION', 'TRANSACTIONS', 'TRANSFORM', 'TRIM', 'TRUE', 'TRUNCATE', 'TRY_CAST', 'TYPE', 'UNARCHIVE', 'UNBOUNDED', 'UNCACHE', 'UNION', 'UNIQUE', 'UNKNOWN', 'UNLOCK', 'UNSET', 'UPDATE', 'USE', 'USER', 'USING', 'VALUES', 'VIEW', 'VIEWS', 'WHEN', 'WHERE', 'WINDOW', 'WITH', 'YEAR', 'ZONE', '+', '-', '*', '~', STRING, BIGINT_LITERAL, SMALLINT_LITERAL, TINYINT_LITERAL, INTEGER_VALUE, EXPONENT_VALUE, DECIMAL_VALUE, FLOAT_LITERAL, DOUBLE_LITERAL, BIGDECIMAL_LITERAL, IDENTIFIER, BACKQUOTED_IDENTIFIER}(line 1, pos 30)",
            "log_location":"(write) log_w_sql_parquet (line 5125)",
            "pass":true
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws127; line 1 pos 14;",
            "write_value":"No output, find exception: <console>:1: error: illegal start of simple expression",
            "log_location":"(read) log_w_df_r_df_parquet (line 2998)",
            "pass":true
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: mismatched input '<EOF>' expecting {'(', 'ADD', 'AFTER', 'ALL', 'ALTER', 'ANALYZE', 'AND', 'ANTI', 'ANY', 'ARCHIVE', 'ARRAY', 'AS', 'ASC', 'AT', 'AUTHORIZATION', 'BETWEEN', 'BOTH', 'BUCKET', 'BUCKETS', 'BY', 'CACHE', 'CASCADE', 'CASE', 'CAST', 'CHANGE', 'CHECK', 'CLEAR', 'CLUSTER', 'CLUSTERED', 'CODEGEN', 'COLLATE', 'COLLECTION', 'COLUMN', 'COLUMNS', 'COMMENT', 'COMMIT', 'COMPACT', 'COMPACTIONS', 'COMPUTE', 'CONCATENATE', 'CONSTRAINT', 'COST', 'CREATE', 'CROSS', 'CUBE', 'CURRENT', 'CURRENT_DATE', 'CURRENT_TIME', 'CURRENT_TIMESTAMP', 'CURRENT_USER', 'DAY', 'DATA', 'DATABASE', DATABASES, 'DBPROPERTIES', 'DEFINED', 'DELETE', 'DELIMITED', 'DESC', 'DESCRIBE', 'DFS', 'DIRECTORIES', 'DIRECTORY', 'DISTINCT', 'DISTRIBUTE', 'DIV', 'DROP', 'ELSE', 'END', 'ESCAPE', 'ESCAPED', 'EXCEPT', 'EXCHANGE', 'EXISTS', 'EXPLAIN', 'EXPORT', 'EXTENDED', 'EXTERNAL', 'EXTRACT', 'FALSE', 'FETCH', 'FIELDS', 'FILTER', 'FILEFORMAT', 'FIRST', 'FOLLOWING', 'FOR', 'FOREIGN', 'FORMAT', 'FORMATTED', 'FROM', 'FULL', 'FUNCTION', 'FUNCTIONS', 'GLOBAL', 'GRANT', 'GROUP', 'GROUPING', 'HAVING', 'HOUR', 'IF', 'IGNORE', 'IMPORT', 'IN', 'INDEX', 'INDEXES', 'INNER', 'INPATH', 'INPUTFORMAT', 'INSERT', 'INTERSECT', 'INTERVAL', 'INTO', 'IS', 'ITEMS', 'JOIN', 'KEYS', 'LAST', 'LATERAL', 'LAZY', 'LEADING', 'LEFT', 'LIKE', 'LIMIT', 'LINES', 'LIST', 'LOAD', 'LOCAL', 'LOCATION', 'LOCK', 'LOCKS', 'LOGICAL', 'MACRO', 'MAP', 'MATCHED', 'MERGE', 'MINUTE', 'MONTH', 'MSCK', 'NAMESPACE', 'NAMESPACES', 'NATURAL', 'NO', NOT, 'NULL', 'NULLS', 'OF', 'ON', 'ONLY', 'OPTION', 'OPTIONS', 'OR', 'ORDER', 'OUT', 'OUTER', 'OUTPUTFORMAT', 'OVER', 'OVERLAPS', 'OVERLAY', 'OVERWRITE', 'PARTITION', 'PARTITIONED', 'PARTITIONS', 'PERCENT', 'PIVOT', 'PLACING', 'POSITION', 'PRECEDING', 'PRIMARY', 'PRINCIPALS', 'PROPERTIES', 'PURGE', 'QUERY', 'RANGE', 'RECORDREADER', 'RECORDWRITER', 'RECOVER', 'REDUCE', 'REFERENCES', 'REFRESH', 'RENAME', 'REPAIR', 'REPLACE', 'RESET', 'RESPECT', 'RESTRICT', 'REVOKE', 'RIGHT', RLIKE, 'ROLE', 'ROLES', 'ROLLBACK', 'ROLLUP', 'ROW', 'ROWS', 'SECOND', 'SCHEMA', 'SELECT', 'SEMI', 'SEPARATED', 'SERDE', 'SERDEPROPERTIES', 'SESSION_USER', 'SET', 'MINUS', 'SETS', 'SHOW', 'SKEWED', 'SOME', 'SORT', 'SORTED', 'START', 'STATISTICS', 'STORED', 'STRATIFY', 'STRUCT', 'SUBSTR', 'SUBSTRING', 'SYNC', 'TABLE', 'TABLES', 'TABLESAMPLE', 'TBLPROPERTIES', TEMPORARY, 'TERMINATED', 'THEN', 'TIME', 'TO', 'TOUCH', 'TRAILING', 'TRANSACTION', 'TRANSACTIONS', 'TRANSFORM', 'TRIM', 'TRUE', 'TRUNCATE', 'TRY_CAST', 'TYPE', 'UNARCHIVE', 'UNBOUNDED', 'UNCACHE', 'UNION', 'UNIQUE', 'UNKNOWN', 'UNLOCK', 'UNSET', 'UPDATE', 'USE', 'USER', 'USING', 'VALUES', 'VIEW', 'VIEWS', 'WHEN', 'WHERE', 'WINDOW', 'WITH', 'YEAR', 'ZONE', '+', '-', '*', '~', STRING, BIGINT_LITERAL, SMALLINT_LITERAL, TINYINT_LITERAL, INTEGER_VALUE, EXPONENT_VALUE, DECIMAL_VALUE, FLOAT_LITERAL, DOUBLE_LITERAL, BIGDECIMAL_LITERAL, IDENTIFIER, BACKQUOTED_IDENTIFIER}(line 1, pos 30)",
            "log_location":"(write) log_w_sql_orc (line 5124)",
            "pass":true
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: mismatched input '<EOF>' expecting {'(', 'ADD', 'AFTER', 'ALL', 'ALTER', 'ANALYZE', 'AND', 'ANTI', 'ANY', 'ARCHIVE', 'ARRAY', 'AS', 'ASC', 'AT', 'AUTHORIZATION', 'BETWEEN', 'BOTH', 'BUCKET', 'BUCKETS', 'BY', 'CACHE', 'CASCADE', 'CASE', 'CAST', 'CHANGE', 'CHECK', 'CLEAR', 'CLUSTER', 'CLUSTERED', 'CODEGEN', 'COLLATE', 'COLLECTION', 'COLUMN', 'COLUMNS', 'COMMENT', 'COMMIT', 'COMPACT', 'COMPACTIONS', 'COMPUTE', 'CONCATENATE', 'CONSTRAINT', 'COST', 'CREATE', 'CROSS', 'CUBE', 'CURRENT', 'CURRENT_DATE', 'CURRENT_TIME', 'CURRENT_TIMESTAMP', 'CURRENT_USER', 'DAY', 'DATA', 'DATABASE', DATABASES, 'DBPROPERTIES', 'DEFINED', 'DELETE', 'DELIMITED', 'DESC', 'DESCRIBE', 'DFS', 'DIRECTORIES', 'DIRECTORY', 'DISTINCT', 'DISTRIBUTE', 'DIV', 'DROP', 'ELSE', 'END', 'ESCAPE', 'ESCAPED', 'EXCEPT', 'EXCHANGE', 'EXISTS', 'EXPLAIN', 'EXPORT', 'EXTENDED', 'EXTERNAL', 'EXTRACT', 'FALSE', 'FETCH', 'FIELDS', 'FILTER', 'FILEFORMAT', 'FIRST', 'FOLLOWING', 'FOR', 'FOREIGN', 'FORMAT', 'FORMATTED', 'FROM', 'FULL', 'FUNCTION', 'FUNCTIONS', 'GLOBAL', 'GRANT', 'GROUP', 'GROUPING', 'HAVING', 'HOUR', 'IF', 'IGNORE', 'IMPORT', 'IN', 'INDEX', 'INDEXES', 'INNER', 'INPATH', 'INPUTFORMAT', 'INSERT', 'INTERSECT', 'INTERVAL', 'INTO', 'IS', 'ITEMS', 'JOIN', 'KEYS', 'LAST', 'LATERAL', 'LAZY', 'LEADING', 'LEFT', 'LIKE', 'LIMIT', 'LINES', 'LIST', 'LOAD', 'LOCAL', 'LOCATION', 'LOCK', 'LOCKS', 'LOGICAL', 'MACRO', 'MAP', 'MATCHED', 'MERGE', 'MINUTE', 'MONTH', 'MSCK', 'NAMESPACE', 'NAMESPACES', 'NATURAL', 'NO', NOT, 'NULL', 'NULLS', 'OF', 'ON', 'ONLY', 'OPTION', 'OPTIONS', 'OR', 'ORDER', 'OUT', 'OUTER', 'OUTPUTFORMAT', 'OVER', 'OVERLAPS', 'OVERLAY', 'OVERWRITE', 'PARTITION', 'PARTITIONED', 'PARTITIONS', 'PERCENT', 'PIVOT', 'PLACING', 'POSITION', 'PRECEDING', 'PRIMARY', 'PRINCIPALS', 'PROPERTIES', 'PURGE', 'QUERY', 'RANGE', 'RECORDREADER', 'RECORDWRITER', 'RECOVER', 'REDUCE', 'REFERENCES', 'REFRESH', 'RENAME', 'REPAIR', 'REPLACE', 'RESET', 'RESPECT', 'RESTRICT', 'REVOKE', 'RIGHT', RLIKE, 'ROLE', 'ROLES', 'ROLLBACK', 'ROLLUP', 'ROW', 'ROWS', 'SECOND', 'SCHEMA', 'SELECT', 'SEMI', 'SEPARATED', 'SERDE', 'SERDEPROPERTIES', 'SESSION_USER', 'SET', 'MINUS', 'SETS', 'SHOW', 'SKEWED', 'SOME', 'SORT', 'SORTED', 'START', 'STATISTICS', 'STORED', 'STRATIFY', 'STRUCT', 'SUBSTR', 'SUBSTRING', 'SYNC', 'TABLE', 'TABLES', 'TABLESAMPLE', 'TBLPROPERTIES', TEMPORARY, 'TERMINATED', 'THEN', 'TIME', 'TO', 'TOUCH', 'TRAILING', 'TRANSACTION', 'TRANSACTIONS', 'TRANSFORM', 'TRIM', 'TRUE', 'TRUNCATE', 'TRY_CAST', 'TYPE', 'UNARCHIVE', 'UNBOUNDED', 'UNCACHE', 'UNION', 'UNIQUE', 'UNKNOWN', 'UNLOCK', 'UNSET', 'UPDATE', 'USE', 'USER', 'USING', 'VALUES', 'VIEW', 'VIEWS', 'WHEN', 'WHERE', 'WINDOW', 'WITH', 'YEAR', 'ZONE', '+', '-', '*', '~', STRING, BIGINT_LITERAL, SMALLINT_LITERAL, TINYINT_LITERAL, INTEGER_VALUE, EXPONENT_VALUE, DECIMAL_VALUE, FLOAT_LITERAL, DOUBLE_LITERAL, BIGDECIMAL_LITERAL, IDENTIFIER, BACKQUOTED_IDENTIFIER}(line 1, pos 30)",
            "log_location":"(write) log_w_sql_parquet (line 5125)",
            "pass":true
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws127; line 1 pos 14;",
            "write_value":"No output, find exception: <console>:1: error: illegal start of simple expression",
            "log_location":"(read) log_w_df_r_df_avro (line 4686)",
            "pass":true
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: <console>:1: error: illegal start of simple expression",
            "log_location":"(write) log_w_df_orc (line 16042)",
            "pass":true
        }
    },
    "128":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"1, find exception: Error in query: Cannot write incompatible data to table '`default`.`ws128`':",
            "log_location":"(write) log_w_sql_avro (line 4261)",
            "pass":true
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws128; line 1 pos 14;",
            "write_value":"No output, find exception: java.lang.RuntimeException: Error while encoding: java.lang.RuntimeException: java.lang.Integer is not a valid external type for schema of boolean",
            "log_location":"(read) log_w_df_r_df_orc (line 3036)",
            "pass":true
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"1, find exception: Error in query: Cannot write incompatible data to table '`default`.`ws128`':",
            "log_location":"(write) log_w_sql_orc (line 5137)",
            "pass":true
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.RuntimeException: Error while encoding: java.lang.RuntimeException: java.lang.Integer is not a valid external type for schema of boolean",
            "log_location":"(write) log_w_df_avro (line 8483)",
            "pass":true
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"1, find exception: Error in query: Cannot write incompatible data to table '`default`.`ws128`':",
            "log_location":"(write) log_w_sql_avro (line 4261)",
            "pass":true
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.RuntimeException: Error while encoding: java.lang.RuntimeException: java.lang.Integer is not a valid external type for schema of boolean",
            "log_location":"(write) log_w_df_parquet (line 16078)",
            "pass":true
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"1, find exception: Error in query: Cannot write incompatible data to table '`default`.`ws128`':",
            "log_location":"(write) log_w_sql_parquet (line 5138)",
            "pass":true
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws128; line 1 pos 14;",
            "write_value":"No output, find exception: java.lang.RuntimeException: Error while encoding: java.lang.RuntimeException: java.lang.Integer is not a valid external type for schema of boolean",
            "log_location":"(read) log_w_df_r_df_parquet (line 3038)",
            "pass":true
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"1, find exception: Error in query: Cannot write incompatible data to table '`default`.`ws128`':",
            "log_location":"(write) log_w_sql_orc (line 5137)",
            "pass":true
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"1, find exception: Error in query: Cannot write incompatible data to table '`default`.`ws128`':",
            "log_location":"(write) log_w_sql_parquet (line 5138)",
            "pass":true
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws128; line 1 pos 14;",
            "write_value":"No output, find exception: java.lang.RuntimeException: Error while encoding: java.lang.RuntimeException: java.lang.Integer is not a valid external type for schema of boolean",
            "log_location":"(read) log_w_df_r_df_avro (line 4726)",
            "pass":true
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.RuntimeException: Error while encoding: java.lang.RuntimeException: java.lang.Integer is not a valid external type for schema of boolean",
            "log_location":"(write) log_w_df_orc (line 16075)",
            "pass":true
        }
    },
    "129":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: 'InsertIntoStatement 'UnresolvedRelation [ws129], [], false, false, false",
            "log_location":"(write) log_w_sql_avro (line 4274)",
            "pass":true
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws129; line 1 pos 14;",
            "write_value":"No output, find exception: <console>:28: error: not found: value foo",
            "log_location":"(read) log_w_df_r_df_orc (line 3076)",
            "pass":true
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: 'InsertIntoStatement 'UnresolvedRelation [ws129], [], false, false, false",
            "log_location":"(write) log_w_sql_orc (line 5149)",
            "pass":true
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: <console>:28: error: not found: value foo",
            "log_location":"(write) log_w_df_avro (line 8958)",
            "pass":true
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: 'InsertIntoStatement 'UnresolvedRelation [ws129], [], false, false, false",
            "log_location":"(write) log_w_sql_avro (line 4274)",
            "pass":true
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: <console>:28: error: not found: value foo",
            "log_location":"(write) log_w_df_parquet (line 16553)",
            "pass":true
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: 'InsertIntoStatement 'UnresolvedRelation [ws129], [], false, false, false",
            "log_location":"(write) log_w_sql_parquet (line 5150)",
            "pass":true
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws129; line 1 pos 14;",
            "write_value":"No output, find exception: <console>:28: error: not found: value foo",
            "log_location":"(read) log_w_df_r_df_parquet (line 3078)",
            "pass":true
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: 'InsertIntoStatement 'UnresolvedRelation [ws129], [], false, false, false",
            "log_location":"(write) log_w_sql_orc (line 5149)",
            "pass":true
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: 'InsertIntoStatement 'UnresolvedRelation [ws129], [], false, false, false",
            "log_location":"(write) log_w_sql_parquet (line 5150)",
            "pass":true
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws129; line 1 pos 14;",
            "write_value":"No output, find exception: <console>:28: error: not found: value foo",
            "log_location":"(read) log_w_df_r_df_avro (line 4766)",
            "pass":true
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: <console>:28: error: not found: value foo",
            "log_location":"(write) log_w_df_orc (line 16550)",
            "pass":true
        }
    },
    "130":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"1.1, find exception: Error in query: Cannot write incompatible data to table '`default`.`ws130`':",
            "log_location":"(write) log_w_sql_avro (line 4286)",
            "pass":true
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws130; line 1 pos 14;",
            "write_value":"No output, find exception: java.lang.RuntimeException: Error while encoding: java.lang.RuntimeException: java.lang.Double is not a valid external type for schema of boolean",
            "log_location":"(read) log_w_df_r_df_orc (line 3116)",
            "pass":true
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"1.1, find exception: Error in query: Cannot write incompatible data to table '`default`.`ws130`':",
            "log_location":"(write) log_w_sql_orc (line 5160)",
            "pass":true
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.RuntimeException: Error while encoding: java.lang.RuntimeException: java.lang.Double is not a valid external type for schema of boolean",
            "log_location":"(write) log_w_df_avro (line 8992)",
            "pass":true
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"1.1, find exception: Error in query: Cannot write incompatible data to table '`default`.`ws130`':",
            "log_location":"(write) log_w_sql_avro (line 4286)",
            "pass":true
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.RuntimeException: Error while encoding: java.lang.RuntimeException: java.lang.Double is not a valid external type for schema of boolean",
            "log_location":"(write) log_w_df_parquet (line 16587)",
            "pass":true
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"1.1, find exception: Error in query: Cannot write incompatible data to table '`default`.`ws130`':",
            "log_location":"(write) log_w_sql_parquet (line 5161)",
            "pass":true
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws130; line 1 pos 14;",
            "write_value":"No output, find exception: java.lang.RuntimeException: Error while encoding: java.lang.RuntimeException: java.lang.Double is not a valid external type for schema of boolean",
            "log_location":"(read) log_w_df_r_df_parquet (line 3118)",
            "pass":true
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"1.1, find exception: Error in query: Cannot write incompatible data to table '`default`.`ws130`':",
            "log_location":"(write) log_w_sql_orc (line 5160)",
            "pass":true
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"1.1, find exception: Error in query: Cannot write incompatible data to table '`default`.`ws130`':",
            "log_location":"(write) log_w_sql_parquet (line 5161)",
            "pass":true
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws130; line 1 pos 14;",
            "write_value":"No output, find exception: java.lang.RuntimeException: Error while encoding: java.lang.RuntimeException: java.lang.Double is not a valid external type for schema of boolean",
            "log_location":"(read) log_w_df_r_df_avro (line 4806)",
            "pass":true
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.RuntimeException: Error while encoding: java.lang.RuntimeException: java.lang.Double is not a valid external type for schema of boolean",
            "log_location":"(write) log_w_df_orc (line 16584)",
            "pass":true
        }
    },
    "194":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        }
    },
    "195":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        }
    },
    "196":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        }
    },
    "197":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        }
    },
    "198":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        }
    },
    "199":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        }
    },
    "200":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        }
    },
    "201":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        }
    },
    "202":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        }
    },
    "203":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        }
    },
    "204":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        }
    },
    "205":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        }
    },
    "206":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        }
    },
    "207":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        }
    },
    "208":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        }
    },
    "209":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        }
    },
    "210":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        }
    },
    "211":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        }
    },
    "212":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        }
    },
    "213":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        }
    },
    "214":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        }
    },
    "215":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        }
    },
    "216":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        }
    },
    "217":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        }
    },
    "218":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        }
    },
    "219":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        }
    },
    "220":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        }
    },
    "221":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        }
    },
    "222":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        }
    },
    "223":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        }
    },
    "224":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        }
    },
    "225":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        }
    },
    "226":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        }
    },
    "227":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        }
    },
    "228":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        }
    },
    "229":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        }
    },
    "230":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        }
    },
    "231":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        }
    },
    "232":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        }
    },
    "233":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        }
    },
    "234":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        }
    },
    "235":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        }
    },
    "236":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        }
    },
    "237":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        }
    },
    "238":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        }
    },
    "239":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot mix year-month and day-time fields: INTERVAL '10 years -11 month -12 days'(line 1, pos 57)",
            "log_location":"(write) log_w_sql_avro (line 5274)",
            "pass":true
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws239; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_orc (line 4020)",
            "pass":true
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot mix year-month and day-time fields: INTERVAL '10 years -11 month -12 days'(line 1, pos 57)",
            "log_location":"(write) log_w_sql_orc (line 6039)",
            "pass":true
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_avro (line 11925)",
            "pass":true
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot mix year-month and day-time fields: INTERVAL '10 years -11 month -12 days'(line 1, pos 57)",
            "log_location":"(write) log_w_sql_avro (line 5274)",
            "pass":true
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_parquet (line 19520)",
            "pass":true
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot mix year-month and day-time fields: INTERVAL '10 years -11 month -12 days'(line 1, pos 57)",
            "log_location":"(write) log_w_sql_parquet (line 6040)",
            "pass":true
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws239; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_parquet (line 4022)",
            "pass":true
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot mix year-month and day-time fields: INTERVAL '10 years -11 month -12 days'(line 1, pos 57)",
            "log_location":"(write) log_w_sql_orc (line 6039)",
            "pass":true
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot mix year-month and day-time fields: INTERVAL '10 years -11 month -12 days'(line 1, pos 57)",
            "log_location":"(write) log_w_sql_parquet (line 6040)",
            "pass":true
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws239; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_avro (line 5710)",
            "pass":true
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_orc (line 19521)",
            "pass":true
        }
    },
    "240":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_sql_avro (line 5577)",
            "pass":true
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws240; line 1 pos 14;",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(read) log_w_df_r_df_orc (line 4060)",
            "pass":true
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_sql_orc (line 6341)",
            "pass":true
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_df_avro (line 11991)",
            "pass":true
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_sql_avro (line 5577)",
            "pass":true
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_df_parquet (line 19586)",
            "pass":true
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_sql_parquet (line 6342)",
            "pass":true
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws240; line 1 pos 14;",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(read) log_w_df_r_df_parquet (line 4062)",
            "pass":true
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_sql_orc (line 6341)",
            "pass":true
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_sql_parquet (line 6342)",
            "pass":true
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws240; line 1 pos 14;",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(read) log_w_df_r_df_avro (line 5750)",
            "pass":true
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_df_orc (line 19587)",
            "pass":true
        }
    },
    "241":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_sql_avro (line 6113)",
            "pass":true
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws241; line 1 pos 14;",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(read) log_w_df_r_df_orc (line 4100)",
            "pass":true
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_sql_orc (line 6876)",
            "pass":true
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_df_avro (line 12052)",
            "pass":true
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_sql_avro (line 6113)",
            "pass":true
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_df_parquet (line 19647)",
            "pass":true
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_sql_parquet (line 6877)",
            "pass":true
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws241; line 1 pos 14;",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(read) log_w_df_r_df_parquet (line 4102)",
            "pass":true
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_sql_orc (line 6876)",
            "pass":true
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_sql_parquet (line 6877)",
            "pass":true
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws241; line 1 pos 14;",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(read) log_w_df_r_df_avro (line 5790)",
            "pass":true
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_df_orc (line 19648)",
            "pass":true
        }
    },
    "242":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Error in query: Literals of type 'epoch' are currently not supported for the interval second type.; line 1 pos 57",
            "log_location":"(write) log_w_sql_avro (line 6359)",
            "pass":true
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws242; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Literals of type 'epoch' are currently not supported for the interval second type.; line 1 pos 0",
            "log_location":"(read) log_w_df_r_df_orc (line 4140)",
            "pass":true
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Error in query: Literals of type 'epoch' are currently not supported for the interval second type.; line 1 pos 57",
            "log_location":"(write) log_w_sql_orc (line 7121)",
            "pass":true
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Literals of type 'epoch' are currently not supported for the interval second type.; line 1 pos 0",
            "log_location":"(write) log_w_df_avro (line 12123)",
            "pass":true
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Error in query: Literals of type 'epoch' are currently not supported for the interval second type.; line 1 pos 57",
            "log_location":"(write) log_w_sql_avro (line 6359)",
            "pass":true
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Literals of type 'epoch' are currently not supported for the interval second type.; line 1 pos 0",
            "log_location":"(write) log_w_df_parquet (line 19718)",
            "pass":true
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Error in query: Literals of type 'epoch' are currently not supported for the interval second type.; line 1 pos 57",
            "log_location":"(write) log_w_sql_parquet (line 7122)",
            "pass":true
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws242; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Literals of type 'epoch' are currently not supported for the interval second type.; line 1 pos 0",
            "log_location":"(read) log_w_df_r_df_parquet (line 4142)",
            "pass":true
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Error in query: Literals of type 'epoch' are currently not supported for the interval second type.; line 1 pos 57",
            "log_location":"(write) log_w_sql_orc (line 7121)",
            "pass":true
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Error in query: Literals of type 'epoch' are currently not supported for the interval second type.; line 1 pos 57",
            "log_location":"(write) log_w_sql_parquet (line 7122)",
            "pass":true
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws242; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Literals of type 'epoch' are currently not supported for the interval second type.; line 1 pos 0",
            "log_location":"(read) log_w_df_r_df_avro (line 5830)",
            "pass":true
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Literals of type 'epoch' are currently not supported for the interval second type.; line 1 pos 0",
            "log_location":"(write) log_w_df_orc (line 19719)",
            "pass":true
        }
    },
    "243":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Interval string does not match day-time format of `[+|-]d h`, `INTERVAL [+|-]'[+|-]d h' DAY TO HOUR` when cast to interval day to hour: 1 2:03:04, set spark.sql.legacy.fromDayTimeString.enabled to true to restore the behavior before Spark 3.0.(line 1, pos 66)",
            "log_location":"(write) log_w_sql_avro (line 6374)",
            "pass":true
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws243; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_orc (line 4180)",
            "pass":true
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Interval string does not match day-time format of `[+|-]d h`, `INTERVAL [+|-]'[+|-]d h' DAY TO HOUR` when cast to interval day to hour: 1 2:03:04, set spark.sql.legacy.fromDayTimeString.enabled to true to restore the behavior before Spark 3.0.(line 1, pos 66)",
            "log_location":"(write) log_w_sql_orc (line 7135)",
            "pass":true
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_avro (line 12238)",
            "pass":true
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Interval string does not match day-time format of `[+|-]d h`, `INTERVAL [+|-]'[+|-]d h' DAY TO HOUR` when cast to interval day to hour: 1 2:03:04, set spark.sql.legacy.fromDayTimeString.enabled to true to restore the behavior before Spark 3.0.(line 1, pos 66)",
            "log_location":"(write) log_w_sql_avro (line 6374)",
            "pass":true
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_parquet (line 19833)",
            "pass":true
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Interval string does not match day-time format of `[+|-]d h`, `INTERVAL [+|-]'[+|-]d h' DAY TO HOUR` when cast to interval day to hour: 1 2:03:04, set spark.sql.legacy.fromDayTimeString.enabled to true to restore the behavior before Spark 3.0.(line 1, pos 66)",
            "log_location":"(write) log_w_sql_parquet (line 7136)",
            "pass":true
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws243; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_parquet (line 4182)",
            "pass":true
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Interval string does not match day-time format of `[+|-]d h`, `INTERVAL [+|-]'[+|-]d h' DAY TO HOUR` when cast to interval day to hour: 1 2:03:04, set spark.sql.legacy.fromDayTimeString.enabled to true to restore the behavior before Spark 3.0.(line 1, pos 66)",
            "log_location":"(write) log_w_sql_orc (line 7135)",
            "pass":true
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Interval string does not match day-time format of `[+|-]d h`, `INTERVAL [+|-]'[+|-]d h' DAY TO HOUR` when cast to interval day to hour: 1 2:03:04, set spark.sql.legacy.fromDayTimeString.enabled to true to restore the behavior before Spark 3.0.(line 1, pos 66)",
            "log_location":"(write) log_w_sql_parquet (line 7136)",
            "pass":true
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws243; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_avro (line 5870)",
            "pass":true
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_orc (line 19834)",
            "pass":true
        }
    },
    "244":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot parse the INTERVAL value: 1-2(line 1, pos 57)",
            "log_location":"(write) log_w_sql_avro (line 6394)",
            "pass":true
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws244; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_orc (line 4220)",
            "pass":true
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot parse the INTERVAL value: 1-2(line 1, pos 57)",
            "log_location":"(write) log_w_sql_orc (line 7154)",
            "pass":true
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_avro (line 12315)",
            "pass":true
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot parse the INTERVAL value: 1-2(line 1, pos 57)",
            "log_location":"(write) log_w_sql_avro (line 6394)",
            "pass":true
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_parquet (line 19910)",
            "pass":true
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot parse the INTERVAL value: 1-2(line 1, pos 57)",
            "log_location":"(write) log_w_sql_parquet (line 7155)",
            "pass":true
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws244; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_parquet (line 4222)",
            "pass":true
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot parse the INTERVAL value: 1-2(line 1, pos 57)",
            "log_location":"(write) log_w_sql_orc (line 7154)",
            "pass":true
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot parse the INTERVAL value: 1-2(line 1, pos 57)",
            "log_location":"(write) log_w_sql_parquet (line 7155)",
            "pass":true
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws244; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_avro (line 5910)",
            "pass":true
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_orc (line 19911)",
            "pass":true
        }
    },
    "245":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot parse the INTERVAL value: 1 day 01:23:45.6789(line 1, pos 57)",
            "log_location":"(write) log_w_sql_avro (line 6414)",
            "pass":true
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws245; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_orc (line 4260)",
            "pass":true
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot parse the INTERVAL value: 1 day 01:23:45.6789(line 1, pos 57)",
            "log_location":"(write) log_w_sql_orc (line 7173)",
            "pass":true
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_avro (line 12381)",
            "pass":true
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot parse the INTERVAL value: 1 day 01:23:45.6789(line 1, pos 57)",
            "log_location":"(write) log_w_sql_avro (line 6414)",
            "pass":true
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_parquet (line 19976)",
            "pass":true
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot parse the INTERVAL value: 1 day 01:23:45.6789(line 1, pos 57)",
            "log_location":"(write) log_w_sql_parquet (line 7174)",
            "pass":true
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws245; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_parquet (line 4262)",
            "pass":true
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot parse the INTERVAL value: 1 day 01:23:45.6789(line 1, pos 57)",
            "log_location":"(write) log_w_sql_orc (line 7173)",
            "pass":true
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot parse the INTERVAL value: 1 day 01:23:45.6789(line 1, pos 57)",
            "log_location":"(write) log_w_sql_parquet (line 7174)",
            "pass":true
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws245; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_avro (line 5950)",
            "pass":true
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_orc (line 19977)",
            "pass":true
        }
    },
    "246":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Error parsing ' 123 11 day' to interval, invalid unit '11'(line 1, pos 66)",
            "log_location":"(write) log_w_sql_avro (line 6434)",
            "pass":true
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws246; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_orc (line 4300)",
            "pass":true
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Error parsing ' 123 11 day' to interval, invalid unit '11'(line 1, pos 66)",
            "log_location":"(write) log_w_sql_orc (line 7192)",
            "pass":true
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_avro (line 12447)",
            "pass":true
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Error parsing ' 123 11 day' to interval, invalid unit '11'(line 1, pos 66)",
            "log_location":"(write) log_w_sql_avro (line 6434)",
            "pass":true
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_parquet (line 20042)",
            "pass":true
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Error parsing ' 123 11 day' to interval, invalid unit '11'(line 1, pos 66)",
            "log_location":"(write) log_w_sql_parquet (line 7193)",
            "pass":true
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws246; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_parquet (line 4302)",
            "pass":true
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Error parsing ' 123 11 day' to interval, invalid unit '11'(line 1, pos 66)",
            "log_location":"(write) log_w_sql_orc (line 7192)",
            "pass":true
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Error parsing ' 123 11 day' to interval, invalid unit '11'(line 1, pos 66)",
            "log_location":"(write) log_w_sql_parquet (line 7193)",
            "pass":true
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws246; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_avro (line 5990)",
            "pass":true
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_orc (line 20043)",
            "pass":true
        }
    },
    "247":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Can only use numbers in the interval value part for multiple unit value pairs interval form, but got invalid value: 123 days hours(line 1, pos 66)",
            "log_location":"(write) log_w_sql_avro (line 6454)",
            "pass":true
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws247; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_orc (line 4340)",
            "pass":true
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Can only use numbers in the interval value part for multiple unit value pairs interval form, but got invalid value: 123 days hours(line 1, pos 66)",
            "log_location":"(write) log_w_sql_orc (line 7211)",
            "pass":true
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_avro (line 12522)",
            "pass":true
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Can only use numbers in the interval value part for multiple unit value pairs interval form, but got invalid value: 123 days hours(line 1, pos 66)",
            "log_location":"(write) log_w_sql_avro (line 6454)",
            "pass":true
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_parquet (line 20117)",
            "pass":true
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Can only use numbers in the interval value part for multiple unit value pairs interval form, but got invalid value: 123 days hours(line 1, pos 66)",
            "log_location":"(write) log_w_sql_parquet (line 7212)",
            "pass":true
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws247; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_parquet (line 4342)",
            "pass":true
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Can only use numbers in the interval value part for multiple unit value pairs interval form, but got invalid value: 123 days hours(line 1, pos 66)",
            "log_location":"(write) log_w_sql_orc (line 7211)",
            "pass":true
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Can only use numbers in the interval value part for multiple unit value pairs interval form, but got invalid value: 123 days hours(line 1, pos 66)",
            "log_location":"(write) log_w_sql_parquet (line 7212)",
            "pass":true
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws247; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_avro (line 6030)",
            "pass":true
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_orc (line 20118)",
            "pass":true
        }
    },
    "248":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot mix year-month and day-time fields: INTERVAL '10 years -11 month -12 days'(line 1, pos 60)",
            "log_location":"(write) log_w_sql_avro (line 6474)",
            "pass":true
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws248; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_orc (line 4380)",
            "pass":true
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot mix year-month and day-time fields: INTERVAL '10 years -11 month -12 days'(line 1, pos 60)",
            "log_location":"(write) log_w_sql_orc (line 7230)",
            "pass":true
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_avro (line 12603)",
            "pass":true
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot mix year-month and day-time fields: INTERVAL '10 years -11 month -12 days'(line 1, pos 60)",
            "log_location":"(write) log_w_sql_avro (line 6474)",
            "pass":true
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_parquet (line 20198)",
            "pass":true
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot mix year-month and day-time fields: INTERVAL '10 years -11 month -12 days'(line 1, pos 60)",
            "log_location":"(write) log_w_sql_parquet (line 7231)",
            "pass":true
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws248; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_parquet (line 4382)",
            "pass":true
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot mix year-month and day-time fields: INTERVAL '10 years -11 month -12 days'(line 1, pos 60)",
            "log_location":"(write) log_w_sql_orc (line 7230)",
            "pass":true
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot mix year-month and day-time fields: INTERVAL '10 years -11 month -12 days'(line 1, pos 60)",
            "log_location":"(write) log_w_sql_parquet (line 7231)",
            "pass":true
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws248; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_avro (line 6070)",
            "pass":true
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_orc (line 20199)",
            "pass":true
        }
    },
    "249":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_sql_avro (line 6777)",
            "pass":true
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws249; line 1 pos 14;",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(read) log_w_df_r_df_orc (line 4420)",
            "pass":true
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_sql_orc (line 7532)",
            "pass":true
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_df_avro (line 12669)",
            "pass":true
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_sql_avro (line 6777)",
            "pass":true
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_df_parquet (line 20264)",
            "pass":true
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_sql_parquet (line 7533)",
            "pass":true
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws249; line 1 pos 14;",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(read) log_w_df_r_df_parquet (line 4422)",
            "pass":true
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_sql_orc (line 7532)",
            "pass":true
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_sql_parquet (line 7533)",
            "pass":true
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws249; line 1 pos 14;",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(read) log_w_df_r_df_avro (line 6110)",
            "pass":true
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_df_orc (line 20265)",
            "pass":true
        }
    },
    "250":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_sql_avro (line 7313)",
            "pass":true
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws250; line 1 pos 14;",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(read) log_w_df_r_df_orc (line 4460)",
            "pass":true
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_sql_orc (line 8067)",
            "pass":true
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_df_avro (line 12730)",
            "pass":true
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_sql_avro (line 7313)",
            "pass":true
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_df_parquet (line 20325)",
            "pass":true
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_sql_parquet (line 8068)",
            "pass":true
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws250; line 1 pos 14;",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(read) log_w_df_r_df_parquet (line 4462)",
            "pass":true
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_sql_orc (line 8067)",
            "pass":true
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_sql_parquet (line 8068)",
            "pass":true
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws250; line 1 pos 14;",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(read) log_w_df_r_df_avro (line 6150)",
            "pass":true
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_df_orc (line 20326)",
            "pass":true
        }
    },
    "251":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Error in query: Literals of type 'epoch' are currently not supported for the interval second type.; line 1 pos 60",
            "log_location":"(write) log_w_sql_avro (line 7559)",
            "pass":true
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws251; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Literals of type 'epoch' are currently not supported for the interval second type.; line 1 pos 0",
            "log_location":"(read) log_w_df_r_df_orc (line 4500)",
            "pass":true
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Error in query: Literals of type 'epoch' are currently not supported for the interval second type.; line 1 pos 60",
            "log_location":"(write) log_w_sql_orc (line 8312)",
            "pass":true
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Literals of type 'epoch' are currently not supported for the interval second type.; line 1 pos 0",
            "log_location":"(write) log_w_df_avro (line 12801)",
            "pass":true
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Error in query: Literals of type 'epoch' are currently not supported for the interval second type.; line 1 pos 60",
            "log_location":"(write) log_w_sql_avro (line 7559)",
            "pass":true
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Literals of type 'epoch' are currently not supported for the interval second type.; line 1 pos 0",
            "log_location":"(write) log_w_df_parquet (line 20396)",
            "pass":true
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Error in query: Literals of type 'epoch' are currently not supported for the interval second type.; line 1 pos 60",
            "log_location":"(write) log_w_sql_parquet (line 8313)",
            "pass":true
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws251; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Literals of type 'epoch' are currently not supported for the interval second type.; line 1 pos 0",
            "log_location":"(read) log_w_df_r_df_parquet (line 4502)",
            "pass":true
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Error in query: Literals of type 'epoch' are currently not supported for the interval second type.; line 1 pos 60",
            "log_location":"(write) log_w_sql_orc (line 8312)",
            "pass":true
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Error in query: Literals of type 'epoch' are currently not supported for the interval second type.; line 1 pos 60",
            "log_location":"(write) log_w_sql_parquet (line 8313)",
            "pass":true
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws251; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Literals of type 'epoch' are currently not supported for the interval second type.; line 1 pos 0",
            "log_location":"(read) log_w_df_r_df_avro (line 6190)",
            "pass":true
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Literals of type 'epoch' are currently not supported for the interval second type.; line 1 pos 0",
            "log_location":"(write) log_w_df_orc (line 20397)",
            "pass":true
        }
    },
    "252":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Interval string does not match day-time format of `[+|-]d h`, `INTERVAL [+|-]'[+|-]d h' DAY TO HOUR` when cast to interval day to hour: 1 2:03:04, set spark.sql.legacy.fromDayTimeString.enabled to true to restore the behavior before Spark 3.0.(line 1, pos 69)",
            "log_location":"(write) log_w_sql_avro (line 7574)",
            "pass":true
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws252; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_orc (line 4540)",
            "pass":true
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Interval string does not match day-time format of `[+|-]d h`, `INTERVAL [+|-]'[+|-]d h' DAY TO HOUR` when cast to interval day to hour: 1 2:03:04, set spark.sql.legacy.fromDayTimeString.enabled to true to restore the behavior before Spark 3.0.(line 1, pos 69)",
            "log_location":"(write) log_w_sql_orc (line 8326)",
            "pass":true
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_avro (line 12916)",
            "pass":true
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Interval string does not match day-time format of `[+|-]d h`, `INTERVAL [+|-]'[+|-]d h' DAY TO HOUR` when cast to interval day to hour: 1 2:03:04, set spark.sql.legacy.fromDayTimeString.enabled to true to restore the behavior before Spark 3.0.(line 1, pos 69)",
            "log_location":"(write) log_w_sql_avro (line 7574)",
            "pass":true
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_parquet (line 20511)",
            "pass":true
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Interval string does not match day-time format of `[+|-]d h`, `INTERVAL [+|-]'[+|-]d h' DAY TO HOUR` when cast to interval day to hour: 1 2:03:04, set spark.sql.legacy.fromDayTimeString.enabled to true to restore the behavior before Spark 3.0.(line 1, pos 69)",
            "log_location":"(write) log_w_sql_parquet (line 8327)",
            "pass":true
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws252; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_parquet (line 4542)",
            "pass":true
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Interval string does not match day-time format of `[+|-]d h`, `INTERVAL [+|-]'[+|-]d h' DAY TO HOUR` when cast to interval day to hour: 1 2:03:04, set spark.sql.legacy.fromDayTimeString.enabled to true to restore the behavior before Spark 3.0.(line 1, pos 69)",
            "log_location":"(write) log_w_sql_orc (line 8326)",
            "pass":true
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Interval string does not match day-time format of `[+|-]d h`, `INTERVAL [+|-]'[+|-]d h' DAY TO HOUR` when cast to interval day to hour: 1 2:03:04, set spark.sql.legacy.fromDayTimeString.enabled to true to restore the behavior before Spark 3.0.(line 1, pos 69)",
            "log_location":"(write) log_w_sql_parquet (line 8327)",
            "pass":true
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws252; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_avro (line 6230)",
            "pass":true
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_orc (line 20512)",
            "pass":true
        }
    },
    "253":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot parse the INTERVAL value: 1-2(line 1, pos 60)",
            "log_location":"(write) log_w_sql_avro (line 7594)",
            "pass":true
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws253; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_orc (line 4580)",
            "pass":true
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot parse the INTERVAL value: 1-2(line 1, pos 60)",
            "log_location":"(write) log_w_sql_orc (line 8345)",
            "pass":true
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_avro (line 12993)",
            "pass":true
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot parse the INTERVAL value: 1-2(line 1, pos 60)",
            "log_location":"(write) log_w_sql_avro (line 7594)",
            "pass":true
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_parquet (line 20588)",
            "pass":true
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot parse the INTERVAL value: 1-2(line 1, pos 60)",
            "log_location":"(write) log_w_sql_parquet (line 8346)",
            "pass":true
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws253; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_parquet (line 4582)",
            "pass":true
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot parse the INTERVAL value: 1-2(line 1, pos 60)",
            "log_location":"(write) log_w_sql_orc (line 8345)",
            "pass":true
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot parse the INTERVAL value: 1-2(line 1, pos 60)",
            "log_location":"(write) log_w_sql_parquet (line 8346)",
            "pass":true
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws253; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_avro (line 6270)",
            "pass":true
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_orc (line 20589)",
            "pass":true
        }
    },
    "254":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot parse the INTERVAL value: 1 day 01:23:45.6789(line 1, pos 60)",
            "log_location":"(write) log_w_sql_avro (line 7614)",
            "pass":true
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws254; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_orc (line 4620)",
            "pass":true
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot parse the INTERVAL value: 1 day 01:23:45.6789(line 1, pos 60)",
            "log_location":"(write) log_w_sql_orc (line 8364)",
            "pass":true
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_avro (line 13059)",
            "pass":true
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot parse the INTERVAL value: 1 day 01:23:45.6789(line 1, pos 60)",
            "log_location":"(write) log_w_sql_avro (line 7614)",
            "pass":true
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_parquet (line 20654)",
            "pass":true
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot parse the INTERVAL value: 1 day 01:23:45.6789(line 1, pos 60)",
            "log_location":"(write) log_w_sql_parquet (line 8365)",
            "pass":true
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws254; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_parquet (line 4622)",
            "pass":true
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot parse the INTERVAL value: 1 day 01:23:45.6789(line 1, pos 60)",
            "log_location":"(write) log_w_sql_orc (line 8364)",
            "pass":true
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot parse the INTERVAL value: 1 day 01:23:45.6789(line 1, pos 60)",
            "log_location":"(write) log_w_sql_parquet (line 8365)",
            "pass":true
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws254; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_avro (line 6310)",
            "pass":true
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_orc (line 20655)",
            "pass":true
        }
    },
    "255":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Error parsing ' 123 11 day' to interval, invalid unit '11'(line 1, pos 69)",
            "log_location":"(write) log_w_sql_avro (line 7634)",
            "pass":true
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws255; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_orc (line 4660)",
            "pass":true
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Error parsing ' 123 11 day' to interval, invalid unit '11'(line 1, pos 69)",
            "log_location":"(write) log_w_sql_orc (line 8383)",
            "pass":true
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_avro (line 13125)",
            "pass":true
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Error parsing ' 123 11 day' to interval, invalid unit '11'(line 1, pos 69)",
            "log_location":"(write) log_w_sql_avro (line 7634)",
            "pass":true
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_parquet (line 20720)",
            "pass":true
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Error parsing ' 123 11 day' to interval, invalid unit '11'(line 1, pos 69)",
            "log_location":"(write) log_w_sql_parquet (line 8384)",
            "pass":true
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws255; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_parquet (line 4662)",
            "pass":true
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Error parsing ' 123 11 day' to interval, invalid unit '11'(line 1, pos 69)",
            "log_location":"(write) log_w_sql_orc (line 8383)",
            "pass":true
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Error parsing ' 123 11 day' to interval, invalid unit '11'(line 1, pos 69)",
            "log_location":"(write) log_w_sql_parquet (line 8384)",
            "pass":true
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws255; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_avro (line 6350)",
            "pass":true
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_orc (line 20721)",
            "pass":true
        }
    },
    "256":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Can only use numbers in the interval value part for multiple unit value pairs interval form, but got invalid value: 123 days hours(line 1, pos 69)",
            "log_location":"(write) log_w_sql_avro (line 7654)",
            "pass":true
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws256; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_orc (line 4700)",
            "pass":true
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Can only use numbers in the interval value part for multiple unit value pairs interval form, but got invalid value: 123 days hours(line 1, pos 69)",
            "log_location":"(write) log_w_sql_orc (line 8402)",
            "pass":true
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_avro (line 13200)",
            "pass":true
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Can only use numbers in the interval value part for multiple unit value pairs interval form, but got invalid value: 123 days hours(line 1, pos 69)",
            "log_location":"(write) log_w_sql_avro (line 7654)",
            "pass":true
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_parquet (line 20795)",
            "pass":true
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Can only use numbers in the interval value part for multiple unit value pairs interval form, but got invalid value: 123 days hours(line 1, pos 69)",
            "log_location":"(write) log_w_sql_parquet (line 8403)",
            "pass":true
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws256; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_parquet (line 4702)",
            "pass":true
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Can only use numbers in the interval value part for multiple unit value pairs interval form, but got invalid value: 123 days hours(line 1, pos 69)",
            "log_location":"(write) log_w_sql_orc (line 8402)",
            "pass":true
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Can only use numbers in the interval value part for multiple unit value pairs interval form, but got invalid value: 123 days hours(line 1, pos 69)",
            "log_location":"(write) log_w_sql_parquet (line 8403)",
            "pass":true
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws256; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_avro (line 6390)",
            "pass":true
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_orc (line 20796)",
            "pass":true
        }
    },
    "257":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot mix year-month and day-time fields: INTERVAL '10 years -11 month -12 days'(line 1, pos 63)",
            "log_location":"(write) log_w_sql_avro (line 7674)",
            "pass":true
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws257; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_orc (line 4740)",
            "pass":true
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot mix year-month and day-time fields: INTERVAL '10 years -11 month -12 days'(line 1, pos 63)",
            "log_location":"(write) log_w_sql_orc (line 8421)",
            "pass":true
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_avro (line 13281)",
            "pass":true
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot mix year-month and day-time fields: INTERVAL '10 years -11 month -12 days'(line 1, pos 63)",
            "log_location":"(write) log_w_sql_avro (line 7674)",
            "pass":true
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_parquet (line 20876)",
            "pass":true
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot mix year-month and day-time fields: INTERVAL '10 years -11 month -12 days'(line 1, pos 63)",
            "log_location":"(write) log_w_sql_parquet (line 8422)",
            "pass":true
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws257; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_parquet (line 4742)",
            "pass":true
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot mix year-month and day-time fields: INTERVAL '10 years -11 month -12 days'(line 1, pos 63)",
            "log_location":"(write) log_w_sql_orc (line 8421)",
            "pass":true
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot mix year-month and day-time fields: INTERVAL '10 years -11 month -12 days'(line 1, pos 63)",
            "log_location":"(write) log_w_sql_parquet (line 8422)",
            "pass":true
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws257; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_avro (line 6430)",
            "pass":true
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_orc (line 20877)",
            "pass":true
        }
    },
    "258":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_sql_avro (line 7977)",
            "pass":true
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws258; line 1 pos 14;",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(read) log_w_df_r_df_orc (line 4780)",
            "pass":true
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_sql_orc (line 8723)",
            "pass":true
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_df_avro (line 13347)",
            "pass":true
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_sql_avro (line 7977)",
            "pass":true
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_df_parquet (line 20942)",
            "pass":true
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_sql_parquet (line 8724)",
            "pass":true
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws258; line 1 pos 14;",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(read) log_w_df_r_df_parquet (line 4782)",
            "pass":true
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_sql_orc (line 8723)",
            "pass":true
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_sql_parquet (line 8724)",
            "pass":true
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws258; line 1 pos 14;",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(read) log_w_df_r_df_avro (line 6470)",
            "pass":true
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_df_orc (line 20943)",
            "pass":true
        }
    },
    "259":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_sql_avro (line 8513)",
            "pass":true
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws259; line 1 pos 14;",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(read) log_w_df_r_df_orc (line 4820)",
            "pass":true
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_sql_orc (line 9258)",
            "pass":true
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_df_avro (line 13408)",
            "pass":true
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_sql_avro (line 8513)",
            "pass":true
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_df_parquet (line 21003)",
            "pass":true
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_sql_parquet (line 9259)",
            "pass":true
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws259; line 1 pos 14;",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(read) log_w_df_r_df_parquet (line 4822)",
            "pass":true
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_sql_orc (line 9258)",
            "pass":true
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_sql_parquet (line 9259)",
            "pass":true
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws259; line 1 pos 14;",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(read) log_w_df_r_df_avro (line 6510)",
            "pass":true
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_df_orc (line 21004)",
            "pass":true
        }
    },
    "260":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Error in query: Literals of type 'epoch' are currently not supported for the interval second type.; line 1 pos 63",
            "log_location":"(write) log_w_sql_avro (line 8759)",
            "pass":true
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws260; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Literals of type 'epoch' are currently not supported for the interval second type.; line 1 pos 0",
            "log_location":"(read) log_w_df_r_df_orc (line 4860)",
            "pass":true
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Error in query: Literals of type 'epoch' are currently not supported for the interval second type.; line 1 pos 63",
            "log_location":"(write) log_w_sql_orc (line 9503)",
            "pass":true
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Literals of type 'epoch' are currently not supported for the interval second type.; line 1 pos 0",
            "log_location":"(write) log_w_df_avro (line 13479)",
            "pass":true
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Error in query: Literals of type 'epoch' are currently not supported for the interval second type.; line 1 pos 63",
            "log_location":"(write) log_w_sql_avro (line 8759)",
            "pass":true
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Literals of type 'epoch' are currently not supported for the interval second type.; line 1 pos 0",
            "log_location":"(write) log_w_df_parquet (line 21074)",
            "pass":true
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Error in query: Literals of type 'epoch' are currently not supported for the interval second type.; line 1 pos 63",
            "log_location":"(write) log_w_sql_parquet (line 9504)",
            "pass":true
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws260; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Literals of type 'epoch' are currently not supported for the interval second type.; line 1 pos 0",
            "log_location":"(read) log_w_df_r_df_parquet (line 4862)",
            "pass":true
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Error in query: Literals of type 'epoch' are currently not supported for the interval second type.; line 1 pos 63",
            "log_location":"(write) log_w_sql_orc (line 9503)",
            "pass":true
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Error in query: Literals of type 'epoch' are currently not supported for the interval second type.; line 1 pos 63",
            "log_location":"(write) log_w_sql_parquet (line 9504)",
            "pass":true
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws260; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Literals of type 'epoch' are currently not supported for the interval second type.; line 1 pos 0",
            "log_location":"(read) log_w_df_r_df_avro (line 6550)",
            "pass":true
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Literals of type 'epoch' are currently not supported for the interval second type.; line 1 pos 0",
            "log_location":"(write) log_w_df_orc (line 21075)",
            "pass":true
        }
    },
    "261":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Interval string does not match day-time format of `[+|-]d h`, `INTERVAL [+|-]'[+|-]d h' DAY TO HOUR` when cast to interval day to hour: 1 2:03:04, set spark.sql.legacy.fromDayTimeString.enabled to true to restore the behavior before Spark 3.0.(line 1, pos 72)",
            "log_location":"(write) log_w_sql_avro (line 8774)",
            "pass":true
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws261; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_orc (line 4900)",
            "pass":true
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Interval string does not match day-time format of `[+|-]d h`, `INTERVAL [+|-]'[+|-]d h' DAY TO HOUR` when cast to interval day to hour: 1 2:03:04, set spark.sql.legacy.fromDayTimeString.enabled to true to restore the behavior before Spark 3.0.(line 1, pos 72)",
            "log_location":"(write) log_w_sql_orc (line 9517)",
            "pass":true
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_avro (line 13594)",
            "pass":true
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Interval string does not match day-time format of `[+|-]d h`, `INTERVAL [+|-]'[+|-]d h' DAY TO HOUR` when cast to interval day to hour: 1 2:03:04, set spark.sql.legacy.fromDayTimeString.enabled to true to restore the behavior before Spark 3.0.(line 1, pos 72)",
            "log_location":"(write) log_w_sql_avro (line 8774)",
            "pass":true
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_parquet (line 21189)",
            "pass":true
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Interval string does not match day-time format of `[+|-]d h`, `INTERVAL [+|-]'[+|-]d h' DAY TO HOUR` when cast to interval day to hour: 1 2:03:04, set spark.sql.legacy.fromDayTimeString.enabled to true to restore the behavior before Spark 3.0.(line 1, pos 72)",
            "log_location":"(write) log_w_sql_parquet (line 9518)",
            "pass":true
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws261; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_parquet (line 4902)",
            "pass":true
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Interval string does not match day-time format of `[+|-]d h`, `INTERVAL [+|-]'[+|-]d h' DAY TO HOUR` when cast to interval day to hour: 1 2:03:04, set spark.sql.legacy.fromDayTimeString.enabled to true to restore the behavior before Spark 3.0.(line 1, pos 72)",
            "log_location":"(write) log_w_sql_orc (line 9517)",
            "pass":true
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Interval string does not match day-time format of `[+|-]d h`, `INTERVAL [+|-]'[+|-]d h' DAY TO HOUR` when cast to interval day to hour: 1 2:03:04, set spark.sql.legacy.fromDayTimeString.enabled to true to restore the behavior before Spark 3.0.(line 1, pos 72)",
            "log_location":"(write) log_w_sql_parquet (line 9518)",
            "pass":true
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws261; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_avro (line 6590)",
            "pass":true
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_orc (line 21190)",
            "pass":true
        }
    },
    "262":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot parse the INTERVAL value: 1-2(line 1, pos 63)",
            "log_location":"(write) log_w_sql_avro (line 8794)",
            "pass":true
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws262; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_orc (line 4940)",
            "pass":true
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot parse the INTERVAL value: 1-2(line 1, pos 63)",
            "log_location":"(write) log_w_sql_orc (line 9536)",
            "pass":true
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_avro (line 13671)",
            "pass":true
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot parse the INTERVAL value: 1-2(line 1, pos 63)",
            "log_location":"(write) log_w_sql_avro (line 8794)",
            "pass":true
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_parquet (line 21266)",
            "pass":true
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot parse the INTERVAL value: 1-2(line 1, pos 63)",
            "log_location":"(write) log_w_sql_parquet (line 9537)",
            "pass":true
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws262; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_parquet (line 4942)",
            "pass":true
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot parse the INTERVAL value: 1-2(line 1, pos 63)",
            "log_location":"(write) log_w_sql_orc (line 9536)",
            "pass":true
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot parse the INTERVAL value: 1-2(line 1, pos 63)",
            "log_location":"(write) log_w_sql_parquet (line 9537)",
            "pass":true
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws262; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_avro (line 6630)",
            "pass":true
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_orc (line 21267)",
            "pass":true
        }
    },
    "263":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot parse the INTERVAL value: 1 day 01:23:45.6789(line 1, pos 63)",
            "log_location":"(write) log_w_sql_avro (line 8814)",
            "pass":true
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws263; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_orc (line 4980)",
            "pass":true
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot parse the INTERVAL value: 1 day 01:23:45.6789(line 1, pos 63)",
            "log_location":"(write) log_w_sql_orc (line 9555)",
            "pass":true
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_avro (line 13737)",
            "pass":true
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot parse the INTERVAL value: 1 day 01:23:45.6789(line 1, pos 63)",
            "log_location":"(write) log_w_sql_avro (line 8814)",
            "pass":true
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_parquet (line 21332)",
            "pass":true
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot parse the INTERVAL value: 1 day 01:23:45.6789(line 1, pos 63)",
            "log_location":"(write) log_w_sql_parquet (line 9556)",
            "pass":true
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws263; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_parquet (line 4982)",
            "pass":true
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot parse the INTERVAL value: 1 day 01:23:45.6789(line 1, pos 63)",
            "log_location":"(write) log_w_sql_orc (line 9555)",
            "pass":true
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot parse the INTERVAL value: 1 day 01:23:45.6789(line 1, pos 63)",
            "log_location":"(write) log_w_sql_parquet (line 9556)",
            "pass":true
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws263; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_avro (line 6670)",
            "pass":true
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_orc (line 21333)",
            "pass":true
        }
    },
    "264":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Error parsing ' 123 11 day' to interval, invalid unit '11'(line 1, pos 72)",
            "log_location":"(write) log_w_sql_avro (line 8834)",
            "pass":true
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws264; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_orc (line 5020)",
            "pass":true
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Error parsing ' 123 11 day' to interval, invalid unit '11'(line 1, pos 72)",
            "log_location":"(write) log_w_sql_orc (line 9574)",
            "pass":true
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_avro (line 13803)",
            "pass":true
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Error parsing ' 123 11 day' to interval, invalid unit '11'(line 1, pos 72)",
            "log_location":"(write) log_w_sql_avro (line 8834)",
            "pass":true
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_parquet (line 21398)",
            "pass":true
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Error parsing ' 123 11 day' to interval, invalid unit '11'(line 1, pos 72)",
            "log_location":"(write) log_w_sql_parquet (line 9575)",
            "pass":true
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws264; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_parquet (line 5022)",
            "pass":true
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Error parsing ' 123 11 day' to interval, invalid unit '11'(line 1, pos 72)",
            "log_location":"(write) log_w_sql_orc (line 9574)",
            "pass":true
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Error parsing ' 123 11 day' to interval, invalid unit '11'(line 1, pos 72)",
            "log_location":"(write) log_w_sql_parquet (line 9575)",
            "pass":true
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws264; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_avro (line 6710)",
            "pass":true
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_orc (line 21399)",
            "pass":true
        }
    },
    "265":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Can only use numbers in the interval value part for multiple unit value pairs interval form, but got invalid value: 123 days hours(line 1, pos 72)",
            "log_location":"(write) log_w_sql_avro (line 8854)",
            "pass":true
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws265; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_orc (line 5060)",
            "pass":true
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Can only use numbers in the interval value part for multiple unit value pairs interval form, but got invalid value: 123 days hours(line 1, pos 72)",
            "log_location":"(write) log_w_sql_orc (line 9593)",
            "pass":true
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_avro (line 13878)",
            "pass":true
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Can only use numbers in the interval value part for multiple unit value pairs interval form, but got invalid value: 123 days hours(line 1, pos 72)",
            "log_location":"(write) log_w_sql_avro (line 8854)",
            "pass":true
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_parquet (line 21473)",
            "pass":true
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Can only use numbers in the interval value part for multiple unit value pairs interval form, but got invalid value: 123 days hours(line 1, pos 72)",
            "log_location":"(write) log_w_sql_parquet (line 9594)",
            "pass":true
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws265; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_parquet (line 5062)",
            "pass":true
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Can only use numbers in the interval value part for multiple unit value pairs interval form, but got invalid value: 123 days hours(line 1, pos 72)",
            "log_location":"(write) log_w_sql_orc (line 9593)",
            "pass":true
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Can only use numbers in the interval value part for multiple unit value pairs interval form, but got invalid value: 123 days hours(line 1, pos 72)",
            "log_location":"(write) log_w_sql_parquet (line 9594)",
            "pass":true
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws265; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_avro (line 6750)",
            "pass":true
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_orc (line 21474)",
            "pass":true
        }
    },
    "266":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot mix year-month and day-time fields: INTERVAL '10 years -11 month -12 days'(line 1, pos 76)",
            "log_location":"(write) log_w_sql_avro (line 8874)",
            "pass":true
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws266; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_orc (line 5100)",
            "pass":true
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot mix year-month and day-time fields: INTERVAL '10 years -11 month -12 days'(line 1, pos 76)",
            "log_location":"(write) log_w_sql_orc (line 9612)",
            "pass":true
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_avro (line 13959)",
            "pass":true
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot mix year-month and day-time fields: INTERVAL '10 years -11 month -12 days'(line 1, pos 76)",
            "log_location":"(write) log_w_sql_avro (line 8874)",
            "pass":true
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_parquet (line 21554)",
            "pass":true
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot mix year-month and day-time fields: INTERVAL '10 years -11 month -12 days'(line 1, pos 76)",
            "log_location":"(write) log_w_sql_parquet (line 9613)",
            "pass":true
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws266; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_parquet (line 5102)",
            "pass":true
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot mix year-month and day-time fields: INTERVAL '10 years -11 month -12 days'(line 1, pos 76)",
            "log_location":"(write) log_w_sql_orc (line 9612)",
            "pass":true
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot mix year-month and day-time fields: INTERVAL '10 years -11 month -12 days'(line 1, pos 76)",
            "log_location":"(write) log_w_sql_parquet (line 9613)",
            "pass":true
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws266; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_avro (line 6790)",
            "pass":true
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_orc (line 21555)",
            "pass":true
        }
    },
    "267":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_sql_avro (line 9177)",
            "pass":true
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws267; line 1 pos 14;",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(read) log_w_df_r_df_orc (line 5140)",
            "pass":true
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_sql_orc (line 9914)",
            "pass":true
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_df_avro (line 14025)",
            "pass":true
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_sql_avro (line 9177)",
            "pass":true
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_df_parquet (line 21620)",
            "pass":true
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_sql_parquet (line 9915)",
            "pass":true
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws267; line 1 pos 14;",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(read) log_w_df_r_df_parquet (line 5142)",
            "pass":true
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_sql_orc (line 9914)",
            "pass":true
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_sql_parquet (line 9915)",
            "pass":true
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws267; line 1 pos 14;",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(read) log_w_df_r_df_avro (line 6830)",
            "pass":true
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_df_orc (line 21621)",
            "pass":true
        }
    },
    "268":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_sql_avro (line 9713)",
            "pass":true
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws268; line 1 pos 14;",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(read) log_w_df_r_df_orc (line 5180)",
            "pass":true
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_sql_orc (line 10449)",
            "pass":true
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_df_avro (line 14086)",
            "pass":true
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_sql_avro (line 9713)",
            "pass":true
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_df_parquet (line 21681)",
            "pass":true
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_sql_parquet (line 10450)",
            "pass":true
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws268; line 1 pos 14;",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(read) log_w_df_r_df_parquet (line 5182)",
            "pass":true
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_sql_orc (line 10449)",
            "pass":true
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_sql_parquet (line 10450)",
            "pass":true
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws268; line 1 pos 14;",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(read) log_w_df_r_df_avro (line 6870)",
            "pass":true
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_df_orc (line 21682)",
            "pass":true
        }
    },
    "269":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Error in query: Literals of type 'epoch' are currently not supported for the interval second type.; line 1 pos 76",
            "log_location":"(write) log_w_sql_avro (line 9959)",
            "pass":true
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws269; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Literals of type 'epoch' are currently not supported for the interval second type.; line 1 pos 0",
            "log_location":"(read) log_w_df_r_df_orc (line 5220)",
            "pass":true
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Error in query: Literals of type 'epoch' are currently not supported for the interval second type.; line 1 pos 76",
            "log_location":"(write) log_w_sql_orc (line 10694)",
            "pass":true
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Literals of type 'epoch' are currently not supported for the interval second type.; line 1 pos 0",
            "log_location":"(write) log_w_df_avro (line 14157)",
            "pass":true
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Error in query: Literals of type 'epoch' are currently not supported for the interval second type.; line 1 pos 76",
            "log_location":"(write) log_w_sql_avro (line 9959)",
            "pass":true
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Literals of type 'epoch' are currently not supported for the interval second type.; line 1 pos 0",
            "log_location":"(write) log_w_df_parquet (line 21752)",
            "pass":true
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Error in query: Literals of type 'epoch' are currently not supported for the interval second type.; line 1 pos 76",
            "log_location":"(write) log_w_sql_parquet (line 10695)",
            "pass":true
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws269; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Literals of type 'epoch' are currently not supported for the interval second type.; line 1 pos 0",
            "log_location":"(read) log_w_df_r_df_parquet (line 5222)",
            "pass":true
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Error in query: Literals of type 'epoch' are currently not supported for the interval second type.; line 1 pos 76",
            "log_location":"(write) log_w_sql_orc (line 10694)",
            "pass":true
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Error in query: Literals of type 'epoch' are currently not supported for the interval second type.; line 1 pos 76",
            "log_location":"(write) log_w_sql_parquet (line 10695)",
            "pass":true
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws269; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Literals of type 'epoch' are currently not supported for the interval second type.; line 1 pos 0",
            "log_location":"(read) log_w_df_r_df_avro (line 6910)",
            "pass":true
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Literals of type 'epoch' are currently not supported for the interval second type.; line 1 pos 0",
            "log_location":"(write) log_w_df_orc (line 21753)",
            "pass":true
        }
    },
    "270":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Interval string does not match day-time format of `[+|-]d h`, `INTERVAL [+|-]'[+|-]d h' DAY TO HOUR` when cast to interval day to hour: 1 2:03:04, set spark.sql.legacy.fromDayTimeString.enabled to true to restore the behavior before Spark 3.0.(line 1, pos 85)",
            "log_location":"(write) log_w_sql_avro (line 9974)",
            "pass":true
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws270; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_orc (line 5260)",
            "pass":true
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Interval string does not match day-time format of `[+|-]d h`, `INTERVAL [+|-]'[+|-]d h' DAY TO HOUR` when cast to interval day to hour: 1 2:03:04, set spark.sql.legacy.fromDayTimeString.enabled to true to restore the behavior before Spark 3.0.(line 1, pos 85)",
            "log_location":"(write) log_w_sql_orc (line 10708)",
            "pass":true
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_avro (line 14272)",
            "pass":true
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Interval string does not match day-time format of `[+|-]d h`, `INTERVAL [+|-]'[+|-]d h' DAY TO HOUR` when cast to interval day to hour: 1 2:03:04, set spark.sql.legacy.fromDayTimeString.enabled to true to restore the behavior before Spark 3.0.(line 1, pos 85)",
            "log_location":"(write) log_w_sql_avro (line 9974)",
            "pass":true
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_parquet (line 21867)",
            "pass":true
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Interval string does not match day-time format of `[+|-]d h`, `INTERVAL [+|-]'[+|-]d h' DAY TO HOUR` when cast to interval day to hour: 1 2:03:04, set spark.sql.legacy.fromDayTimeString.enabled to true to restore the behavior before Spark 3.0.(line 1, pos 85)",
            "log_location":"(write) log_w_sql_parquet (line 10709)",
            "pass":true
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws270; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_parquet (line 5262)",
            "pass":true
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Interval string does not match day-time format of `[+|-]d h`, `INTERVAL [+|-]'[+|-]d h' DAY TO HOUR` when cast to interval day to hour: 1 2:03:04, set spark.sql.legacy.fromDayTimeString.enabled to true to restore the behavior before Spark 3.0.(line 1, pos 85)",
            "log_location":"(write) log_w_sql_orc (line 10708)",
            "pass":true
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Interval string does not match day-time format of `[+|-]d h`, `INTERVAL [+|-]'[+|-]d h' DAY TO HOUR` when cast to interval day to hour: 1 2:03:04, set spark.sql.legacy.fromDayTimeString.enabled to true to restore the behavior before Spark 3.0.(line 1, pos 85)",
            "log_location":"(write) log_w_sql_parquet (line 10709)",
            "pass":true
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws270; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_avro (line 6950)",
            "pass":true
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_orc (line 21868)",
            "pass":true
        }
    },
    "271":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot parse the INTERVAL value: 1-2(line 1, pos 76)",
            "log_location":"(write) log_w_sql_avro (line 9994)",
            "pass":true
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws271; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_orc (line 5300)",
            "pass":true
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot parse the INTERVAL value: 1-2(line 1, pos 76)",
            "log_location":"(write) log_w_sql_orc (line 10727)",
            "pass":true
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_avro (line 14349)",
            "pass":true
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot parse the INTERVAL value: 1-2(line 1, pos 76)",
            "log_location":"(write) log_w_sql_avro (line 9994)",
            "pass":true
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_parquet (line 21944)",
            "pass":true
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot parse the INTERVAL value: 1-2(line 1, pos 76)",
            "log_location":"(write) log_w_sql_parquet (line 10728)",
            "pass":true
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws271; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_parquet (line 5302)",
            "pass":true
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot parse the INTERVAL value: 1-2(line 1, pos 76)",
            "log_location":"(write) log_w_sql_orc (line 10727)",
            "pass":true
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot parse the INTERVAL value: 1-2(line 1, pos 76)",
            "log_location":"(write) log_w_sql_parquet (line 10728)",
            "pass":true
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws271; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_avro (line 6990)",
            "pass":true
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_orc (line 21945)",
            "pass":true
        }
    },
    "272":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot parse the INTERVAL value: 1 day 01:23:45.6789(line 1, pos 76)",
            "log_location":"(write) log_w_sql_avro (line 10014)",
            "pass":true
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws272; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_orc (line 5340)",
            "pass":true
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot parse the INTERVAL value: 1 day 01:23:45.6789(line 1, pos 76)",
            "log_location":"(write) log_w_sql_orc (line 10746)",
            "pass":true
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_avro (line 14415)",
            "pass":true
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot parse the INTERVAL value: 1 day 01:23:45.6789(line 1, pos 76)",
            "log_location":"(write) log_w_sql_avro (line 10014)",
            "pass":true
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_parquet (line 22010)",
            "pass":true
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot parse the INTERVAL value: 1 day 01:23:45.6789(line 1, pos 76)",
            "log_location":"(write) log_w_sql_parquet (line 10747)",
            "pass":true
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws272; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_parquet (line 5342)",
            "pass":true
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot parse the INTERVAL value: 1 day 01:23:45.6789(line 1, pos 76)",
            "log_location":"(write) log_w_sql_orc (line 10746)",
            "pass":true
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot parse the INTERVAL value: 1 day 01:23:45.6789(line 1, pos 76)",
            "log_location":"(write) log_w_sql_parquet (line 10747)",
            "pass":true
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws272; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_avro (line 7030)",
            "pass":true
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_orc (line 22011)",
            "pass":true
        }
    },
    "273":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Error parsing ' 123 11 day' to interval, invalid unit '11'(line 1, pos 85)",
            "log_location":"(write) log_w_sql_avro (line 10034)",
            "pass":true
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws273; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_orc (line 5380)",
            "pass":true
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Error parsing ' 123 11 day' to interval, invalid unit '11'(line 1, pos 85)",
            "log_location":"(write) log_w_sql_orc (line 10765)",
            "pass":true
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_avro (line 14481)",
            "pass":true
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Error parsing ' 123 11 day' to interval, invalid unit '11'(line 1, pos 85)",
            "log_location":"(write) log_w_sql_avro (line 10034)",
            "pass":true
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_parquet (line 22076)",
            "pass":true
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Error parsing ' 123 11 day' to interval, invalid unit '11'(line 1, pos 85)",
            "log_location":"(write) log_w_sql_parquet (line 10766)",
            "pass":true
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws273; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_parquet (line 5382)",
            "pass":true
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Error parsing ' 123 11 day' to interval, invalid unit '11'(line 1, pos 85)",
            "log_location":"(write) log_w_sql_orc (line 10765)",
            "pass":true
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Error parsing ' 123 11 day' to interval, invalid unit '11'(line 1, pos 85)",
            "log_location":"(write) log_w_sql_parquet (line 10766)",
            "pass":true
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws273; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_avro (line 7070)",
            "pass":true
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_orc (line 22077)",
            "pass":true
        }
    },
    "274":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Can only use numbers in the interval value part for multiple unit value pairs interval form, but got invalid value: 123 days hours(line 1, pos 85)",
            "log_location":"(write) log_w_sql_avro (line 10054)",
            "pass":true
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws274; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_orc (line 5420)",
            "pass":true
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Can only use numbers in the interval value part for multiple unit value pairs interval form, but got invalid value: 123 days hours(line 1, pos 85)",
            "log_location":"(write) log_w_sql_orc (line 10784)",
            "pass":true
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_avro (line 14556)",
            "pass":true
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Can only use numbers in the interval value part for multiple unit value pairs interval form, but got invalid value: 123 days hours(line 1, pos 85)",
            "log_location":"(write) log_w_sql_avro (line 10054)",
            "pass":true
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_parquet (line 22151)",
            "pass":true
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Can only use numbers in the interval value part for multiple unit value pairs interval form, but got invalid value: 123 days hours(line 1, pos 85)",
            "log_location":"(write) log_w_sql_parquet (line 10785)",
            "pass":true
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws274; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_parquet (line 5422)",
            "pass":true
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Can only use numbers in the interval value part for multiple unit value pairs interval form, but got invalid value: 123 days hours(line 1, pos 85)",
            "log_location":"(write) log_w_sql_orc (line 10784)",
            "pass":true
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Can only use numbers in the interval value part for multiple unit value pairs interval form, but got invalid value: 123 days hours(line 1, pos 85)",
            "log_location":"(write) log_w_sql_parquet (line 10785)",
            "pass":true
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws274; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_avro (line 7110)",
            "pass":true
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_orc (line 22152)",
            "pass":true
        }
    },
    "275":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot mix year-month and day-time fields: INTERVAL '10 years -11 month -12 days'(line 1, pos 77)",
            "log_location":"(write) log_w_sql_avro (line 10074)",
            "pass":true
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws275; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_orc (line 5460)",
            "pass":true
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot mix year-month and day-time fields: INTERVAL '10 years -11 month -12 days'(line 1, pos 77)",
            "log_location":"(write) log_w_sql_orc (line 10803)",
            "pass":true
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_avro (line 14637)",
            "pass":true
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot mix year-month and day-time fields: INTERVAL '10 years -11 month -12 days'(line 1, pos 77)",
            "log_location":"(write) log_w_sql_avro (line 10074)",
            "pass":true
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_parquet (line 22232)",
            "pass":true
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot mix year-month and day-time fields: INTERVAL '10 years -11 month -12 days'(line 1, pos 77)",
            "log_location":"(write) log_w_sql_parquet (line 10804)",
            "pass":true
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws275; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_parquet (line 5462)",
            "pass":true
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot mix year-month and day-time fields: INTERVAL '10 years -11 month -12 days'(line 1, pos 77)",
            "log_location":"(write) log_w_sql_orc (line 10803)",
            "pass":true
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot mix year-month and day-time fields: INTERVAL '10 years -11 month -12 days'(line 1, pos 77)",
            "log_location":"(write) log_w_sql_parquet (line 10804)",
            "pass":true
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws275; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_avro (line 7150)",
            "pass":true
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_orc (line 22233)",
            "pass":true
        }
    },
    "276":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_sql_avro (line 10377)",
            "pass":true
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws276; line 1 pos 14;",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(read) log_w_df_r_df_orc (line 5500)",
            "pass":true
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_sql_orc (line 11105)",
            "pass":true
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_df_avro (line 14703)",
            "pass":true
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_sql_avro (line 10377)",
            "pass":true
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_df_parquet (line 22298)",
            "pass":true
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_sql_parquet (line 11106)",
            "pass":true
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws276; line 1 pos 14;",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(read) log_w_df_r_df_parquet (line 5502)",
            "pass":true
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_sql_orc (line 11105)",
            "pass":true
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_sql_parquet (line 11106)",
            "pass":true
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws276; line 1 pos 14;",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(read) log_w_df_r_df_avro (line 7190)",
            "pass":true
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_df_orc (line 22299)",
            "pass":true
        }
    },
    "277":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_sql_avro (line 10913)",
            "pass":true
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws277; line 1 pos 14;",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(read) log_w_df_r_df_orc (line 5540)",
            "pass":true
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_sql_orc (line 11640)",
            "pass":true
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_df_avro (line 14764)",
            "pass":true
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_sql_avro (line 10913)",
            "pass":true
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_df_parquet (line 22359)",
            "pass":true
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_sql_parquet (line 11641)",
            "pass":true
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws277; line 1 pos 14;",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(read) log_w_df_r_df_parquet (line 5542)",
            "pass":true
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_sql_orc (line 11640)",
            "pass":true
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_sql_parquet (line 11641)",
            "pass":true
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws277; line 1 pos 14;",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(read) log_w_df_r_df_avro (line 7230)",
            "pass":true
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_df_orc (line 22360)",
            "pass":true
        }
    },
    "278":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Error in query: Literals of type 'epoch' are currently not supported for the interval second type.; line 1 pos 77",
            "log_location":"(write) log_w_sql_avro (line 11159)",
            "pass":true
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws278; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Literals of type 'epoch' are currently not supported for the interval second type.; line 1 pos 0",
            "log_location":"(read) log_w_df_r_df_orc (line 5580)",
            "pass":true
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Error in query: Literals of type 'epoch' are currently not supported for the interval second type.; line 1 pos 77",
            "log_location":"(write) log_w_sql_orc (line 11885)",
            "pass":true
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Literals of type 'epoch' are currently not supported for the interval second type.; line 1 pos 0",
            "log_location":"(write) log_w_df_avro (line 14835)",
            "pass":true
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Error in query: Literals of type 'epoch' are currently not supported for the interval second type.; line 1 pos 77",
            "log_location":"(write) log_w_sql_avro (line 11159)",
            "pass":true
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Literals of type 'epoch' are currently not supported for the interval second type.; line 1 pos 0",
            "log_location":"(write) log_w_df_parquet (line 22430)",
            "pass":true
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Error in query: Literals of type 'epoch' are currently not supported for the interval second type.; line 1 pos 77",
            "log_location":"(write) log_w_sql_parquet (line 11886)",
            "pass":true
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws278; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Literals of type 'epoch' are currently not supported for the interval second type.; line 1 pos 0",
            "log_location":"(read) log_w_df_r_df_parquet (line 5582)",
            "pass":true
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Error in query: Literals of type 'epoch' are currently not supported for the interval second type.; line 1 pos 77",
            "log_location":"(write) log_w_sql_orc (line 11885)",
            "pass":true
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Error in query: Literals of type 'epoch' are currently not supported for the interval second type.; line 1 pos 77",
            "log_location":"(write) log_w_sql_parquet (line 11886)",
            "pass":true
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws278; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Literals of type 'epoch' are currently not supported for the interval second type.; line 1 pos 0",
            "log_location":"(read) log_w_df_r_df_avro (line 7270)",
            "pass":true
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Literals of type 'epoch' are currently not supported for the interval second type.; line 1 pos 0",
            "log_location":"(write) log_w_df_orc (line 22431)",
            "pass":true
        }
    },
    "279":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Interval string does not match day-time format of `[+|-]d h`, `INTERVAL [+|-]'[+|-]d h' DAY TO HOUR` when cast to interval day to hour: 1 2:03:04, set spark.sql.legacy.fromDayTimeString.enabled to true to restore the behavior before Spark 3.0.(line 1, pos 86)",
            "log_location":"(write) log_w_sql_avro (line 11174)",
            "pass":true
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws279; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_orc (line 5620)",
            "pass":true
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Interval string does not match day-time format of `[+|-]d h`, `INTERVAL [+|-]'[+|-]d h' DAY TO HOUR` when cast to interval day to hour: 1 2:03:04, set spark.sql.legacy.fromDayTimeString.enabled to true to restore the behavior before Spark 3.0.(line 1, pos 86)",
            "log_location":"(write) log_w_sql_orc (line 11899)",
            "pass":true
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_avro (line 14950)",
            "pass":true
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Interval string does not match day-time format of `[+|-]d h`, `INTERVAL [+|-]'[+|-]d h' DAY TO HOUR` when cast to interval day to hour: 1 2:03:04, set spark.sql.legacy.fromDayTimeString.enabled to true to restore the behavior before Spark 3.0.(line 1, pos 86)",
            "log_location":"(write) log_w_sql_avro (line 11174)",
            "pass":true
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_parquet (line 22545)",
            "pass":true
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Interval string does not match day-time format of `[+|-]d h`, `INTERVAL [+|-]'[+|-]d h' DAY TO HOUR` when cast to interval day to hour: 1 2:03:04, set spark.sql.legacy.fromDayTimeString.enabled to true to restore the behavior before Spark 3.0.(line 1, pos 86)",
            "log_location":"(write) log_w_sql_parquet (line 11900)",
            "pass":true
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws279; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_parquet (line 5622)",
            "pass":true
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Interval string does not match day-time format of `[+|-]d h`, `INTERVAL [+|-]'[+|-]d h' DAY TO HOUR` when cast to interval day to hour: 1 2:03:04, set spark.sql.legacy.fromDayTimeString.enabled to true to restore the behavior before Spark 3.0.(line 1, pos 86)",
            "log_location":"(write) log_w_sql_orc (line 11899)",
            "pass":true
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Interval string does not match day-time format of `[+|-]d h`, `INTERVAL [+|-]'[+|-]d h' DAY TO HOUR` when cast to interval day to hour: 1 2:03:04, set spark.sql.legacy.fromDayTimeString.enabled to true to restore the behavior before Spark 3.0.(line 1, pos 86)",
            "log_location":"(write) log_w_sql_parquet (line 11900)",
            "pass":true
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws279; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_avro (line 7310)",
            "pass":true
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_orc (line 22546)",
            "pass":true
        }
    },
    "280":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot parse the INTERVAL value: 1-2(line 1, pos 77)",
            "log_location":"(write) log_w_sql_avro (line 11194)",
            "pass":true
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws280; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_orc (line 5660)",
            "pass":true
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot parse the INTERVAL value: 1-2(line 1, pos 77)",
            "log_location":"(write) log_w_sql_orc (line 11918)",
            "pass":true
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_avro (line 15027)",
            "pass":true
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot parse the INTERVAL value: 1-2(line 1, pos 77)",
            "log_location":"(write) log_w_sql_avro (line 11194)",
            "pass":true
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_parquet (line 22622)",
            "pass":true
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot parse the INTERVAL value: 1-2(line 1, pos 77)",
            "log_location":"(write) log_w_sql_parquet (line 11919)",
            "pass":true
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws280; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_parquet (line 5662)",
            "pass":true
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot parse the INTERVAL value: 1-2(line 1, pos 77)",
            "log_location":"(write) log_w_sql_orc (line 11918)",
            "pass":true
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot parse the INTERVAL value: 1-2(line 1, pos 77)",
            "log_location":"(write) log_w_sql_parquet (line 11919)",
            "pass":true
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws280; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_avro (line 7350)",
            "pass":true
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_orc (line 22623)",
            "pass":true
        }
    },
    "281":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot parse the INTERVAL value: 1 day 01:23:45.6789(line 1, pos 77)",
            "log_location":"(write) log_w_sql_avro (line 11214)",
            "pass":true
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws281; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_orc (line 5700)",
            "pass":true
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot parse the INTERVAL value: 1 day 01:23:45.6789(line 1, pos 77)",
            "log_location":"(write) log_w_sql_orc (line 11937)",
            "pass":true
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_avro (line 15093)",
            "pass":true
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot parse the INTERVAL value: 1 day 01:23:45.6789(line 1, pos 77)",
            "log_location":"(write) log_w_sql_avro (line 11214)",
            "pass":true
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_parquet (line 22688)",
            "pass":true
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot parse the INTERVAL value: 1 day 01:23:45.6789(line 1, pos 77)",
            "log_location":"(write) log_w_sql_parquet (line 11938)",
            "pass":true
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws281; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_parquet (line 5702)",
            "pass":true
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot parse the INTERVAL value: 1 day 01:23:45.6789(line 1, pos 77)",
            "log_location":"(write) log_w_sql_orc (line 11937)",
            "pass":true
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot parse the INTERVAL value: 1 day 01:23:45.6789(line 1, pos 77)",
            "log_location":"(write) log_w_sql_parquet (line 11938)",
            "pass":true
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws281; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_avro (line 7390)",
            "pass":true
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_orc (line 22689)",
            "pass":true
        }
    },
    "282":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Error parsing ' 123 11 day' to interval, invalid unit '11'(line 1, pos 86)",
            "log_location":"(write) log_w_sql_avro (line 11234)",
            "pass":true
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws282; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_orc (line 5740)",
            "pass":true
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Error parsing ' 123 11 day' to interval, invalid unit '11'(line 1, pos 86)",
            "log_location":"(write) log_w_sql_orc (line 11956)",
            "pass":true
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_avro (line 15159)",
            "pass":true
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Error parsing ' 123 11 day' to interval, invalid unit '11'(line 1, pos 86)",
            "log_location":"(write) log_w_sql_avro (line 11234)",
            "pass":true
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_parquet (line 22754)",
            "pass":true
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Error parsing ' 123 11 day' to interval, invalid unit '11'(line 1, pos 86)",
            "log_location":"(write) log_w_sql_parquet (line 11957)",
            "pass":true
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws282; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_parquet (line 5742)",
            "pass":true
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Error parsing ' 123 11 day' to interval, invalid unit '11'(line 1, pos 86)",
            "log_location":"(write) log_w_sql_orc (line 11956)",
            "pass":true
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Error parsing ' 123 11 day' to interval, invalid unit '11'(line 1, pos 86)",
            "log_location":"(write) log_w_sql_parquet (line 11957)",
            "pass":true
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws282; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_avro (line 7430)",
            "pass":true
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_orc (line 22755)",
            "pass":true
        }
    },
    "283":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Can only use numbers in the interval value part for multiple unit value pairs interval form, but got invalid value: 123 days hours(line 1, pos 86)",
            "log_location":"(write) log_w_sql_avro (line 11254)",
            "pass":true
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws283; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_orc (line 5780)",
            "pass":true
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Can only use numbers in the interval value part for multiple unit value pairs interval form, but got invalid value: 123 days hours(line 1, pos 86)",
            "log_location":"(write) log_w_sql_orc (line 11975)",
            "pass":true
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_avro (line 15234)",
            "pass":true
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Can only use numbers in the interval value part for multiple unit value pairs interval form, but got invalid value: 123 days hours(line 1, pos 86)",
            "log_location":"(write) log_w_sql_avro (line 11254)",
            "pass":true
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_parquet (line 22829)",
            "pass":true
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Can only use numbers in the interval value part for multiple unit value pairs interval form, but got invalid value: 123 days hours(line 1, pos 86)",
            "log_location":"(write) log_w_sql_parquet (line 11976)",
            "pass":true
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws283; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_parquet (line 5782)",
            "pass":true
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Can only use numbers in the interval value part for multiple unit value pairs interval form, but got invalid value: 123 days hours(line 1, pos 86)",
            "log_location":"(write) log_w_sql_orc (line 11975)",
            "pass":true
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Can only use numbers in the interval value part for multiple unit value pairs interval form, but got invalid value: 123 days hours(line 1, pos 86)",
            "log_location":"(write) log_w_sql_parquet (line 11976)",
            "pass":true
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws283; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_avro (line 7470)",
            "pass":true
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_orc (line 22830)",
            "pass":true
        }
    },
    "284":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot mix year-month and day-time fields: INTERVAL '10 years -11 month -12 days'(line 1, pos 79)",
            "log_location":"(write) log_w_sql_avro (line 11274)",
            "pass":true
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws284; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_orc (line 5820)",
            "pass":true
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot mix year-month and day-time fields: INTERVAL '10 years -11 month -12 days'(line 1, pos 79)",
            "log_location":"(write) log_w_sql_orc (line 11994)",
            "pass":true
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_avro (line 15315)",
            "pass":true
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot mix year-month and day-time fields: INTERVAL '10 years -11 month -12 days'(line 1, pos 79)",
            "log_location":"(write) log_w_sql_avro (line 11274)",
            "pass":true
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_parquet (line 22910)",
            "pass":true
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot mix year-month and day-time fields: INTERVAL '10 years -11 month -12 days'(line 1, pos 79)",
            "log_location":"(write) log_w_sql_parquet (line 11995)",
            "pass":true
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws284; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_parquet (line 5822)",
            "pass":true
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot mix year-month and day-time fields: INTERVAL '10 years -11 month -12 days'(line 1, pos 79)",
            "log_location":"(write) log_w_sql_orc (line 11994)",
            "pass":true
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot mix year-month and day-time fields: INTERVAL '10 years -11 month -12 days'(line 1, pos 79)",
            "log_location":"(write) log_w_sql_parquet (line 11995)",
            "pass":true
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws284; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_avro (line 7510)",
            "pass":true
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_orc (line 22911)",
            "pass":true
        }
    },
    "285":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_sql_avro (line 11577)",
            "pass":true
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws285; line 1 pos 14;",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(read) log_w_df_r_df_orc (line 5860)",
            "pass":true
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_sql_orc (line 12296)",
            "pass":true
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_df_avro (line 15381)",
            "pass":true
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_sql_avro (line 11577)",
            "pass":true
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_df_parquet (line 22976)",
            "pass":true
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_sql_parquet (line 12297)",
            "pass":true
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws285; line 1 pos 14;",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(read) log_w_df_r_df_parquet (line 5862)",
            "pass":true
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_sql_orc (line 12296)",
            "pass":true
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_sql_parquet (line 12297)",
            "pass":true
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws285; line 1 pos 14;",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(read) log_w_df_r_df_avro (line 7550)",
            "pass":true
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_df_orc (line 22977)",
            "pass":true
        }
    },
    "286":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_sql_avro (line 12113)",
            "pass":true
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws286; line 1 pos 14;",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(read) log_w_df_r_df_orc (line 5900)",
            "pass":true
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_sql_orc (line 12831)",
            "pass":true
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_df_avro (line 15442)",
            "pass":true
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_sql_avro (line 12113)",
            "pass":true
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_df_parquet (line 23037)",
            "pass":true
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_sql_parquet (line 12832)",
            "pass":true
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws286; line 1 pos 14;",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(read) log_w_df_r_df_parquet (line 5902)",
            "pass":true
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_sql_orc (line 12831)",
            "pass":true
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_sql_parquet (line 12832)",
            "pass":true
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws286; line 1 pos 14;",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(read) log_w_df_r_df_avro (line 7590)",
            "pass":true
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_df_orc (line 23038)",
            "pass":true
        }
    },
    "287":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Error in query: Literals of type 'epoch' are currently not supported for the interval second type.; line 1 pos 79",
            "log_location":"(write) log_w_sql_avro (line 12359)",
            "pass":true
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws287; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Literals of type 'epoch' are currently not supported for the interval second type.; line 1 pos 0",
            "log_location":"(read) log_w_df_r_df_orc (line 5940)",
            "pass":true
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Error in query: Literals of type 'epoch' are currently not supported for the interval second type.; line 1 pos 79",
            "log_location":"(write) log_w_sql_orc (line 13076)",
            "pass":true
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Literals of type 'epoch' are currently not supported for the interval second type.; line 1 pos 0",
            "log_location":"(write) log_w_df_avro (line 15513)",
            "pass":true
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Error in query: Literals of type 'epoch' are currently not supported for the interval second type.; line 1 pos 79",
            "log_location":"(write) log_w_sql_avro (line 12359)",
            "pass":true
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Literals of type 'epoch' are currently not supported for the interval second type.; line 1 pos 0",
            "log_location":"(write) log_w_df_parquet (line 23108)",
            "pass":true
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Error in query: Literals of type 'epoch' are currently not supported for the interval second type.; line 1 pos 79",
            "log_location":"(write) log_w_sql_parquet (line 13077)",
            "pass":true
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws287; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Literals of type 'epoch' are currently not supported for the interval second type.; line 1 pos 0",
            "log_location":"(read) log_w_df_r_df_parquet (line 5942)",
            "pass":true
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Error in query: Literals of type 'epoch' are currently not supported for the interval second type.; line 1 pos 79",
            "log_location":"(write) log_w_sql_orc (line 13076)",
            "pass":true
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Error in query: Literals of type 'epoch' are currently not supported for the interval second type.; line 1 pos 79",
            "log_location":"(write) log_w_sql_parquet (line 13077)",
            "pass":true
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws287; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Literals of type 'epoch' are currently not supported for the interval second type.; line 1 pos 0",
            "log_location":"(read) log_w_df_r_df_avro (line 7630)",
            "pass":true
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Literals of type 'epoch' are currently not supported for the interval second type.; line 1 pos 0",
            "log_location":"(write) log_w_df_orc (line 23109)",
            "pass":true
        }
    },
    "288":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Interval string does not match day-time format of `[+|-]d h`, `INTERVAL [+|-]'[+|-]d h' DAY TO HOUR` when cast to interval day to hour: 1 2:03:04, set spark.sql.legacy.fromDayTimeString.enabled to true to restore the behavior before Spark 3.0.(line 1, pos 88)",
            "log_location":"(write) log_w_sql_avro (line 12374)",
            "pass":true
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws288; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_orc (line 5980)",
            "pass":true
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Interval string does not match day-time format of `[+|-]d h`, `INTERVAL [+|-]'[+|-]d h' DAY TO HOUR` when cast to interval day to hour: 1 2:03:04, set spark.sql.legacy.fromDayTimeString.enabled to true to restore the behavior before Spark 3.0.(line 1, pos 88)",
            "log_location":"(write) log_w_sql_orc (line 13090)",
            "pass":true
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_avro (line 15628)",
            "pass":true
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Interval string does not match day-time format of `[+|-]d h`, `INTERVAL [+|-]'[+|-]d h' DAY TO HOUR` when cast to interval day to hour: 1 2:03:04, set spark.sql.legacy.fromDayTimeString.enabled to true to restore the behavior before Spark 3.0.(line 1, pos 88)",
            "log_location":"(write) log_w_sql_avro (line 12374)",
            "pass":true
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_parquet (line 23223)",
            "pass":true
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Interval string does not match day-time format of `[+|-]d h`, `INTERVAL [+|-]'[+|-]d h' DAY TO HOUR` when cast to interval day to hour: 1 2:03:04, set spark.sql.legacy.fromDayTimeString.enabled to true to restore the behavior before Spark 3.0.(line 1, pos 88)",
            "log_location":"(write) log_w_sql_parquet (line 13091)",
            "pass":true
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws288; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_parquet (line 5982)",
            "pass":true
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Interval string does not match day-time format of `[+|-]d h`, `INTERVAL [+|-]'[+|-]d h' DAY TO HOUR` when cast to interval day to hour: 1 2:03:04, set spark.sql.legacy.fromDayTimeString.enabled to true to restore the behavior before Spark 3.0.(line 1, pos 88)",
            "log_location":"(write) log_w_sql_orc (line 13090)",
            "pass":true
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Interval string does not match day-time format of `[+|-]d h`, `INTERVAL [+|-]'[+|-]d h' DAY TO HOUR` when cast to interval day to hour: 1 2:03:04, set spark.sql.legacy.fromDayTimeString.enabled to true to restore the behavior before Spark 3.0.(line 1, pos 88)",
            "log_location":"(write) log_w_sql_parquet (line 13091)",
            "pass":true
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws288; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_avro (line 7670)",
            "pass":true
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_orc (line 23224)",
            "pass":true
        }
    },
    "289":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot parse the INTERVAL value: 1-2(line 1, pos 79)",
            "log_location":"(write) log_w_sql_avro (line 12394)",
            "pass":true
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws289; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_orc (line 6020)",
            "pass":true
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot parse the INTERVAL value: 1-2(line 1, pos 79)",
            "log_location":"(write) log_w_sql_orc (line 13109)",
            "pass":true
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_avro (line 15705)",
            "pass":true
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot parse the INTERVAL value: 1-2(line 1, pos 79)",
            "log_location":"(write) log_w_sql_avro (line 12394)",
            "pass":true
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_parquet (line 23300)",
            "pass":true
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot parse the INTERVAL value: 1-2(line 1, pos 79)",
            "log_location":"(write) log_w_sql_parquet (line 13110)",
            "pass":true
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws289; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_parquet (line 6022)",
            "pass":true
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot parse the INTERVAL value: 1-2(line 1, pos 79)",
            "log_location":"(write) log_w_sql_orc (line 13109)",
            "pass":true
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot parse the INTERVAL value: 1-2(line 1, pos 79)",
            "log_location":"(write) log_w_sql_parquet (line 13110)",
            "pass":true
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws289; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_avro (line 7710)",
            "pass":true
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_orc (line 23301)",
            "pass":true
        }
    },
    "290":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot parse the INTERVAL value: 1 day 01:23:45.6789(line 1, pos 79)",
            "log_location":"(write) log_w_sql_avro (line 12414)",
            "pass":true
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws290; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_orc (line 6060)",
            "pass":true
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot parse the INTERVAL value: 1 day 01:23:45.6789(line 1, pos 79)",
            "log_location":"(write) log_w_sql_orc (line 13128)",
            "pass":true
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_avro (line 15771)",
            "pass":true
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot parse the INTERVAL value: 1 day 01:23:45.6789(line 1, pos 79)",
            "log_location":"(write) log_w_sql_avro (line 12414)",
            "pass":true
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_parquet (line 23366)",
            "pass":true
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot parse the INTERVAL value: 1 day 01:23:45.6789(line 1, pos 79)",
            "log_location":"(write) log_w_sql_parquet (line 13129)",
            "pass":true
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws290; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_parquet (line 6062)",
            "pass":true
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot parse the INTERVAL value: 1 day 01:23:45.6789(line 1, pos 79)",
            "log_location":"(write) log_w_sql_orc (line 13128)",
            "pass":true
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot parse the INTERVAL value: 1 day 01:23:45.6789(line 1, pos 79)",
            "log_location":"(write) log_w_sql_parquet (line 13129)",
            "pass":true
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws290; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_avro (line 7750)",
            "pass":true
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_orc (line 23367)",
            "pass":true
        }
    },
    "291":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Error parsing ' 123 11 day' to interval, invalid unit '11'(line 1, pos 88)",
            "log_location":"(write) log_w_sql_avro (line 12434)",
            "pass":true
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws291; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_orc (line 6100)",
            "pass":true
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Error parsing ' 123 11 day' to interval, invalid unit '11'(line 1, pos 88)",
            "log_location":"(write) log_w_sql_orc (line 13147)",
            "pass":true
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_avro (line 15837)",
            "pass":true
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Error parsing ' 123 11 day' to interval, invalid unit '11'(line 1, pos 88)",
            "log_location":"(write) log_w_sql_avro (line 12434)",
            "pass":true
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_parquet (line 23432)",
            "pass":true
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Error parsing ' 123 11 day' to interval, invalid unit '11'(line 1, pos 88)",
            "log_location":"(write) log_w_sql_parquet (line 13148)",
            "pass":true
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws291; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_parquet (line 6102)",
            "pass":true
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Error parsing ' 123 11 day' to interval, invalid unit '11'(line 1, pos 88)",
            "log_location":"(write) log_w_sql_orc (line 13147)",
            "pass":true
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Error parsing ' 123 11 day' to interval, invalid unit '11'(line 1, pos 88)",
            "log_location":"(write) log_w_sql_parquet (line 13148)",
            "pass":true
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws291; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_avro (line 7790)",
            "pass":true
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_orc (line 23433)",
            "pass":true
        }
    },
    "292":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Can only use numbers in the interval value part for multiple unit value pairs interval form, but got invalid value: 123 days hours(line 1, pos 88)",
            "log_location":"(write) log_w_sql_avro (line 12454)",
            "pass":true
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws292; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_orc (line 6140)",
            "pass":true
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Can only use numbers in the interval value part for multiple unit value pairs interval form, but got invalid value: 123 days hours(line 1, pos 88)",
            "log_location":"(write) log_w_sql_orc (line 13166)",
            "pass":true
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_avro (line 15912)",
            "pass":true
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Can only use numbers in the interval value part for multiple unit value pairs interval form, but got invalid value: 123 days hours(line 1, pos 88)",
            "log_location":"(write) log_w_sql_avro (line 12454)",
            "pass":true
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_parquet (line 23507)",
            "pass":true
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Can only use numbers in the interval value part for multiple unit value pairs interval form, but got invalid value: 123 days hours(line 1, pos 88)",
            "log_location":"(write) log_w_sql_parquet (line 13167)",
            "pass":true
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws292; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_parquet (line 6142)",
            "pass":true
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Can only use numbers in the interval value part for multiple unit value pairs interval form, but got invalid value: 123 days hours(line 1, pos 88)",
            "log_location":"(write) log_w_sql_orc (line 13166)",
            "pass":true
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Can only use numbers in the interval value part for multiple unit value pairs interval form, but got invalid value: 123 days hours(line 1, pos 88)",
            "log_location":"(write) log_w_sql_parquet (line 13167)",
            "pass":true
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws292; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_avro (line 7830)",
            "pass":true
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_orc (line 23508)",
            "pass":true
        }
    },
    "293":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot mix year-month and day-time fields: INTERVAL '10 years -11 month -12 days'(line 1, pos 79)",
            "log_location":"(write) log_w_sql_avro (line 12474)",
            "pass":true
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws293; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_orc (line 6180)",
            "pass":true
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot mix year-month and day-time fields: INTERVAL '10 years -11 month -12 days'(line 1, pos 79)",
            "log_location":"(write) log_w_sql_orc (line 13185)",
            "pass":true
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_avro (line 15993)",
            "pass":true
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot mix year-month and day-time fields: INTERVAL '10 years -11 month -12 days'(line 1, pos 79)",
            "log_location":"(write) log_w_sql_avro (line 12474)",
            "pass":true
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_parquet (line 23588)",
            "pass":true
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot mix year-month and day-time fields: INTERVAL '10 years -11 month -12 days'(line 1, pos 79)",
            "log_location":"(write) log_w_sql_parquet (line 13186)",
            "pass":true
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws293; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_parquet (line 6182)",
            "pass":true
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot mix year-month and day-time fields: INTERVAL '10 years -11 month -12 days'(line 1, pos 79)",
            "log_location":"(write) log_w_sql_orc (line 13185)",
            "pass":true
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot mix year-month and day-time fields: INTERVAL '10 years -11 month -12 days'(line 1, pos 79)",
            "log_location":"(write) log_w_sql_parquet (line 13186)",
            "pass":true
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws293; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_avro (line 7870)",
            "pass":true
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_orc (line 23589)",
            "pass":true
        }
    },
    "294":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_sql_avro (line 12777)",
            "pass":true
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws294; line 1 pos 14;",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(read) log_w_df_r_df_orc (line 6220)",
            "pass":true
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_sql_orc (line 13487)",
            "pass":true
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_df_avro (line 16059)",
            "pass":true
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_sql_avro (line 12777)",
            "pass":true
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_df_parquet (line 23654)",
            "pass":true
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_sql_parquet (line 13488)",
            "pass":true
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws294; line 1 pos 14;",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(read) log_w_df_r_df_parquet (line 6222)",
            "pass":true
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_sql_orc (line 13487)",
            "pass":true
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_sql_parquet (line 13488)",
            "pass":true
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws294; line 1 pos 14;",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(read) log_w_df_r_df_avro (line 7910)",
            "pass":true
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_df_orc (line 23655)",
            "pass":true
        }
    },
    "295":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_sql_avro (line 13313)",
            "pass":true
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws295; line 1 pos 14;",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(read) log_w_df_r_df_orc (line 6260)",
            "pass":true
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_sql_orc (line 14022)",
            "pass":true
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_df_avro (line 16120)",
            "pass":true
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_sql_avro (line 13313)",
            "pass":true
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_df_parquet (line 23715)",
            "pass":true
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_sql_parquet (line 14023)",
            "pass":true
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws295; line 1 pos 14;",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(read) log_w_df_r_df_parquet (line 6262)",
            "pass":true
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_sql_orc (line 14022)",
            "pass":true
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_sql_parquet (line 14023)",
            "pass":true
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws295; line 1 pos 14;",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(read) log_w_df_r_df_avro (line 7950)",
            "pass":true
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_df_orc (line 23716)",
            "pass":true
        }
    },
    "296":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Error in query: Literals of type 'epoch' are currently not supported for the interval second type.; line 1 pos 79",
            "log_location":"(write) log_w_sql_avro (line 13559)",
            "pass":true
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws296; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Literals of type 'epoch' are currently not supported for the interval second type.; line 1 pos 0",
            "log_location":"(read) log_w_df_r_df_orc (line 6300)",
            "pass":true
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Error in query: Literals of type 'epoch' are currently not supported for the interval second type.; line 1 pos 79",
            "log_location":"(write) log_w_sql_orc (line 14267)",
            "pass":true
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Literals of type 'epoch' are currently not supported for the interval second type.; line 1 pos 0",
            "log_location":"(write) log_w_df_avro (line 16191)",
            "pass":true
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Error in query: Literals of type 'epoch' are currently not supported for the interval second type.; line 1 pos 79",
            "log_location":"(write) log_w_sql_avro (line 13559)",
            "pass":true
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Literals of type 'epoch' are currently not supported for the interval second type.; line 1 pos 0",
            "log_location":"(write) log_w_df_parquet (line 23786)",
            "pass":true
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Error in query: Literals of type 'epoch' are currently not supported for the interval second type.; line 1 pos 79",
            "log_location":"(write) log_w_sql_parquet (line 14268)",
            "pass":true
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws296; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Literals of type 'epoch' are currently not supported for the interval second type.; line 1 pos 0",
            "log_location":"(read) log_w_df_r_df_parquet (line 6302)",
            "pass":true
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Error in query: Literals of type 'epoch' are currently not supported for the interval second type.; line 1 pos 79",
            "log_location":"(write) log_w_sql_orc (line 14267)",
            "pass":true
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Error in query: Literals of type 'epoch' are currently not supported for the interval second type.; line 1 pos 79",
            "log_location":"(write) log_w_sql_parquet (line 14268)",
            "pass":true
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws296; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Literals of type 'epoch' are currently not supported for the interval second type.; line 1 pos 0",
            "log_location":"(read) log_w_df_r_df_avro (line 7990)",
            "pass":true
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Literals of type 'epoch' are currently not supported for the interval second type.; line 1 pos 0",
            "log_location":"(write) log_w_df_orc (line 23787)",
            "pass":true
        }
    },
    "297":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Interval string does not match day-time format of `[+|-]d h`, `INTERVAL [+|-]'[+|-]d h' DAY TO HOUR` when cast to interval day to hour: 1 2:03:04, set spark.sql.legacy.fromDayTimeString.enabled to true to restore the behavior before Spark 3.0.(line 1, pos 88)",
            "log_location":"(write) log_w_sql_avro (line 13574)",
            "pass":true
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws297; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_orc (line 6340)",
            "pass":true
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Interval string does not match day-time format of `[+|-]d h`, `INTERVAL [+|-]'[+|-]d h' DAY TO HOUR` when cast to interval day to hour: 1 2:03:04, set spark.sql.legacy.fromDayTimeString.enabled to true to restore the behavior before Spark 3.0.(line 1, pos 88)",
            "log_location":"(write) log_w_sql_orc (line 14281)",
            "pass":true
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_avro (line 16306)",
            "pass":true
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Interval string does not match day-time format of `[+|-]d h`, `INTERVAL [+|-]'[+|-]d h' DAY TO HOUR` when cast to interval day to hour: 1 2:03:04, set spark.sql.legacy.fromDayTimeString.enabled to true to restore the behavior before Spark 3.0.(line 1, pos 88)",
            "log_location":"(write) log_w_sql_avro (line 13574)",
            "pass":true
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_parquet (line 23901)",
            "pass":true
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Interval string does not match day-time format of `[+|-]d h`, `INTERVAL [+|-]'[+|-]d h' DAY TO HOUR` when cast to interval day to hour: 1 2:03:04, set spark.sql.legacy.fromDayTimeString.enabled to true to restore the behavior before Spark 3.0.(line 1, pos 88)",
            "log_location":"(write) log_w_sql_parquet (line 14282)",
            "pass":true
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws297; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_parquet (line 6342)",
            "pass":true
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Interval string does not match day-time format of `[+|-]d h`, `INTERVAL [+|-]'[+|-]d h' DAY TO HOUR` when cast to interval day to hour: 1 2:03:04, set spark.sql.legacy.fromDayTimeString.enabled to true to restore the behavior before Spark 3.0.(line 1, pos 88)",
            "log_location":"(write) log_w_sql_orc (line 14281)",
            "pass":true
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Interval string does not match day-time format of `[+|-]d h`, `INTERVAL [+|-]'[+|-]d h' DAY TO HOUR` when cast to interval day to hour: 1 2:03:04, set spark.sql.legacy.fromDayTimeString.enabled to true to restore the behavior before Spark 3.0.(line 1, pos 88)",
            "log_location":"(write) log_w_sql_parquet (line 14282)",
            "pass":true
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws297; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_avro (line 8030)",
            "pass":true
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_orc (line 23902)",
            "pass":true
        }
    },
    "298":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot parse the INTERVAL value: 1-2(line 1, pos 79)",
            "log_location":"(write) log_w_sql_avro (line 13594)",
            "pass":true
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws298; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_orc (line 6380)",
            "pass":true
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot parse the INTERVAL value: 1-2(line 1, pos 79)",
            "log_location":"(write) log_w_sql_orc (line 14300)",
            "pass":true
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_avro (line 16383)",
            "pass":true
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot parse the INTERVAL value: 1-2(line 1, pos 79)",
            "log_location":"(write) log_w_sql_avro (line 13594)",
            "pass":true
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_parquet (line 23978)",
            "pass":true
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot parse the INTERVAL value: 1-2(line 1, pos 79)",
            "log_location":"(write) log_w_sql_parquet (line 14301)",
            "pass":true
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws298; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_parquet (line 6382)",
            "pass":true
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot parse the INTERVAL value: 1-2(line 1, pos 79)",
            "log_location":"(write) log_w_sql_orc (line 14300)",
            "pass":true
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot parse the INTERVAL value: 1-2(line 1, pos 79)",
            "log_location":"(write) log_w_sql_parquet (line 14301)",
            "pass":true
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws298; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_avro (line 8070)",
            "pass":true
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_orc (line 23979)",
            "pass":true
        }
    },
    "299":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot parse the INTERVAL value: 1 day 01:23:45.6789(line 1, pos 79)",
            "log_location":"(write) log_w_sql_avro (line 13614)",
            "pass":true
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws299; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_orc (line 6420)",
            "pass":true
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot parse the INTERVAL value: 1 day 01:23:45.6789(line 1, pos 79)",
            "log_location":"(write) log_w_sql_orc (line 14319)",
            "pass":true
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_avro (line 16449)",
            "pass":true
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot parse the INTERVAL value: 1 day 01:23:45.6789(line 1, pos 79)",
            "log_location":"(write) log_w_sql_avro (line 13614)",
            "pass":true
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_parquet (line 24044)",
            "pass":true
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot parse the INTERVAL value: 1 day 01:23:45.6789(line 1, pos 79)",
            "log_location":"(write) log_w_sql_parquet (line 14320)",
            "pass":true
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws299; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_parquet (line 6422)",
            "pass":true
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot parse the INTERVAL value: 1 day 01:23:45.6789(line 1, pos 79)",
            "log_location":"(write) log_w_sql_orc (line 14319)",
            "pass":true
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot parse the INTERVAL value: 1 day 01:23:45.6789(line 1, pos 79)",
            "log_location":"(write) log_w_sql_parquet (line 14320)",
            "pass":true
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws299; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_avro (line 8110)",
            "pass":true
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_orc (line 24045)",
            "pass":true
        }
    },
    "300":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Error parsing ' 123 11 day' to interval, invalid unit '11'(line 1, pos 88)",
            "log_location":"(write) log_w_sql_avro (line 13634)",
            "pass":true
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws300; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_orc (line 6460)",
            "pass":true
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Error parsing ' 123 11 day' to interval, invalid unit '11'(line 1, pos 88)",
            "log_location":"(write) log_w_sql_orc (line 14338)",
            "pass":true
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_avro (line 16515)",
            "pass":true
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Error parsing ' 123 11 day' to interval, invalid unit '11'(line 1, pos 88)",
            "log_location":"(write) log_w_sql_avro (line 13634)",
            "pass":true
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_parquet (line 24110)",
            "pass":true
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Error parsing ' 123 11 day' to interval, invalid unit '11'(line 1, pos 88)",
            "log_location":"(write) log_w_sql_parquet (line 14339)",
            "pass":true
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws300; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_parquet (line 6462)",
            "pass":true
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Error parsing ' 123 11 day' to interval, invalid unit '11'(line 1, pos 88)",
            "log_location":"(write) log_w_sql_orc (line 14338)",
            "pass":true
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Error parsing ' 123 11 day' to interval, invalid unit '11'(line 1, pos 88)",
            "log_location":"(write) log_w_sql_parquet (line 14339)",
            "pass":true
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws300; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_avro (line 8150)",
            "pass":true
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_orc (line 24111)",
            "pass":true
        }
    },
    "301":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Can only use numbers in the interval value part for multiple unit value pairs interval form, but got invalid value: 123 days hours(line 1, pos 88)",
            "log_location":"(write) log_w_sql_avro (line 13654)",
            "pass":true
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws301; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_orc (line 6500)",
            "pass":true
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Can only use numbers in the interval value part for multiple unit value pairs interval form, but got invalid value: 123 days hours(line 1, pos 88)",
            "log_location":"(write) log_w_sql_orc (line 14357)",
            "pass":true
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_avro (line 16590)",
            "pass":true
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Can only use numbers in the interval value part for multiple unit value pairs interval form, but got invalid value: 123 days hours(line 1, pos 88)",
            "log_location":"(write) log_w_sql_avro (line 13654)",
            "pass":true
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_parquet (line 24185)",
            "pass":true
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Can only use numbers in the interval value part for multiple unit value pairs interval form, but got invalid value: 123 days hours(line 1, pos 88)",
            "log_location":"(write) log_w_sql_parquet (line 14358)",
            "pass":true
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws301; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_parquet (line 6502)",
            "pass":true
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Can only use numbers in the interval value part for multiple unit value pairs interval form, but got invalid value: 123 days hours(line 1, pos 88)",
            "log_location":"(write) log_w_sql_orc (line 14357)",
            "pass":true
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Can only use numbers in the interval value part for multiple unit value pairs interval form, but got invalid value: 123 days hours(line 1, pos 88)",
            "log_location":"(write) log_w_sql_parquet (line 14358)",
            "pass":true
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws301; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_avro (line 8190)",
            "pass":true
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_orc (line 24186)",
            "pass":true
        }
    },
    "356":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        }
    },
    "357":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        }
    },
    "358":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        }
    },
    "359":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        }
    },
    "360":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        }
    },
    "361":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        }
    },
    "362":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        }
    },
    "363":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        }
    },
    "364":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"null",
            "write_value":"null",
            "pass":false
        }
    },
    "365":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot mix year-month and day-time fields: INTERVAL '10 years -11 month -12 days'(line 1, pos 52)",
            "log_location":"(write) log_w_sql_avro (line 14241)",
            "pass":true
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws365; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_orc (line 7044)",
            "pass":true
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot mix year-month and day-time fields: INTERVAL '10 years -11 month -12 days'(line 1, pos 52)",
            "log_location":"(write) log_w_sql_orc (line 14880)",
            "pass":true
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_avro (line 18044)",
            "pass":true
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot mix year-month and day-time fields: INTERVAL '10 years -11 month -12 days'(line 1, pos 52)",
            "log_location":"(write) log_w_sql_avro (line 14241)",
            "pass":true
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_parquet (line 25639)",
            "pass":true
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot mix year-month and day-time fields: INTERVAL '10 years -11 month -12 days'(line 1, pos 52)",
            "log_location":"(write) log_w_sql_parquet (line 14881)",
            "pass":true
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws365; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_parquet (line 7046)",
            "pass":true
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot mix year-month and day-time fields: INTERVAL '10 years -11 month -12 days'(line 1, pos 52)",
            "log_location":"(write) log_w_sql_orc (line 14880)",
            "pass":true
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot mix year-month and day-time fields: INTERVAL '10 years -11 month -12 days'(line 1, pos 52)",
            "log_location":"(write) log_w_sql_parquet (line 14881)",
            "pass":true
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws365; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_avro (line 8734)",
            "pass":true
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_orc (line 25640)",
            "pass":true
        }
    },
    "366":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_sql_avro (line 14544)",
            "pass":true
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws366; line 1 pos 14;",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(read) log_w_df_r_df_orc (line 7084)",
            "pass":true
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_sql_orc (line 15182)",
            "pass":true
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_df_avro (line 18109)",
            "pass":true
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_sql_avro (line 14544)",
            "pass":true
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_df_parquet (line 25704)",
            "pass":true
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_sql_parquet (line 15183)",
            "pass":true
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws366; line 1 pos 14;",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(read) log_w_df_r_df_parquet (line 7086)",
            "pass":true
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_sql_orc (line 15182)",
            "pass":true
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_sql_parquet (line 15183)",
            "pass":true
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws366; line 1 pos 14;",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(read) log_w_df_r_df_avro (line 8774)",
            "pass":true
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_df_orc (line 25705)",
            "pass":true
        }
    },
    "367":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_sql_avro (line 15080)",
            "pass":true
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws367; line 1 pos 14;",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(read) log_w_df_r_df_orc (line 7124)",
            "pass":true
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_sql_orc (line 15717)",
            "pass":true
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_df_avro (line 18169)",
            "pass":true
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_sql_avro (line 15080)",
            "pass":true
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_df_parquet (line 25764)",
            "pass":true
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_sql_parquet (line 15718)",
            "pass":true
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws367; line 1 pos 14;",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(read) log_w_df_r_df_parquet (line 7126)",
            "pass":true
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_sql_orc (line 15717)",
            "pass":true
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_sql_parquet (line 15718)",
            "pass":true
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws367; line 1 pos 14;",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(read) log_w_df_r_df_avro (line 8814)",
            "pass":true
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_df_orc (line 25765)",
            "pass":true
        }
    },
    "368":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Error in query: Literals of type 'epoch' are currently not supported for the interval second type.; line 1 pos 52",
            "log_location":"(write) log_w_sql_avro (line 15326)",
            "pass":true
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws368; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Literals of type 'epoch' are currently not supported for the interval second type.; line 1 pos 0",
            "log_location":"(read) log_w_df_r_df_orc (line 7164)",
            "pass":true
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Error in query: Literals of type 'epoch' are currently not supported for the interval second type.; line 1 pos 52",
            "log_location":"(write) log_w_sql_orc (line 15962)",
            "pass":true
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Literals of type 'epoch' are currently not supported for the interval second type.; line 1 pos 0",
            "log_location":"(write) log_w_df_avro (line 18239)",
            "pass":true
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Error in query: Literals of type 'epoch' are currently not supported for the interval second type.; line 1 pos 52",
            "log_location":"(write) log_w_sql_avro (line 15326)",
            "pass":true
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Literals of type 'epoch' are currently not supported for the interval second type.; line 1 pos 0",
            "log_location":"(write) log_w_df_parquet (line 25834)",
            "pass":true
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Error in query: Literals of type 'epoch' are currently not supported for the interval second type.; line 1 pos 52",
            "log_location":"(write) log_w_sql_parquet (line 15963)",
            "pass":true
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws368; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Literals of type 'epoch' are currently not supported for the interval second type.; line 1 pos 0",
            "log_location":"(read) log_w_df_r_df_parquet (line 7166)",
            "pass":true
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Error in query: Literals of type 'epoch' are currently not supported for the interval second type.; line 1 pos 52",
            "log_location":"(write) log_w_sql_orc (line 15962)",
            "pass":true
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Error in query: Literals of type 'epoch' are currently not supported for the interval second type.; line 1 pos 52",
            "log_location":"(write) log_w_sql_parquet (line 15963)",
            "pass":true
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws368; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Literals of type 'epoch' are currently not supported for the interval second type.; line 1 pos 0",
            "log_location":"(read) log_w_df_r_df_avro (line 8854)",
            "pass":true
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Literals of type 'epoch' are currently not supported for the interval second type.; line 1 pos 0",
            "log_location":"(write) log_w_df_orc (line 25835)",
            "pass":true
        }
    },
    "369":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Interval string does not match day-time format of `[+|-]d h`, `INTERVAL [+|-]'[+|-]d h' DAY TO HOUR` when cast to interval day to hour: 1 2:03:04, set spark.sql.legacy.fromDayTimeString.enabled to true to restore the behavior before Spark 3.0.(line 1, pos 61)",
            "log_location":"(write) log_w_sql_avro (line 15341)",
            "pass":true
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws369; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_orc (line 7204)",
            "pass":true
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Interval string does not match day-time format of `[+|-]d h`, `INTERVAL [+|-]'[+|-]d h' DAY TO HOUR` when cast to interval day to hour: 1 2:03:04, set spark.sql.legacy.fromDayTimeString.enabled to true to restore the behavior before Spark 3.0.(line 1, pos 61)",
            "log_location":"(write) log_w_sql_orc (line 15976)",
            "pass":true
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_avro (line 18358)",
            "pass":true
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Interval string does not match day-time format of `[+|-]d h`, `INTERVAL [+|-]'[+|-]d h' DAY TO HOUR` when cast to interval day to hour: 1 2:03:04, set spark.sql.legacy.fromDayTimeString.enabled to true to restore the behavior before Spark 3.0.(line 1, pos 61)",
            "log_location":"(write) log_w_sql_avro (line 15341)",
            "pass":true
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_parquet (line 25953)",
            "pass":true
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Interval string does not match day-time format of `[+|-]d h`, `INTERVAL [+|-]'[+|-]d h' DAY TO HOUR` when cast to interval day to hour: 1 2:03:04, set spark.sql.legacy.fromDayTimeString.enabled to true to restore the behavior before Spark 3.0.(line 1, pos 61)",
            "log_location":"(write) log_w_sql_parquet (line 15977)",
            "pass":true
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws369; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_parquet (line 7206)",
            "pass":true
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Interval string does not match day-time format of `[+|-]d h`, `INTERVAL [+|-]'[+|-]d h' DAY TO HOUR` when cast to interval day to hour: 1 2:03:04, set spark.sql.legacy.fromDayTimeString.enabled to true to restore the behavior before Spark 3.0.(line 1, pos 61)",
            "log_location":"(write) log_w_sql_orc (line 15976)",
            "pass":true
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Interval string does not match day-time format of `[+|-]d h`, `INTERVAL [+|-]'[+|-]d h' DAY TO HOUR` when cast to interval day to hour: 1 2:03:04, set spark.sql.legacy.fromDayTimeString.enabled to true to restore the behavior before Spark 3.0.(line 1, pos 61)",
            "log_location":"(write) log_w_sql_parquet (line 15977)",
            "pass":true
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws369; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_avro (line 8894)",
            "pass":true
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_orc (line 25954)",
            "pass":true
        }
    },
    "370":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot parse the INTERVAL value: 1-2(line 1, pos 52)",
            "log_location":"(write) log_w_sql_avro (line 15361)",
            "pass":true
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws370; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_orc (line 7244)",
            "pass":true
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot parse the INTERVAL value: 1-2(line 1, pos 52)",
            "log_location":"(write) log_w_sql_orc (line 15995)",
            "pass":true
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_avro (line 18434)",
            "pass":true
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot parse the INTERVAL value: 1-2(line 1, pos 52)",
            "log_location":"(write) log_w_sql_avro (line 15361)",
            "pass":true
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_parquet (line 26029)",
            "pass":true
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot parse the INTERVAL value: 1-2(line 1, pos 52)",
            "log_location":"(write) log_w_sql_parquet (line 15996)",
            "pass":true
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws370; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_parquet (line 7246)",
            "pass":true
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot parse the INTERVAL value: 1-2(line 1, pos 52)",
            "log_location":"(write) log_w_sql_orc (line 15995)",
            "pass":true
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot parse the INTERVAL value: 1-2(line 1, pos 52)",
            "log_location":"(write) log_w_sql_parquet (line 15996)",
            "pass":true
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws370; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_avro (line 8934)",
            "pass":true
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_orc (line 26030)",
            "pass":true
        }
    },
    "371":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot parse the INTERVAL value: 1 day 01:23:45.6789(line 1, pos 52)",
            "log_location":"(write) log_w_sql_avro (line 15381)",
            "pass":true
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws371; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_orc (line 7284)",
            "pass":true
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot parse the INTERVAL value: 1 day 01:23:45.6789(line 1, pos 52)",
            "log_location":"(write) log_w_sql_orc (line 16014)",
            "pass":true
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_avro (line 18499)",
            "pass":true
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot parse the INTERVAL value: 1 day 01:23:45.6789(line 1, pos 52)",
            "log_location":"(write) log_w_sql_avro (line 15381)",
            "pass":true
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_parquet (line 26094)",
            "pass":true
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot parse the INTERVAL value: 1 day 01:23:45.6789(line 1, pos 52)",
            "log_location":"(write) log_w_sql_parquet (line 16015)",
            "pass":true
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws371; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_parquet (line 7286)",
            "pass":true
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot parse the INTERVAL value: 1 day 01:23:45.6789(line 1, pos 52)",
            "log_location":"(write) log_w_sql_orc (line 16014)",
            "pass":true
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot parse the INTERVAL value: 1 day 01:23:45.6789(line 1, pos 52)",
            "log_location":"(write) log_w_sql_parquet (line 16015)",
            "pass":true
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws371; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_avro (line 8974)",
            "pass":true
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_orc (line 26095)",
            "pass":true
        }
    },
    "372":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Error parsing ' 123 11 day' to interval, invalid unit '11'(line 1, pos 61)",
            "log_location":"(write) log_w_sql_avro (line 15401)",
            "pass":true
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws372; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_orc (line 7324)",
            "pass":true
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Error parsing ' 123 11 day' to interval, invalid unit '11'(line 1, pos 61)",
            "log_location":"(write) log_w_sql_orc (line 16033)",
            "pass":true
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_avro (line 18564)",
            "pass":true
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Error parsing ' 123 11 day' to interval, invalid unit '11'(line 1, pos 61)",
            "log_location":"(write) log_w_sql_avro (line 15401)",
            "pass":true
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_parquet (line 26159)",
            "pass":true
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Error parsing ' 123 11 day' to interval, invalid unit '11'(line 1, pos 61)",
            "log_location":"(write) log_w_sql_parquet (line 16034)",
            "pass":true
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws372; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_parquet (line 7326)",
            "pass":true
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Error parsing ' 123 11 day' to interval, invalid unit '11'(line 1, pos 61)",
            "log_location":"(write) log_w_sql_orc (line 16033)",
            "pass":true
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Error parsing ' 123 11 day' to interval, invalid unit '11'(line 1, pos 61)",
            "log_location":"(write) log_w_sql_parquet (line 16034)",
            "pass":true
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws372; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_avro (line 9014)",
            "pass":true
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_orc (line 26160)",
            "pass":true
        }
    },
    "373":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Can only use numbers in the interval value part for multiple unit value pairs interval form, but got invalid value: 123 days hours(line 1, pos 61)",
            "log_location":"(write) log_w_sql_avro (line 15421)",
            "pass":true
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws373; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_orc (line 7364)",
            "pass":true
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Can only use numbers in the interval value part for multiple unit value pairs interval form, but got invalid value: 123 days hours(line 1, pos 61)",
            "log_location":"(write) log_w_sql_orc (line 16052)",
            "pass":true
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_avro (line 18638)",
            "pass":true
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Can only use numbers in the interval value part for multiple unit value pairs interval form, but got invalid value: 123 days hours(line 1, pos 61)",
            "log_location":"(write) log_w_sql_avro (line 15421)",
            "pass":true
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_parquet (line 26233)",
            "pass":true
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Can only use numbers in the interval value part for multiple unit value pairs interval form, but got invalid value: 123 days hours(line 1, pos 61)",
            "log_location":"(write) log_w_sql_parquet (line 16053)",
            "pass":true
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws373; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_parquet (line 7366)",
            "pass":true
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Can only use numbers in the interval value part for multiple unit value pairs interval form, but got invalid value: 123 days hours(line 1, pos 61)",
            "log_location":"(write) log_w_sql_orc (line 16052)",
            "pass":true
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Can only use numbers in the interval value part for multiple unit value pairs interval form, but got invalid value: 123 days hours(line 1, pos 61)",
            "log_location":"(write) log_w_sql_parquet (line 16053)",
            "pass":true
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws373; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_avro (line 9054)",
            "pass":true
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_orc (line 26234)",
            "pass":true
        }
    },
    "374":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot mix year-month and day-time fields: INTERVAL '10 years -11 month -12 days'(line 1, pos 55)",
            "log_location":"(write) log_w_sql_avro (line 15441)",
            "pass":true
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws374; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_orc (line 7404)",
            "pass":true
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot mix year-month and day-time fields: INTERVAL '10 years -11 month -12 days'(line 1, pos 55)",
            "log_location":"(write) log_w_sql_orc (line 16071)",
            "pass":true
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_avro (line 18718)",
            "pass":true
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot mix year-month and day-time fields: INTERVAL '10 years -11 month -12 days'(line 1, pos 55)",
            "log_location":"(write) log_w_sql_avro (line 15441)",
            "pass":true
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_parquet (line 26313)",
            "pass":true
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot mix year-month and day-time fields: INTERVAL '10 years -11 month -12 days'(line 1, pos 55)",
            "log_location":"(write) log_w_sql_parquet (line 16072)",
            "pass":true
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws374; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_parquet (line 7406)",
            "pass":true
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot mix year-month and day-time fields: INTERVAL '10 years -11 month -12 days'(line 1, pos 55)",
            "log_location":"(write) log_w_sql_orc (line 16071)",
            "pass":true
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot mix year-month and day-time fields: INTERVAL '10 years -11 month -12 days'(line 1, pos 55)",
            "log_location":"(write) log_w_sql_parquet (line 16072)",
            "pass":true
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws374; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_avro (line 9094)",
            "pass":true
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_orc (line 26314)",
            "pass":true
        }
    },
    "375":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_sql_avro (line 15744)",
            "pass":true
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws375; line 1 pos 14;",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(read) log_w_df_r_df_orc (line 7444)",
            "pass":true
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_sql_orc (line 16373)",
            "pass":true
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_df_avro (line 18783)",
            "pass":true
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_sql_avro (line 15744)",
            "pass":true
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_df_parquet (line 26378)",
            "pass":true
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_sql_parquet (line 16374)",
            "pass":true
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws375; line 1 pos 14;",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(read) log_w_df_r_df_parquet (line 7446)",
            "pass":true
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_sql_orc (line 16373)",
            "pass":true
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_sql_parquet (line 16374)",
            "pass":true
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws375; line 1 pos 14;",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(read) log_w_df_r_df_avro (line 9134)",
            "pass":true
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_df_orc (line 26379)",
            "pass":true
        }
    },
    "376":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_sql_avro (line 16280)",
            "pass":true
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws376; line 1 pos 14;",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(read) log_w_df_r_df_orc (line 7484)",
            "pass":true
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_sql_orc (line 16908)",
            "pass":true
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_df_avro (line 18843)",
            "pass":true
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_sql_avro (line 16280)",
            "pass":true
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_df_parquet (line 26438)",
            "pass":true
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_sql_parquet (line 16909)",
            "pass":true
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws376; line 1 pos 14;",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(read) log_w_df_r_df_parquet (line 7486)",
            "pass":true
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_sql_orc (line 16908)",
            "pass":true
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_sql_parquet (line 16909)",
            "pass":true
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws376; line 1 pos 14;",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(read) log_w_df_r_df_avro (line 9174)",
            "pass":true
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_df_orc (line 26439)",
            "pass":true
        }
    },
    "377":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Error in query: Literals of type 'epoch' are currently not supported for the interval second type.; line 1 pos 55",
            "log_location":"(write) log_w_sql_avro (line 16526)",
            "pass":true
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws377; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Literals of type 'epoch' are currently not supported for the interval second type.; line 1 pos 0",
            "log_location":"(read) log_w_df_r_df_orc (line 7524)",
            "pass":true
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Error in query: Literals of type 'epoch' are currently not supported for the interval second type.; line 1 pos 55",
            "log_location":"(write) log_w_sql_orc (line 17153)",
            "pass":true
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Literals of type 'epoch' are currently not supported for the interval second type.; line 1 pos 0",
            "log_location":"(write) log_w_df_avro (line 18913)",
            "pass":true
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Error in query: Literals of type 'epoch' are currently not supported for the interval second type.; line 1 pos 55",
            "log_location":"(write) log_w_sql_avro (line 16526)",
            "pass":true
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Literals of type 'epoch' are currently not supported for the interval second type.; line 1 pos 0",
            "log_location":"(write) log_w_df_parquet (line 26508)",
            "pass":true
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Error in query: Literals of type 'epoch' are currently not supported for the interval second type.; line 1 pos 55",
            "log_location":"(write) log_w_sql_parquet (line 17154)",
            "pass":true
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws377; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Literals of type 'epoch' are currently not supported for the interval second type.; line 1 pos 0",
            "log_location":"(read) log_w_df_r_df_parquet (line 7526)",
            "pass":true
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Error in query: Literals of type 'epoch' are currently not supported for the interval second type.; line 1 pos 55",
            "log_location":"(write) log_w_sql_orc (line 17153)",
            "pass":true
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Error in query: Literals of type 'epoch' are currently not supported for the interval second type.; line 1 pos 55",
            "log_location":"(write) log_w_sql_parquet (line 17154)",
            "pass":true
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws377; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Literals of type 'epoch' are currently not supported for the interval second type.; line 1 pos 0",
            "log_location":"(read) log_w_df_r_df_avro (line 9214)",
            "pass":true
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Literals of type 'epoch' are currently not supported for the interval second type.; line 1 pos 0",
            "log_location":"(write) log_w_df_orc (line 26509)",
            "pass":true
        }
    },
    "378":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Interval string does not match day-time format of `[+|-]d h`, `INTERVAL [+|-]'[+|-]d h' DAY TO HOUR` when cast to interval day to hour: 1 2:03:04, set spark.sql.legacy.fromDayTimeString.enabled to true to restore the behavior before Spark 3.0.(line 1, pos 64)",
            "log_location":"(write) log_w_sql_avro (line 16541)",
            "pass":true
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws378; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_orc (line 7564)",
            "pass":true
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Interval string does not match day-time format of `[+|-]d h`, `INTERVAL [+|-]'[+|-]d h' DAY TO HOUR` when cast to interval day to hour: 1 2:03:04, set spark.sql.legacy.fromDayTimeString.enabled to true to restore the behavior before Spark 3.0.(line 1, pos 64)",
            "log_location":"(write) log_w_sql_orc (line 17167)",
            "pass":true
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_avro (line 19032)",
            "pass":true
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Interval string does not match day-time format of `[+|-]d h`, `INTERVAL [+|-]'[+|-]d h' DAY TO HOUR` when cast to interval day to hour: 1 2:03:04, set spark.sql.legacy.fromDayTimeString.enabled to true to restore the behavior before Spark 3.0.(line 1, pos 64)",
            "log_location":"(write) log_w_sql_avro (line 16541)",
            "pass":true
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_parquet (line 26627)",
            "pass":true
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Interval string does not match day-time format of `[+|-]d h`, `INTERVAL [+|-]'[+|-]d h' DAY TO HOUR` when cast to interval day to hour: 1 2:03:04, set spark.sql.legacy.fromDayTimeString.enabled to true to restore the behavior before Spark 3.0.(line 1, pos 64)",
            "log_location":"(write) log_w_sql_parquet (line 17168)",
            "pass":true
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws378; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_parquet (line 7566)",
            "pass":true
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Interval string does not match day-time format of `[+|-]d h`, `INTERVAL [+|-]'[+|-]d h' DAY TO HOUR` when cast to interval day to hour: 1 2:03:04, set spark.sql.legacy.fromDayTimeString.enabled to true to restore the behavior before Spark 3.0.(line 1, pos 64)",
            "log_location":"(write) log_w_sql_orc (line 17167)",
            "pass":true
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Interval string does not match day-time format of `[+|-]d h`, `INTERVAL [+|-]'[+|-]d h' DAY TO HOUR` when cast to interval day to hour: 1 2:03:04, set spark.sql.legacy.fromDayTimeString.enabled to true to restore the behavior before Spark 3.0.(line 1, pos 64)",
            "log_location":"(write) log_w_sql_parquet (line 17168)",
            "pass":true
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws378; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_avro (line 9254)",
            "pass":true
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_orc (line 26628)",
            "pass":true
        }
    },
    "379":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot parse the INTERVAL value: 1-2(line 1, pos 55)",
            "log_location":"(write) log_w_sql_avro (line 16561)",
            "pass":true
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws379; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_orc (line 7604)",
            "pass":true
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot parse the INTERVAL value: 1-2(line 1, pos 55)",
            "log_location":"(write) log_w_sql_orc (line 17186)",
            "pass":true
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_avro (line 19108)",
            "pass":true
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot parse the INTERVAL value: 1-2(line 1, pos 55)",
            "log_location":"(write) log_w_sql_avro (line 16561)",
            "pass":true
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_parquet (line 26703)",
            "pass":true
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot parse the INTERVAL value: 1-2(line 1, pos 55)",
            "log_location":"(write) log_w_sql_parquet (line 17187)",
            "pass":true
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws379; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_parquet (line 7606)",
            "pass":true
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot parse the INTERVAL value: 1-2(line 1, pos 55)",
            "log_location":"(write) log_w_sql_orc (line 17186)",
            "pass":true
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot parse the INTERVAL value: 1-2(line 1, pos 55)",
            "log_location":"(write) log_w_sql_parquet (line 17187)",
            "pass":true
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws379; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_avro (line 9294)",
            "pass":true
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_orc (line 26704)",
            "pass":true
        }
    },
    "380":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot parse the INTERVAL value: 1 day 01:23:45.6789(line 1, pos 55)",
            "log_location":"(write) log_w_sql_avro (line 16581)",
            "pass":true
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws380; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_orc (line 7644)",
            "pass":true
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot parse the INTERVAL value: 1 day 01:23:45.6789(line 1, pos 55)",
            "log_location":"(write) log_w_sql_orc (line 17205)",
            "pass":true
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_avro (line 19173)",
            "pass":true
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot parse the INTERVAL value: 1 day 01:23:45.6789(line 1, pos 55)",
            "log_location":"(write) log_w_sql_avro (line 16581)",
            "pass":true
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_parquet (line 26768)",
            "pass":true
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot parse the INTERVAL value: 1 day 01:23:45.6789(line 1, pos 55)",
            "log_location":"(write) log_w_sql_parquet (line 17206)",
            "pass":true
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws380; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_parquet (line 7646)",
            "pass":true
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot parse the INTERVAL value: 1 day 01:23:45.6789(line 1, pos 55)",
            "log_location":"(write) log_w_sql_orc (line 17205)",
            "pass":true
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot parse the INTERVAL value: 1 day 01:23:45.6789(line 1, pos 55)",
            "log_location":"(write) log_w_sql_parquet (line 17206)",
            "pass":true
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws380; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_avro (line 9334)",
            "pass":true
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_orc (line 26769)",
            "pass":true
        }
    },
    "381":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Error parsing ' 123 11 day' to interval, invalid unit '11'(line 1, pos 64)",
            "log_location":"(write) log_w_sql_avro (line 16601)",
            "pass":true
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws381; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_orc (line 7684)",
            "pass":true
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Error parsing ' 123 11 day' to interval, invalid unit '11'(line 1, pos 64)",
            "log_location":"(write) log_w_sql_orc (line 17224)",
            "pass":true
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_avro (line 19238)",
            "pass":true
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Error parsing ' 123 11 day' to interval, invalid unit '11'(line 1, pos 64)",
            "log_location":"(write) log_w_sql_avro (line 16601)",
            "pass":true
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_parquet (line 26833)",
            "pass":true
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Error parsing ' 123 11 day' to interval, invalid unit '11'(line 1, pos 64)",
            "log_location":"(write) log_w_sql_parquet (line 17225)",
            "pass":true
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws381; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_parquet (line 7686)",
            "pass":true
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Error parsing ' 123 11 day' to interval, invalid unit '11'(line 1, pos 64)",
            "log_location":"(write) log_w_sql_orc (line 17224)",
            "pass":true
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Error parsing ' 123 11 day' to interval, invalid unit '11'(line 1, pos 64)",
            "log_location":"(write) log_w_sql_parquet (line 17225)",
            "pass":true
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws381; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_avro (line 9374)",
            "pass":true
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_orc (line 26834)",
            "pass":true
        }
    },
    "382":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Can only use numbers in the interval value part for multiple unit value pairs interval form, but got invalid value: 123 days hours(line 1, pos 64)",
            "log_location":"(write) log_w_sql_avro (line 16621)",
            "pass":true
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws382; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_orc (line 7724)",
            "pass":true
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Can only use numbers in the interval value part for multiple unit value pairs interval form, but got invalid value: 123 days hours(line 1, pos 64)",
            "log_location":"(write) log_w_sql_orc (line 17243)",
            "pass":true
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_avro (line 19312)",
            "pass":true
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Can only use numbers in the interval value part for multiple unit value pairs interval form, but got invalid value: 123 days hours(line 1, pos 64)",
            "log_location":"(write) log_w_sql_avro (line 16621)",
            "pass":true
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_parquet (line 26907)",
            "pass":true
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Can only use numbers in the interval value part for multiple unit value pairs interval form, but got invalid value: 123 days hours(line 1, pos 64)",
            "log_location":"(write) log_w_sql_parquet (line 17244)",
            "pass":true
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws382; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_parquet (line 7726)",
            "pass":true
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Can only use numbers in the interval value part for multiple unit value pairs interval form, but got invalid value: 123 days hours(line 1, pos 64)",
            "log_location":"(write) log_w_sql_orc (line 17243)",
            "pass":true
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Can only use numbers in the interval value part for multiple unit value pairs interval form, but got invalid value: 123 days hours(line 1, pos 64)",
            "log_location":"(write) log_w_sql_parquet (line 17244)",
            "pass":true
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws382; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_avro (line 9414)",
            "pass":true
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_orc (line 26908)",
            "pass":true
        }
    },
    "383":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot mix year-month and day-time fields: INTERVAL '10 years -11 month -12 days'(line 1, pos 58)",
            "log_location":"(write) log_w_sql_avro (line 16641)",
            "pass":true
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws383; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_orc (line 7764)",
            "pass":true
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot mix year-month and day-time fields: INTERVAL '10 years -11 month -12 days'(line 1, pos 58)",
            "log_location":"(write) log_w_sql_orc (line 17262)",
            "pass":true
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_avro (line 19392)",
            "pass":true
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot mix year-month and day-time fields: INTERVAL '10 years -11 month -12 days'(line 1, pos 58)",
            "log_location":"(write) log_w_sql_avro (line 16641)",
            "pass":true
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_parquet (line 26987)",
            "pass":true
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot mix year-month and day-time fields: INTERVAL '10 years -11 month -12 days'(line 1, pos 58)",
            "log_location":"(write) log_w_sql_parquet (line 17263)",
            "pass":true
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws383; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_parquet (line 7766)",
            "pass":true
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot mix year-month and day-time fields: INTERVAL '10 years -11 month -12 days'(line 1, pos 58)",
            "log_location":"(write) log_w_sql_orc (line 17262)",
            "pass":true
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot mix year-month and day-time fields: INTERVAL '10 years -11 month -12 days'(line 1, pos 58)",
            "log_location":"(write) log_w_sql_parquet (line 17263)",
            "pass":true
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws383; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_avro (line 9454)",
            "pass":true
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_orc (line 26988)",
            "pass":true
        }
    },
    "384":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_sql_avro (line 16944)",
            "pass":true
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws384; line 1 pos 14;",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(read) log_w_df_r_df_orc (line 7804)",
            "pass":true
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_sql_orc (line 17564)",
            "pass":true
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_df_avro (line 19457)",
            "pass":true
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_sql_avro (line 16944)",
            "pass":true
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_df_parquet (line 27052)",
            "pass":true
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_sql_parquet (line 17565)",
            "pass":true
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws384; line 1 pos 14;",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(read) log_w_df_r_df_parquet (line 7806)",
            "pass":true
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_sql_orc (line 17564)",
            "pass":true
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_sql_parquet (line 17565)",
            "pass":true
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws384; line 1 pos 14;",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(read) log_w_df_r_df_avro (line 9494)",
            "pass":true
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_df_orc (line 27053)",
            "pass":true
        }
    },
    "385":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_sql_avro (line 17480)",
            "pass":true
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws385; line 1 pos 14;",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(read) log_w_df_r_df_orc (line 7844)",
            "pass":true
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_sql_orc (line 18099)",
            "pass":true
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_df_avro (line 19517)",
            "pass":true
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_sql_avro (line 17480)",
            "pass":true
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_df_parquet (line 27112)",
            "pass":true
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_sql_parquet (line 18100)",
            "pass":true
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws385; line 1 pos 14;",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(read) log_w_df_r_df_parquet (line 7846)",
            "pass":true
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_sql_orc (line 18099)",
            "pass":true
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_sql_parquet (line 18100)",
            "pass":true
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws385; line 1 pos 14;",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(read) log_w_df_r_df_avro (line 9534)",
            "pass":true
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_df_orc (line 27113)",
            "pass":true
        }
    },
    "386":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Error in query: Literals of type 'epoch' are currently not supported for the interval second type.; line 1 pos 58",
            "log_location":"(write) log_w_sql_avro (line 17726)",
            "pass":true
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws386; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Literals of type 'epoch' are currently not supported for the interval second type.; line 1 pos 0",
            "log_location":"(read) log_w_df_r_df_orc (line 7884)",
            "pass":true
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Error in query: Literals of type 'epoch' are currently not supported for the interval second type.; line 1 pos 58",
            "log_location":"(write) log_w_sql_orc (line 18344)",
            "pass":true
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Literals of type 'epoch' are currently not supported for the interval second type.; line 1 pos 0",
            "log_location":"(write) log_w_df_avro (line 19587)",
            "pass":true
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Error in query: Literals of type 'epoch' are currently not supported for the interval second type.; line 1 pos 58",
            "log_location":"(write) log_w_sql_avro (line 17726)",
            "pass":true
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Literals of type 'epoch' are currently not supported for the interval second type.; line 1 pos 0",
            "log_location":"(write) log_w_df_parquet (line 27182)",
            "pass":true
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Error in query: Literals of type 'epoch' are currently not supported for the interval second type.; line 1 pos 58",
            "log_location":"(write) log_w_sql_parquet (line 18345)",
            "pass":true
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws386; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Literals of type 'epoch' are currently not supported for the interval second type.; line 1 pos 0",
            "log_location":"(read) log_w_df_r_df_parquet (line 7886)",
            "pass":true
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Error in query: Literals of type 'epoch' are currently not supported for the interval second type.; line 1 pos 58",
            "log_location":"(write) log_w_sql_orc (line 18344)",
            "pass":true
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Error in query: Literals of type 'epoch' are currently not supported for the interval second type.; line 1 pos 58",
            "log_location":"(write) log_w_sql_parquet (line 18345)",
            "pass":true
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws386; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Literals of type 'epoch' are currently not supported for the interval second type.; line 1 pos 0",
            "log_location":"(read) log_w_df_r_df_avro (line 9574)",
            "pass":true
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Literals of type 'epoch' are currently not supported for the interval second type.; line 1 pos 0",
            "log_location":"(write) log_w_df_orc (line 27183)",
            "pass":true
        }
    },
    "387":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Interval string does not match day-time format of `[+|-]d h`, `INTERVAL [+|-]'[+|-]d h' DAY TO HOUR` when cast to interval day to hour: 1 2:03:04, set spark.sql.legacy.fromDayTimeString.enabled to true to restore the behavior before Spark 3.0.(line 1, pos 67)",
            "log_location":"(write) log_w_sql_avro (line 17741)",
            "pass":true
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws387; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_orc (line 7924)",
            "pass":true
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Interval string does not match day-time format of `[+|-]d h`, `INTERVAL [+|-]'[+|-]d h' DAY TO HOUR` when cast to interval day to hour: 1 2:03:04, set spark.sql.legacy.fromDayTimeString.enabled to true to restore the behavior before Spark 3.0.(line 1, pos 67)",
            "log_location":"(write) log_w_sql_orc (line 18358)",
            "pass":true
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_avro (line 19706)",
            "pass":true
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Interval string does not match day-time format of `[+|-]d h`, `INTERVAL [+|-]'[+|-]d h' DAY TO HOUR` when cast to interval day to hour: 1 2:03:04, set spark.sql.legacy.fromDayTimeString.enabled to true to restore the behavior before Spark 3.0.(line 1, pos 67)",
            "log_location":"(write) log_w_sql_avro (line 17741)",
            "pass":true
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_parquet (line 27301)",
            "pass":true
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Interval string does not match day-time format of `[+|-]d h`, `INTERVAL [+|-]'[+|-]d h' DAY TO HOUR` when cast to interval day to hour: 1 2:03:04, set spark.sql.legacy.fromDayTimeString.enabled to true to restore the behavior before Spark 3.0.(line 1, pos 67)",
            "log_location":"(write) log_w_sql_parquet (line 18359)",
            "pass":true
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws387; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_parquet (line 7926)",
            "pass":true
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Interval string does not match day-time format of `[+|-]d h`, `INTERVAL [+|-]'[+|-]d h' DAY TO HOUR` when cast to interval day to hour: 1 2:03:04, set spark.sql.legacy.fromDayTimeString.enabled to true to restore the behavior before Spark 3.0.(line 1, pos 67)",
            "log_location":"(write) log_w_sql_orc (line 18358)",
            "pass":true
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Interval string does not match day-time format of `[+|-]d h`, `INTERVAL [+|-]'[+|-]d h' DAY TO HOUR` when cast to interval day to hour: 1 2:03:04, set spark.sql.legacy.fromDayTimeString.enabled to true to restore the behavior before Spark 3.0.(line 1, pos 67)",
            "log_location":"(write) log_w_sql_parquet (line 18359)",
            "pass":true
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws387; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_avro (line 9614)",
            "pass":true
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_orc (line 27302)",
            "pass":true
        }
    },
    "388":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot parse the INTERVAL value: 1-2(line 1, pos 58)",
            "log_location":"(write) log_w_sql_avro (line 17761)",
            "pass":true
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws388; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_orc (line 7964)",
            "pass":true
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot parse the INTERVAL value: 1-2(line 1, pos 58)",
            "log_location":"(write) log_w_sql_orc (line 18377)",
            "pass":true
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_avro (line 19782)",
            "pass":true
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot parse the INTERVAL value: 1-2(line 1, pos 58)",
            "log_location":"(write) log_w_sql_avro (line 17761)",
            "pass":true
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_parquet (line 27377)",
            "pass":true
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot parse the INTERVAL value: 1-2(line 1, pos 58)",
            "log_location":"(write) log_w_sql_parquet (line 18378)",
            "pass":true
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws388; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_parquet (line 7966)",
            "pass":true
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot parse the INTERVAL value: 1-2(line 1, pos 58)",
            "log_location":"(write) log_w_sql_orc (line 18377)",
            "pass":true
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot parse the INTERVAL value: 1-2(line 1, pos 58)",
            "log_location":"(write) log_w_sql_parquet (line 18378)",
            "pass":true
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws388; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_avro (line 9654)",
            "pass":true
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_orc (line 27378)",
            "pass":true
        }
    },
    "389":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot parse the INTERVAL value: 1 day 01:23:45.6789(line 1, pos 58)",
            "log_location":"(write) log_w_sql_avro (line 17781)",
            "pass":true
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws389; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_orc (line 8004)",
            "pass":true
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot parse the INTERVAL value: 1 day 01:23:45.6789(line 1, pos 58)",
            "log_location":"(write) log_w_sql_orc (line 18396)",
            "pass":true
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_avro (line 19847)",
            "pass":true
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot parse the INTERVAL value: 1 day 01:23:45.6789(line 1, pos 58)",
            "log_location":"(write) log_w_sql_avro (line 17781)",
            "pass":true
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_parquet (line 27442)",
            "pass":true
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot parse the INTERVAL value: 1 day 01:23:45.6789(line 1, pos 58)",
            "log_location":"(write) log_w_sql_parquet (line 18397)",
            "pass":true
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws389; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_parquet (line 8006)",
            "pass":true
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot parse the INTERVAL value: 1 day 01:23:45.6789(line 1, pos 58)",
            "log_location":"(write) log_w_sql_orc (line 18396)",
            "pass":true
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot parse the INTERVAL value: 1 day 01:23:45.6789(line 1, pos 58)",
            "log_location":"(write) log_w_sql_parquet (line 18397)",
            "pass":true
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws389; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_avro (line 9694)",
            "pass":true
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_orc (line 27443)",
            "pass":true
        }
    },
    "390":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Error parsing ' 123 11 day' to interval, invalid unit '11'(line 1, pos 67)",
            "log_location":"(write) log_w_sql_avro (line 17801)",
            "pass":true
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws390; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_orc (line 8044)",
            "pass":true
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Error parsing ' 123 11 day' to interval, invalid unit '11'(line 1, pos 67)",
            "log_location":"(write) log_w_sql_orc (line 18415)",
            "pass":true
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_avro (line 19912)",
            "pass":true
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Error parsing ' 123 11 day' to interval, invalid unit '11'(line 1, pos 67)",
            "log_location":"(write) log_w_sql_avro (line 17801)",
            "pass":true
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_parquet (line 27507)",
            "pass":true
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Error parsing ' 123 11 day' to interval, invalid unit '11'(line 1, pos 67)",
            "log_location":"(write) log_w_sql_parquet (line 18416)",
            "pass":true
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws390; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_parquet (line 8046)",
            "pass":true
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Error parsing ' 123 11 day' to interval, invalid unit '11'(line 1, pos 67)",
            "log_location":"(write) log_w_sql_orc (line 18415)",
            "pass":true
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Error parsing ' 123 11 day' to interval, invalid unit '11'(line 1, pos 67)",
            "log_location":"(write) log_w_sql_parquet (line 18416)",
            "pass":true
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws390; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_avro (line 9734)",
            "pass":true
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_orc (line 27508)",
            "pass":true
        }
    },
    "391":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Can only use numbers in the interval value part for multiple unit value pairs interval form, but got invalid value: 123 days hours(line 1, pos 67)",
            "log_location":"(write) log_w_sql_avro (line 17821)",
            "pass":true
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws391; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_orc (line 8084)",
            "pass":true
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Can only use numbers in the interval value part for multiple unit value pairs interval form, but got invalid value: 123 days hours(line 1, pos 67)",
            "log_location":"(write) log_w_sql_orc (line 18434)",
            "pass":true
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_avro (line 19986)",
            "pass":true
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Can only use numbers in the interval value part for multiple unit value pairs interval form, but got invalid value: 123 days hours(line 1, pos 67)",
            "log_location":"(write) log_w_sql_avro (line 17821)",
            "pass":true
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_parquet (line 27581)",
            "pass":true
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Can only use numbers in the interval value part for multiple unit value pairs interval form, but got invalid value: 123 days hours(line 1, pos 67)",
            "log_location":"(write) log_w_sql_parquet (line 18435)",
            "pass":true
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws391; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_parquet (line 8086)",
            "pass":true
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Can only use numbers in the interval value part for multiple unit value pairs interval form, but got invalid value: 123 days hours(line 1, pos 67)",
            "log_location":"(write) log_w_sql_orc (line 18434)",
            "pass":true
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Can only use numbers in the interval value part for multiple unit value pairs interval form, but got invalid value: 123 days hours(line 1, pos 67)",
            "log_location":"(write) log_w_sql_parquet (line 18435)",
            "pass":true
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws391; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_avro (line 9774)",
            "pass":true
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_orc (line 27582)",
            "pass":true
        }
    },
    "392":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot mix year-month and day-time fields: INTERVAL '10 years -11 month -12 days'(line 1, pos 59)",
            "log_location":"(write) log_w_sql_avro (line 17841)",
            "pass":true
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws392; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_orc (line 8124)",
            "pass":true
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot mix year-month and day-time fields: INTERVAL '10 years -11 month -12 days'(line 1, pos 59)",
            "log_location":"(write) log_w_sql_orc (line 18453)",
            "pass":true
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_avro (line 20066)",
            "pass":true
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot mix year-month and day-time fields: INTERVAL '10 years -11 month -12 days'(line 1, pos 59)",
            "log_location":"(write) log_w_sql_avro (line 17841)",
            "pass":true
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_parquet (line 27661)",
            "pass":true
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot mix year-month and day-time fields: INTERVAL '10 years -11 month -12 days'(line 1, pos 59)",
            "log_location":"(write) log_w_sql_parquet (line 18454)",
            "pass":true
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws392; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_parquet (line 8126)",
            "pass":true
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot mix year-month and day-time fields: INTERVAL '10 years -11 month -12 days'(line 1, pos 59)",
            "log_location":"(write) log_w_sql_orc (line 18453)",
            "pass":true
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot mix year-month and day-time fields: INTERVAL '10 years -11 month -12 days'(line 1, pos 59)",
            "log_location":"(write) log_w_sql_parquet (line 18454)",
            "pass":true
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws392; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_avro (line 9814)",
            "pass":true
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_orc (line 27662)",
            "pass":true
        }
    },
    "393":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_sql_avro (line 18144)",
            "pass":true
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws393; line 1 pos 14;",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(read) log_w_df_r_df_orc (line 8164)",
            "pass":true
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_sql_orc (line 18755)",
            "pass":true
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_df_avro (line 20131)",
            "pass":true
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_sql_avro (line 18144)",
            "pass":true
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_df_parquet (line 27726)",
            "pass":true
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_sql_parquet (line 18756)",
            "pass":true
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws393; line 1 pos 14;",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(read) log_w_df_r_df_parquet (line 8166)",
            "pass":true
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_sql_orc (line 18755)",
            "pass":true
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_sql_parquet (line 18756)",
            "pass":true
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws393; line 1 pos 14;",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(read) log_w_df_r_df_avro (line 9854)",
            "pass":true
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_df_orc (line 27727)",
            "pass":true
        }
    },
    "394":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_sql_avro (line 18680)",
            "pass":true
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws394; line 1 pos 14;",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(read) log_w_df_r_df_orc (line 8204)",
            "pass":true
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_sql_orc (line 19290)",
            "pass":true
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_df_avro (line 20191)",
            "pass":true
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_sql_avro (line 18680)",
            "pass":true
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_df_parquet (line 27786)",
            "pass":true
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_sql_parquet (line 19291)",
            "pass":true
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws394; line 1 pos 14;",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(read) log_w_df_r_df_parquet (line 8206)",
            "pass":true
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_sql_orc (line 19290)",
            "pass":true
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_sql_parquet (line 19291)",
            "pass":true
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws394; line 1 pos 14;",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(read) log_w_df_r_df_avro (line 9894)",
            "pass":true
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_df_orc (line 27787)",
            "pass":true
        }
    },
    "395":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Error in query: Literals of type 'epoch' are currently not supported for the interval second type.; line 1 pos 59",
            "log_location":"(write) log_w_sql_avro (line 18926)",
            "pass":true
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws395; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Literals of type 'epoch' are currently not supported for the interval second type.; line 1 pos 0",
            "log_location":"(read) log_w_df_r_df_orc (line 8244)",
            "pass":true
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Error in query: Literals of type 'epoch' are currently not supported for the interval second type.; line 1 pos 59",
            "log_location":"(write) log_w_sql_orc (line 19535)",
            "pass":true
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Literals of type 'epoch' are currently not supported for the interval second type.; line 1 pos 0",
            "log_location":"(write) log_w_df_avro (line 20261)",
            "pass":true
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Error in query: Literals of type 'epoch' are currently not supported for the interval second type.; line 1 pos 59",
            "log_location":"(write) log_w_sql_avro (line 18926)",
            "pass":true
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Literals of type 'epoch' are currently not supported for the interval second type.; line 1 pos 0",
            "log_location":"(write) log_w_df_parquet (line 27856)",
            "pass":true
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Error in query: Literals of type 'epoch' are currently not supported for the interval second type.; line 1 pos 59",
            "log_location":"(write) log_w_sql_parquet (line 19536)",
            "pass":true
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws395; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Literals of type 'epoch' are currently not supported for the interval second type.; line 1 pos 0",
            "log_location":"(read) log_w_df_r_df_parquet (line 8246)",
            "pass":true
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Error in query: Literals of type 'epoch' are currently not supported for the interval second type.; line 1 pos 59",
            "log_location":"(write) log_w_sql_orc (line 19535)",
            "pass":true
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Error in query: Literals of type 'epoch' are currently not supported for the interval second type.; line 1 pos 59",
            "log_location":"(write) log_w_sql_parquet (line 19536)",
            "pass":true
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws395; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Literals of type 'epoch' are currently not supported for the interval second type.; line 1 pos 0",
            "log_location":"(read) log_w_df_r_df_avro (line 9934)",
            "pass":true
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Literals of type 'epoch' are currently not supported for the interval second type.; line 1 pos 0",
            "log_location":"(write) log_w_df_orc (line 27857)",
            "pass":true
        }
    },
    "396":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Interval string does not match day-time format of `[+|-]d h`, `INTERVAL [+|-]'[+|-]d h' DAY TO HOUR` when cast to interval day to hour: 1 2:03:04, set spark.sql.legacy.fromDayTimeString.enabled to true to restore the behavior before Spark 3.0.(line 1, pos 68)",
            "log_location":"(write) log_w_sql_avro (line 18941)",
            "pass":true
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws396; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_orc (line 8284)",
            "pass":true
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Interval string does not match day-time format of `[+|-]d h`, `INTERVAL [+|-]'[+|-]d h' DAY TO HOUR` when cast to interval day to hour: 1 2:03:04, set spark.sql.legacy.fromDayTimeString.enabled to true to restore the behavior before Spark 3.0.(line 1, pos 68)",
            "log_location":"(write) log_w_sql_orc (line 19549)",
            "pass":true
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_avro (line 20380)",
            "pass":true
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Interval string does not match day-time format of `[+|-]d h`, `INTERVAL [+|-]'[+|-]d h' DAY TO HOUR` when cast to interval day to hour: 1 2:03:04, set spark.sql.legacy.fromDayTimeString.enabled to true to restore the behavior before Spark 3.0.(line 1, pos 68)",
            "log_location":"(write) log_w_sql_avro (line 18941)",
            "pass":true
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_parquet (line 27975)",
            "pass":true
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Interval string does not match day-time format of `[+|-]d h`, `INTERVAL [+|-]'[+|-]d h' DAY TO HOUR` when cast to interval day to hour: 1 2:03:04, set spark.sql.legacy.fromDayTimeString.enabled to true to restore the behavior before Spark 3.0.(line 1, pos 68)",
            "log_location":"(write) log_w_sql_parquet (line 19550)",
            "pass":true
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws396; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_parquet (line 8286)",
            "pass":true
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Interval string does not match day-time format of `[+|-]d h`, `INTERVAL [+|-]'[+|-]d h' DAY TO HOUR` when cast to interval day to hour: 1 2:03:04, set spark.sql.legacy.fromDayTimeString.enabled to true to restore the behavior before Spark 3.0.(line 1, pos 68)",
            "log_location":"(write) log_w_sql_orc (line 19549)",
            "pass":true
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Interval string does not match day-time format of `[+|-]d h`, `INTERVAL [+|-]'[+|-]d h' DAY TO HOUR` when cast to interval day to hour: 1 2:03:04, set spark.sql.legacy.fromDayTimeString.enabled to true to restore the behavior before Spark 3.0.(line 1, pos 68)",
            "log_location":"(write) log_w_sql_parquet (line 19550)",
            "pass":true
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws396; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_avro (line 9974)",
            "pass":true
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_orc (line 27976)",
            "pass":true
        }
    },
    "397":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot parse the INTERVAL value: 1-2(line 1, pos 59)",
            "log_location":"(write) log_w_sql_avro (line 18961)",
            "pass":true
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws397; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_orc (line 8324)",
            "pass":true
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot parse the INTERVAL value: 1-2(line 1, pos 59)",
            "log_location":"(write) log_w_sql_orc (line 19568)",
            "pass":true
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_avro (line 20456)",
            "pass":true
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot parse the INTERVAL value: 1-2(line 1, pos 59)",
            "log_location":"(write) log_w_sql_avro (line 18961)",
            "pass":true
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_parquet (line 28051)",
            "pass":true
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot parse the INTERVAL value: 1-2(line 1, pos 59)",
            "log_location":"(write) log_w_sql_parquet (line 19569)",
            "pass":true
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws397; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_parquet (line 8326)",
            "pass":true
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot parse the INTERVAL value: 1-2(line 1, pos 59)",
            "log_location":"(write) log_w_sql_orc (line 19568)",
            "pass":true
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot parse the INTERVAL value: 1-2(line 1, pos 59)",
            "log_location":"(write) log_w_sql_parquet (line 19569)",
            "pass":true
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws397; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_avro (line 10014)",
            "pass":true
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_orc (line 28052)",
            "pass":true
        }
    },
    "398":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot parse the INTERVAL value: 1 day 01:23:45.6789(line 1, pos 59)",
            "log_location":"(write) log_w_sql_avro (line 18981)",
            "pass":true
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws398; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_orc (line 8364)",
            "pass":true
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot parse the INTERVAL value: 1 day 01:23:45.6789(line 1, pos 59)",
            "log_location":"(write) log_w_sql_orc (line 19587)",
            "pass":true
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_avro (line 20521)",
            "pass":true
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot parse the INTERVAL value: 1 day 01:23:45.6789(line 1, pos 59)",
            "log_location":"(write) log_w_sql_avro (line 18981)",
            "pass":true
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_parquet (line 28116)",
            "pass":true
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot parse the INTERVAL value: 1 day 01:23:45.6789(line 1, pos 59)",
            "log_location":"(write) log_w_sql_parquet (line 19588)",
            "pass":true
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws398; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_parquet (line 8366)",
            "pass":true
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot parse the INTERVAL value: 1 day 01:23:45.6789(line 1, pos 59)",
            "log_location":"(write) log_w_sql_orc (line 19587)",
            "pass":true
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot parse the INTERVAL value: 1 day 01:23:45.6789(line 1, pos 59)",
            "log_location":"(write) log_w_sql_parquet (line 19588)",
            "pass":true
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws398; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_avro (line 10054)",
            "pass":true
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_orc (line 28117)",
            "pass":true
        }
    },
    "399":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Error parsing ' 123 11 day' to interval, invalid unit '11'(line 1, pos 68)",
            "log_location":"(write) log_w_sql_avro (line 19001)",
            "pass":true
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws399; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_orc (line 8404)",
            "pass":true
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Error parsing ' 123 11 day' to interval, invalid unit '11'(line 1, pos 68)",
            "log_location":"(write) log_w_sql_orc (line 19606)",
            "pass":true
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_avro (line 20586)",
            "pass":true
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Error parsing ' 123 11 day' to interval, invalid unit '11'(line 1, pos 68)",
            "log_location":"(write) log_w_sql_avro (line 19001)",
            "pass":true
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_parquet (line 28181)",
            "pass":true
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Error parsing ' 123 11 day' to interval, invalid unit '11'(line 1, pos 68)",
            "log_location":"(write) log_w_sql_parquet (line 19607)",
            "pass":true
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws399; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_parquet (line 8406)",
            "pass":true
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Error parsing ' 123 11 day' to interval, invalid unit '11'(line 1, pos 68)",
            "log_location":"(write) log_w_sql_orc (line 19606)",
            "pass":true
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Error parsing ' 123 11 day' to interval, invalid unit '11'(line 1, pos 68)",
            "log_location":"(write) log_w_sql_parquet (line 19607)",
            "pass":true
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws399; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_avro (line 10094)",
            "pass":true
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_orc (line 28182)",
            "pass":true
        }
    },
    "400":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Can only use numbers in the interval value part for multiple unit value pairs interval form, but got invalid value: 123 days hours(line 1, pos 68)",
            "log_location":"(write) log_w_sql_avro (line 19021)",
            "pass":true
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws400; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_orc (line 8444)",
            "pass":true
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Can only use numbers in the interval value part for multiple unit value pairs interval form, but got invalid value: 123 days hours(line 1, pos 68)",
            "log_location":"(write) log_w_sql_orc (line 19625)",
            "pass":true
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_avro (line 20660)",
            "pass":true
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Can only use numbers in the interval value part for multiple unit value pairs interval form, but got invalid value: 123 days hours(line 1, pos 68)",
            "log_location":"(write) log_w_sql_avro (line 19021)",
            "pass":true
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_parquet (line 28255)",
            "pass":true
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Can only use numbers in the interval value part for multiple unit value pairs interval form, but got invalid value: 123 days hours(line 1, pos 68)",
            "log_location":"(write) log_w_sql_parquet (line 19626)",
            "pass":true
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws400; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_parquet (line 8446)",
            "pass":true
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Can only use numbers in the interval value part for multiple unit value pairs interval form, but got invalid value: 123 days hours(line 1, pos 68)",
            "log_location":"(write) log_w_sql_orc (line 19625)",
            "pass":true
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Can only use numbers in the interval value part for multiple unit value pairs interval form, but got invalid value: 123 days hours(line 1, pos 68)",
            "log_location":"(write) log_w_sql_parquet (line 19626)",
            "pass":true
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws400; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_avro (line 10134)",
            "pass":true
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_orc (line 28256)",
            "pass":true
        }
    },
    "401":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot mix year-month and day-time fields: INTERVAL '10 years -11 month -12 days'(line 1, pos 58)",
            "log_location":"(write) log_w_sql_avro (line 19041)",
            "pass":true
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws401; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_orc (line 8484)",
            "pass":true
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot mix year-month and day-time fields: INTERVAL '10 years -11 month -12 days'(line 1, pos 58)",
            "log_location":"(write) log_w_sql_orc (line 19644)",
            "pass":true
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_avro (line 20740)",
            "pass":true
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot mix year-month and day-time fields: INTERVAL '10 years -11 month -12 days'(line 1, pos 58)",
            "log_location":"(write) log_w_sql_avro (line 19041)",
            "pass":true
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_parquet (line 28335)",
            "pass":true
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot mix year-month and day-time fields: INTERVAL '10 years -11 month -12 days'(line 1, pos 58)",
            "log_location":"(write) log_w_sql_parquet (line 19645)",
            "pass":true
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws401; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_parquet (line 8486)",
            "pass":true
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot mix year-month and day-time fields: INTERVAL '10 years -11 month -12 days'(line 1, pos 58)",
            "log_location":"(write) log_w_sql_orc (line 19644)",
            "pass":true
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot mix year-month and day-time fields: INTERVAL '10 years -11 month -12 days'(line 1, pos 58)",
            "log_location":"(write) log_w_sql_parquet (line 19645)",
            "pass":true
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws401; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_avro (line 10174)",
            "pass":true
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_orc (line 28336)",
            "pass":true
        }
    },
    "402":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_sql_avro (line 19344)",
            "pass":true
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws402; line 1 pos 14;",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(read) log_w_df_r_df_orc (line 8524)",
            "pass":true
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_sql_orc (line 19946)",
            "pass":true
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_df_avro (line 20805)",
            "pass":true
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_sql_avro (line 19344)",
            "pass":true
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_df_parquet (line 28400)",
            "pass":true
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_sql_parquet (line 19947)",
            "pass":true
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws402; line 1 pos 14;",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(read) log_w_df_r_df_parquet (line 8526)",
            "pass":true
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_sql_orc (line 19946)",
            "pass":true
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_sql_parquet (line 19947)",
            "pass":true
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws402; line 1 pos 14;",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(read) log_w_df_r_df_avro (line 10214)",
            "pass":true
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_df_orc (line 28401)",
            "pass":true
        }
    },
    "403":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_sql_avro (line 19880)",
            "pass":true
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws403; line 1 pos 14;",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(read) log_w_df_r_df_orc (line 8564)",
            "pass":true
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_sql_orc (line 20481)",
            "pass":true
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_df_avro (line 20865)",
            "pass":true
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_sql_avro (line 19880)",
            "pass":true
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_df_parquet (line 28460)",
            "pass":true
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_sql_parquet (line 20482)",
            "pass":true
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws403; line 1 pos 14;",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(read) log_w_df_r_df_parquet (line 8566)",
            "pass":true
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_sql_orc (line 20481)",
            "pass":true
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_sql_parquet (line 20482)",
            "pass":true
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws403; line 1 pos 14;",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(read) log_w_df_r_df_avro (line 10254)",
            "pass":true
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_df_orc (line 28461)",
            "pass":true
        }
    },
    "404":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Error in query: Literals of type 'epoch' are currently not supported for the interval second type.; line 1 pos 58",
            "log_location":"(write) log_w_sql_avro (line 20126)",
            "pass":true
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws404; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Literals of type 'epoch' are currently not supported for the interval second type.; line 1 pos 0",
            "log_location":"(read) log_w_df_r_df_orc (line 8604)",
            "pass":true
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Error in query: Literals of type 'epoch' are currently not supported for the interval second type.; line 1 pos 58",
            "log_location":"(write) log_w_sql_orc (line 20726)",
            "pass":true
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Literals of type 'epoch' are currently not supported for the interval second type.; line 1 pos 0",
            "log_location":"(write) log_w_df_avro (line 20935)",
            "pass":true
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Error in query: Literals of type 'epoch' are currently not supported for the interval second type.; line 1 pos 58",
            "log_location":"(write) log_w_sql_avro (line 20126)",
            "pass":true
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Literals of type 'epoch' are currently not supported for the interval second type.; line 1 pos 0",
            "log_location":"(write) log_w_df_parquet (line 28530)",
            "pass":true
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Error in query: Literals of type 'epoch' are currently not supported for the interval second type.; line 1 pos 58",
            "log_location":"(write) log_w_sql_parquet (line 20727)",
            "pass":true
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws404; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Literals of type 'epoch' are currently not supported for the interval second type.; line 1 pos 0",
            "log_location":"(read) log_w_df_r_df_parquet (line 8606)",
            "pass":true
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Error in query: Literals of type 'epoch' are currently not supported for the interval second type.; line 1 pos 58",
            "log_location":"(write) log_w_sql_orc (line 20726)",
            "pass":true
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Error in query: Literals of type 'epoch' are currently not supported for the interval second type.; line 1 pos 58",
            "log_location":"(write) log_w_sql_parquet (line 20727)",
            "pass":true
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws404; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Literals of type 'epoch' are currently not supported for the interval second type.; line 1 pos 0",
            "log_location":"(read) log_w_df_r_df_avro (line 10294)",
            "pass":true
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Literals of type 'epoch' are currently not supported for the interval second type.; line 1 pos 0",
            "log_location":"(write) log_w_df_orc (line 28531)",
            "pass":true
        }
    },
    "405":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Interval string does not match day-time format of `[+|-]d h`, `INTERVAL [+|-]'[+|-]d h' DAY TO HOUR` when cast to interval day to hour: 1 2:03:04, set spark.sql.legacy.fromDayTimeString.enabled to true to restore the behavior before Spark 3.0.(line 1, pos 67)",
            "log_location":"(write) log_w_sql_avro (line 20141)",
            "pass":true
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws405; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_orc (line 8644)",
            "pass":true
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Interval string does not match day-time format of `[+|-]d h`, `INTERVAL [+|-]'[+|-]d h' DAY TO HOUR` when cast to interval day to hour: 1 2:03:04, set spark.sql.legacy.fromDayTimeString.enabled to true to restore the behavior before Spark 3.0.(line 1, pos 67)",
            "log_location":"(write) log_w_sql_orc (line 20740)",
            "pass":true
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_avro (line 21054)",
            "pass":true
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Interval string does not match day-time format of `[+|-]d h`, `INTERVAL [+|-]'[+|-]d h' DAY TO HOUR` when cast to interval day to hour: 1 2:03:04, set spark.sql.legacy.fromDayTimeString.enabled to true to restore the behavior before Spark 3.0.(line 1, pos 67)",
            "log_location":"(write) log_w_sql_avro (line 20141)",
            "pass":true
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_parquet (line 28649)",
            "pass":true
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Interval string does not match day-time format of `[+|-]d h`, `INTERVAL [+|-]'[+|-]d h' DAY TO HOUR` when cast to interval day to hour: 1 2:03:04, set spark.sql.legacy.fromDayTimeString.enabled to true to restore the behavior before Spark 3.0.(line 1, pos 67)",
            "log_location":"(write) log_w_sql_parquet (line 20741)",
            "pass":true
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws405; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_parquet (line 8646)",
            "pass":true
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Interval string does not match day-time format of `[+|-]d h`, `INTERVAL [+|-]'[+|-]d h' DAY TO HOUR` when cast to interval day to hour: 1 2:03:04, set spark.sql.legacy.fromDayTimeString.enabled to true to restore the behavior before Spark 3.0.(line 1, pos 67)",
            "log_location":"(write) log_w_sql_orc (line 20740)",
            "pass":true
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Interval string does not match day-time format of `[+|-]d h`, `INTERVAL [+|-]'[+|-]d h' DAY TO HOUR` when cast to interval day to hour: 1 2:03:04, set spark.sql.legacy.fromDayTimeString.enabled to true to restore the behavior before Spark 3.0.(line 1, pos 67)",
            "log_location":"(write) log_w_sql_parquet (line 20741)",
            "pass":true
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws405; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_avro (line 10334)",
            "pass":true
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_orc (line 28650)",
            "pass":true
        }
    },
    "406":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot parse the INTERVAL value: 1-2(line 1, pos 58)",
            "log_location":"(write) log_w_sql_avro (line 20161)",
            "pass":true
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws406; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_orc (line 8684)",
            "pass":true
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot parse the INTERVAL value: 1-2(line 1, pos 58)",
            "log_location":"(write) log_w_sql_orc (line 20759)",
            "pass":true
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_avro (line 21130)",
            "pass":true
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot parse the INTERVAL value: 1-2(line 1, pos 58)",
            "log_location":"(write) log_w_sql_avro (line 20161)",
            "pass":true
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_parquet (line 28725)",
            "pass":true
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot parse the INTERVAL value: 1-2(line 1, pos 58)",
            "log_location":"(write) log_w_sql_parquet (line 20760)",
            "pass":true
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws406; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_parquet (line 8686)",
            "pass":true
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot parse the INTERVAL value: 1-2(line 1, pos 58)",
            "log_location":"(write) log_w_sql_orc (line 20759)",
            "pass":true
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot parse the INTERVAL value: 1-2(line 1, pos 58)",
            "log_location":"(write) log_w_sql_parquet (line 20760)",
            "pass":true
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws406; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_avro (line 10374)",
            "pass":true
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_orc (line 28726)",
            "pass":true
        }
    },
    "407":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot parse the INTERVAL value: 1 day 01:23:45.6789(line 1, pos 58)",
            "log_location":"(write) log_w_sql_avro (line 20181)",
            "pass":true
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws407; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_orc (line 8724)",
            "pass":true
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot parse the INTERVAL value: 1 day 01:23:45.6789(line 1, pos 58)",
            "log_location":"(write) log_w_sql_orc (line 20778)",
            "pass":true
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_avro (line 21195)",
            "pass":true
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot parse the INTERVAL value: 1 day 01:23:45.6789(line 1, pos 58)",
            "log_location":"(write) log_w_sql_avro (line 20181)",
            "pass":true
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_parquet (line 28790)",
            "pass":true
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot parse the INTERVAL value: 1 day 01:23:45.6789(line 1, pos 58)",
            "log_location":"(write) log_w_sql_parquet (line 20779)",
            "pass":true
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws407; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_parquet (line 8726)",
            "pass":true
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot parse the INTERVAL value: 1 day 01:23:45.6789(line 1, pos 58)",
            "log_location":"(write) log_w_sql_orc (line 20778)",
            "pass":true
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot parse the INTERVAL value: 1 day 01:23:45.6789(line 1, pos 58)",
            "log_location":"(write) log_w_sql_parquet (line 20779)",
            "pass":true
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws407; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_avro (line 10414)",
            "pass":true
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_orc (line 28791)",
            "pass":true
        }
    },
    "408":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Error parsing ' 123 11 day' to interval, invalid unit '11'(line 1, pos 67)",
            "log_location":"(write) log_w_sql_avro (line 20201)",
            "pass":true
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws408; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_orc (line 8764)",
            "pass":true
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Error parsing ' 123 11 day' to interval, invalid unit '11'(line 1, pos 67)",
            "log_location":"(write) log_w_sql_orc (line 20797)",
            "pass":true
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_avro (line 21260)",
            "pass":true
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Error parsing ' 123 11 day' to interval, invalid unit '11'(line 1, pos 67)",
            "log_location":"(write) log_w_sql_avro (line 20201)",
            "pass":true
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_parquet (line 28855)",
            "pass":true
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Error parsing ' 123 11 day' to interval, invalid unit '11'(line 1, pos 67)",
            "log_location":"(write) log_w_sql_parquet (line 20798)",
            "pass":true
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws408; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_parquet (line 8766)",
            "pass":true
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Error parsing ' 123 11 day' to interval, invalid unit '11'(line 1, pos 67)",
            "log_location":"(write) log_w_sql_orc (line 20797)",
            "pass":true
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Error parsing ' 123 11 day' to interval, invalid unit '11'(line 1, pos 67)",
            "log_location":"(write) log_w_sql_parquet (line 20798)",
            "pass":true
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws408; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_avro (line 10454)",
            "pass":true
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_orc (line 28856)",
            "pass":true
        }
    },
    "409":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Can only use numbers in the interval value part for multiple unit value pairs interval form, but got invalid value: 123 days hours(line 1, pos 67)",
            "log_location":"(write) log_w_sql_avro (line 20221)",
            "pass":true
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws409; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_orc (line 8804)",
            "pass":true
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Can only use numbers in the interval value part for multiple unit value pairs interval form, but got invalid value: 123 days hours(line 1, pos 67)",
            "log_location":"(write) log_w_sql_orc (line 20816)",
            "pass":true
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_avro (line 21334)",
            "pass":true
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Can only use numbers in the interval value part for multiple unit value pairs interval form, but got invalid value: 123 days hours(line 1, pos 67)",
            "log_location":"(write) log_w_sql_avro (line 20221)",
            "pass":true
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_parquet (line 28929)",
            "pass":true
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Can only use numbers in the interval value part for multiple unit value pairs interval form, but got invalid value: 123 days hours(line 1, pos 67)",
            "log_location":"(write) log_w_sql_parquet (line 20817)",
            "pass":true
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws409; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_parquet (line 8806)",
            "pass":true
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Can only use numbers in the interval value part for multiple unit value pairs interval form, but got invalid value: 123 days hours(line 1, pos 67)",
            "log_location":"(write) log_w_sql_orc (line 20816)",
            "pass":true
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Can only use numbers in the interval value part for multiple unit value pairs interval form, but got invalid value: 123 days hours(line 1, pos 67)",
            "log_location":"(write) log_w_sql_parquet (line 20817)",
            "pass":true
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws409; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_avro (line 10494)",
            "pass":true
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_orc (line 28930)",
            "pass":true
        }
    },
    "410":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot mix year-month and day-time fields: INTERVAL '10 years -11 month -12 days'(line 1, pos 71)",
            "log_location":"(write) log_w_sql_avro (line 20241)",
            "pass":true
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws410; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_orc (line 8844)",
            "pass":true
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot mix year-month and day-time fields: INTERVAL '10 years -11 month -12 days'(line 1, pos 71)",
            "log_location":"(write) log_w_sql_orc (line 20835)",
            "pass":true
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_avro (line 21415)",
            "pass":true
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot mix year-month and day-time fields: INTERVAL '10 years -11 month -12 days'(line 1, pos 71)",
            "log_location":"(write) log_w_sql_avro (line 20241)",
            "pass":true
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_parquet (line 29010)",
            "pass":true
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot mix year-month and day-time fields: INTERVAL '10 years -11 month -12 days'(line 1, pos 71)",
            "log_location":"(write) log_w_sql_parquet (line 20836)",
            "pass":true
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws410; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_parquet (line 8846)",
            "pass":true
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot mix year-month and day-time fields: INTERVAL '10 years -11 month -12 days'(line 1, pos 71)",
            "log_location":"(write) log_w_sql_orc (line 20835)",
            "pass":true
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot mix year-month and day-time fields: INTERVAL '10 years -11 month -12 days'(line 1, pos 71)",
            "log_location":"(write) log_w_sql_parquet (line 20836)",
            "pass":true
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws410; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_avro (line 10534)",
            "pass":true
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_orc (line 29011)",
            "pass":true
        }
    },
    "411":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_sql_avro (line 20544)",
            "pass":true
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws411; line 1 pos 14;",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(read) log_w_df_r_df_orc (line 8884)",
            "pass":true
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_sql_orc (line 21137)",
            "pass":true
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_df_avro (line 21481)",
            "pass":true
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_sql_avro (line 20544)",
            "pass":true
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_df_parquet (line 29076)",
            "pass":true
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_sql_parquet (line 21138)",
            "pass":true
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws411; line 1 pos 14;",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(read) log_w_df_r_df_parquet (line 8886)",
            "pass":true
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_sql_orc (line 21137)",
            "pass":true
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_sql_parquet (line 21138)",
            "pass":true
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws411; line 1 pos 14;",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(read) log_w_df_r_df_avro (line 10574)",
            "pass":true
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_df_orc (line 29077)",
            "pass":true
        }
    },
    "412":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_sql_avro (line 21080)",
            "pass":true
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws412; line 1 pos 14;",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(read) log_w_df_r_df_orc (line 8924)",
            "pass":true
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_sql_orc (line 21672)",
            "pass":true
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_df_avro (line 21542)",
            "pass":true
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_sql_avro (line 21080)",
            "pass":true
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_df_parquet (line 29137)",
            "pass":true
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_sql_parquet (line 21673)",
            "pass":true
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws412; line 1 pos 14;",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(read) log_w_df_r_df_parquet (line 8926)",
            "pass":true
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_sql_orc (line 21672)",
            "pass":true
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_sql_parquet (line 21673)",
            "pass":true
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws412; line 1 pos 14;",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(read) log_w_df_r_df_avro (line 10614)",
            "pass":true
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: java.lang.ArithmeticException: long overflow",
            "log_location":"(write) log_w_df_orc (line 29138)",
            "pass":true
        }
    },
    "413":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Error in query: Literals of type 'epoch' are currently not supported for the interval second type.; line 1 pos 71",
            "log_location":"(write) log_w_sql_avro (line 21326)",
            "pass":true
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws413; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Literals of type 'epoch' are currently not supported for the interval second type.; line 1 pos 0",
            "log_location":"(read) log_w_df_r_df_orc (line 8964)",
            "pass":true
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Error in query: Literals of type 'epoch' are currently not supported for the interval second type.; line 1 pos 71",
            "log_location":"(write) log_w_sql_orc (line 21917)",
            "pass":true
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Literals of type 'epoch' are currently not supported for the interval second type.; line 1 pos 0",
            "log_location":"(write) log_w_df_avro (line 21613)",
            "pass":true
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Error in query: Literals of type 'epoch' are currently not supported for the interval second type.; line 1 pos 71",
            "log_location":"(write) log_w_sql_avro (line 21326)",
            "pass":true
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Literals of type 'epoch' are currently not supported for the interval second type.; line 1 pos 0",
            "log_location":"(write) log_w_df_parquet (line 29208)",
            "pass":true
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Error in query: Literals of type 'epoch' are currently not supported for the interval second type.; line 1 pos 71",
            "log_location":"(write) log_w_sql_parquet (line 21918)",
            "pass":true
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws413; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Literals of type 'epoch' are currently not supported for the interval second type.; line 1 pos 0",
            "log_location":"(read) log_w_df_r_df_parquet (line 8966)",
            "pass":true
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Error in query: Literals of type 'epoch' are currently not supported for the interval second type.; line 1 pos 71",
            "log_location":"(write) log_w_sql_orc (line 21917)",
            "pass":true
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Error in query: Literals of type 'epoch' are currently not supported for the interval second type.; line 1 pos 71",
            "log_location":"(write) log_w_sql_parquet (line 21918)",
            "pass":true
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws413; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Literals of type 'epoch' are currently not supported for the interval second type.; line 1 pos 0",
            "log_location":"(read) log_w_df_r_df_avro (line 10654)",
            "pass":true
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Literals of type 'epoch' are currently not supported for the interval second type.; line 1 pos 0",
            "log_location":"(write) log_w_df_orc (line 29209)",
            "pass":true
        }
    },
    "414":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Interval string does not match day-time format of `[+|-]d h`, `INTERVAL [+|-]'[+|-]d h' DAY TO HOUR` when cast to interval day to hour: 1 2:03:04, set spark.sql.legacy.fromDayTimeString.enabled to true to restore the behavior before Spark 3.0.(line 1, pos 80)",
            "log_location":"(write) log_w_sql_avro (line 21341)",
            "pass":true
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws414; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_orc (line 9004)",
            "pass":true
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Interval string does not match day-time format of `[+|-]d h`, `INTERVAL [+|-]'[+|-]d h' DAY TO HOUR` when cast to interval day to hour: 1 2:03:04, set spark.sql.legacy.fromDayTimeString.enabled to true to restore the behavior before Spark 3.0.(line 1, pos 80)",
            "log_location":"(write) log_w_sql_orc (line 21931)",
            "pass":true
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_avro (line 21733)",
            "pass":true
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Interval string does not match day-time format of `[+|-]d h`, `INTERVAL [+|-]'[+|-]d h' DAY TO HOUR` when cast to interval day to hour: 1 2:03:04, set spark.sql.legacy.fromDayTimeString.enabled to true to restore the behavior before Spark 3.0.(line 1, pos 80)",
            "log_location":"(write) log_w_sql_avro (line 21341)",
            "pass":true
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_parquet (line 29328)",
            "pass":true
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Interval string does not match day-time format of `[+|-]d h`, `INTERVAL [+|-]'[+|-]d h' DAY TO HOUR` when cast to interval day to hour: 1 2:03:04, set spark.sql.legacy.fromDayTimeString.enabled to true to restore the behavior before Spark 3.0.(line 1, pos 80)",
            "log_location":"(write) log_w_sql_parquet (line 21932)",
            "pass":true
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws414; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_parquet (line 9006)",
            "pass":true
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Interval string does not match day-time format of `[+|-]d h`, `INTERVAL [+|-]'[+|-]d h' DAY TO HOUR` when cast to interval day to hour: 1 2:03:04, set spark.sql.legacy.fromDayTimeString.enabled to true to restore the behavior before Spark 3.0.(line 1, pos 80)",
            "log_location":"(write) log_w_sql_orc (line 21931)",
            "pass":true
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Interval string does not match day-time format of `[+|-]d h`, `INTERVAL [+|-]'[+|-]d h' DAY TO HOUR` when cast to interval day to hour: 1 2:03:04, set spark.sql.legacy.fromDayTimeString.enabled to true to restore the behavior before Spark 3.0.(line 1, pos 80)",
            "log_location":"(write) log_w_sql_parquet (line 21932)",
            "pass":true
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws414; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_avro (line 10694)",
            "pass":true
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_orc (line 29329)",
            "pass":true
        }
    },
    "415":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot parse the INTERVAL value: 1-2(line 1, pos 71)",
            "log_location":"(write) log_w_sql_avro (line 21361)",
            "pass":true
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws415; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_orc (line 9044)",
            "pass":true
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot parse the INTERVAL value: 1-2(line 1, pos 71)",
            "log_location":"(write) log_w_sql_orc (line 21950)",
            "pass":true
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_avro (line 21810)",
            "pass":true
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot parse the INTERVAL value: 1-2(line 1, pos 71)",
            "log_location":"(write) log_w_sql_avro (line 21361)",
            "pass":true
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_parquet (line 29405)",
            "pass":true
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot parse the INTERVAL value: 1-2(line 1, pos 71)",
            "log_location":"(write) log_w_sql_parquet (line 21951)",
            "pass":true
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws415; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_parquet (line 9046)",
            "pass":true
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot parse the INTERVAL value: 1-2(line 1, pos 71)",
            "log_location":"(write) log_w_sql_orc (line 21950)",
            "pass":true
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot parse the INTERVAL value: 1-2(line 1, pos 71)",
            "log_location":"(write) log_w_sql_parquet (line 21951)",
            "pass":true
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws415; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_avro (line 10734)",
            "pass":true
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_orc (line 29406)",
            "pass":true
        }
    },
    "416":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot parse the INTERVAL value: 1 day 01:23:45.6789(line 1, pos 71)",
            "log_location":"(write) log_w_sql_avro (line 21381)",
            "pass":true
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws416; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_orc (line 9084)",
            "pass":true
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot parse the INTERVAL value: 1 day 01:23:45.6789(line 1, pos 71)",
            "log_location":"(write) log_w_sql_orc (line 21969)",
            "pass":true
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_avro (line 21876)",
            "pass":true
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot parse the INTERVAL value: 1 day 01:23:45.6789(line 1, pos 71)",
            "log_location":"(write) log_w_sql_avro (line 21381)",
            "pass":true
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_parquet (line 29471)",
            "pass":true
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot parse the INTERVAL value: 1 day 01:23:45.6789(line 1, pos 71)",
            "log_location":"(write) log_w_sql_parquet (line 21970)",
            "pass":true
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws416; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_parquet (line 9086)",
            "pass":true
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot parse the INTERVAL value: 1 day 01:23:45.6789(line 1, pos 71)",
            "log_location":"(write) log_w_sql_orc (line 21969)",
            "pass":true
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Cannot parse the INTERVAL value: 1 day 01:23:45.6789(line 1, pos 71)",
            "log_location":"(write) log_w_sql_parquet (line 21970)",
            "pass":true
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws416; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_avro (line 10774)",
            "pass":true
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_orc (line 29472)",
            "pass":true
        }
    },
    "417":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Error parsing ' 123 11 day' to interval, invalid unit '11'(line 1, pos 80)",
            "log_location":"(write) log_w_sql_avro (line 21401)",
            "pass":true
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws417; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_orc (line 9124)",
            "pass":true
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Error parsing ' 123 11 day' to interval, invalid unit '11'(line 1, pos 80)",
            "log_location":"(write) log_w_sql_orc (line 21988)",
            "pass":true
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_avro (line 21942)",
            "pass":true
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Error parsing ' 123 11 day' to interval, invalid unit '11'(line 1, pos 80)",
            "log_location":"(write) log_w_sql_avro (line 21401)",
            "pass":true
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_parquet (line 29537)",
            "pass":true
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Error parsing ' 123 11 day' to interval, invalid unit '11'(line 1, pos 80)",
            "log_location":"(write) log_w_sql_parquet (line 21989)",
            "pass":true
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws417; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_parquet (line 9126)",
            "pass":true
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Error parsing ' 123 11 day' to interval, invalid unit '11'(line 1, pos 80)",
            "log_location":"(write) log_w_sql_orc (line 21988)",
            "pass":true
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Error parsing ' 123 11 day' to interval, invalid unit '11'(line 1, pos 80)",
            "log_location":"(write) log_w_sql_parquet (line 21989)",
            "pass":true
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws417; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_avro (line 10814)",
            "pass":true
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_orc (line 29538)",
            "pass":true
        }
    },
    "418":{
        "t_w_sql_r_sql_avro":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Can only use numbers in the interval value part for multiple unit value pairs interval form, but got invalid value: 123 days hours(line 1, pos 80)",
            "log_location":"(write) log_w_sql_avro (line 21421)",
            "pass":true
        },
        "t_w_df_r_df_orc":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws418; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_orc (line 9164)",
            "pass":true
        },
        "t_w_sql_r_df_orc":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Can only use numbers in the interval value part for multiple unit value pairs interval form, but got invalid value: 123 days hours(line 1, pos 80)",
            "log_location":"(write) log_w_sql_orc (line 22007)",
            "pass":true
        },
        "t_w_df_r_sql_avro":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_avro (line 22017)",
            "pass":true
        },
        "t_w_sql_r_df_avro":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output",
            "write_value":"No output, find exception: Can only use numbers in the interval value part for multiple unit value pairs interval form, but got invalid value: 123 days hours(line 1, pos 80)",
            "log_location":"(write) log_w_sql_avro (line 21421)",
            "pass":true
        },
        "t_w_df_r_sql_parquet":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_parquet (line 29612)",
            "pass":true
        },
        "t_w_sql_r_sql_parquet":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Can only use numbers in the interval value part for multiple unit value pairs interval form, but got invalid value: 123 days hours(line 1, pos 80)",
            "log_location":"(write) log_w_sql_parquet (line 22008)",
            "pass":true
        },
        "t_w_df_r_df_parquet":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws418; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_parquet (line 9166)",
            "pass":true
        },
        "t_w_sql_r_sql_orc":{
            "write_interface":"sql",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: Can only use numbers in the interval value part for multiple unit value pairs interval form, but got invalid value: 123 days hours(line 1, pos 80)",
            "log_location":"(write) log_w_sql_orc (line 22007)",
            "pass":true
        },
        "t_w_sql_r_df_parquet":{
            "write_interface":"sql",
            "read_interface":"df",
            "format_type":"parquet",
            "read_value":"No output",
            "write_value":"No output, find exception: Can only use numbers in the interval value part for multiple unit value pairs interval form, but got invalid value: 123 days hours(line 1, pos 80)",
            "log_location":"(write) log_w_sql_parquet (line 22008)",
            "pass":true
        },
        "t_w_df_r_df_avro":{
            "write_interface":"df",
            "read_interface":"df",
            "format_type":"avro",
            "read_value":"No output, find exception: org.apache.spark.sql.AnalysisException: Table or view not found: ws418; line 1 pos 14;",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(read) log_w_df_r_df_avro (line 10854)",
            "pass":true
        },
        "t_w_df_r_sql_orc":{
            "write_interface":"df",
            "read_interface":"sql",
            "format_type":"orc",
            "read_value":"No output",
            "write_value":"No output, find exception: org.apache.spark.sql.catalyst.parser.ParseException:",
            "log_location":"(write) log_w_df_orc (line 29613)",
            "pass":true
        }
    }
}